{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentimentTestinUSDLoop.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hugegene/LSTM-Prediction-of-Stock-Price-Movement/blob/master/SentimentTestingUSDLoop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "dIF_jv42m9pH"
      },
      "cell_type": "markdown",
      "source": [
        "# Rolling LSTM testing on USD price movements"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "hOXobOBeKwDe"
      },
      "cell_type": "markdown",
      "source": [
        "## 1 Objective and Overview\n",
        "\n",
        "A supervised learning was done to make trading decisions (Sell or Buy) for USD using USD's past price and MarketRisk TRMI.\n",
        "\n",
        "The supervised model used was LSTM.  \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "mPEtY17IU0Qs"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.1 Splitting Train-Validate-Test Set\n",
        "\n",
        "2016-11-01 data to 2017-12-31 USD price and TRMI MarketRisk data was used for this experiment.\n",
        "\n",
        "The data is plit to rolling train and test sets as shown:\n",
        "\n",
        "\n",
        "The effective test period is from 2017-01-01 to 2017-12-31\n",
        "\n",
        "The graph below shows the USD price over the effective test period.\n"
      ]
    },
    {
      "metadata": {
        "id": "OjlMoZGeFd32",
        "colab_type": "code",
        "outputId": "89bd273d-c246-451b-af3c-31221d4694e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        }
      },
      "cell_type": "code",
      "source": [
        "# Plotting USD prices\n",
        "ax = tradeTable[\"2017-01-01\":\"2017-12-31\"].plot(y=['Close'], figsize=(12,6), grid=True)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/plotting/_core.py:1716: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
            "  series.name = label\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsYAAAF/CAYAAABKRQ+VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCEASYyGEEEIIIQBJjIUQQgghhADg/wcJbaq6pgv24AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f16032137f0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "PCC2x8HuU4Y3"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.2 Feature Engineering"
      ]
    },
    {
      "metadata": {
        "id": "tYnpeunFweP9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://github.com/hugegene/LSTM-Prediction-of-Stock-Price-Movement/blob/master/USDprice.png?raw=1)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "NHFZ6CcMVDaX"
      },
      "cell_type": "markdown",
      "source": [
        "USD price was overlayed with various TRMIs moving averages and the 30-days MarketRisk moving average seems to co-related well with the price visually. As such, the following was feature-enginnered as the input to the LSTM model for all the experiments:\n",
        "\n",
        "1. 90-day MarketRisk moving-average\n",
        "2. 30-day MarketRisk moving-average\n",
        "3. Close\n",
        "4. 100-day Close moving-average\n",
        "\n",
        "## 1.3 Evaluation Metrics\n",
        "\n",
        "The evaluation metrics are:\n",
        "1. Matthews correlation coefficient (MCC)\n",
        "2. Accuracy\n",
        "3. Profit and Loss\n",
        "\n",
        "\n",
        "## 1.4 Labels\n",
        "\n",
        "Each timestep is labelled Sell(0) or Buy(1) depending on whether the next closing price is up or down.\n",
        "\n",
        "\n",
        "## 1.5 LSTM Set-Up\n",
        "\n",
        "The LSTM is set-up to predict the labels based on the sequential data of the past 5 time-steps.\n",
        "\n",
        "\n",
        "## 1.6 Trading\n",
        "\n",
        "Each LSTM prediction of Buy will result in a buying and holding of USD for 1 day  and each LSTM prediction of Sell will result in a short-sell and holding of USD for 1 day."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "2c-AjZEwWonj"
      },
      "cell_type": "markdown",
      "source": [
        "## 2 Results "
      ]
    },
    {
      "metadata": {
        "id": "9X8agEoisJKp",
        "colab_type": "code",
        "outputId": "3e16c366-cd49-47cd-b2a2-6a0dc3a3aacc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 893
        }
      },
      "cell_type": "code",
      "source": [
        "# calculating cumulative predicted and labelled trade returns over each time step\n",
        "PNLTable =  tradeTable.loc[\"2017-01-01\":\"2017-02-28\",:].copy()\n",
        "PNLTable[\"labelledPNL\"] =  (1+PNLTable['labelledreturns']).cumprod()\n",
        "PNLTable[\"PNL\"] = (1+PNLTable['predictedreturns']).cumprod()\n",
        "PNLTable.head()\n",
        "\n",
        "#plotting Confusion Matrix\n",
        "f = plt.figure(figsize=(15,7))\n",
        "ax = f.add_subplot(131)\n",
        "plotConfusion(\"2017-01-01\", \"2017-02-28\", \"\")\n",
        "\n",
        "f = plt.figure(figsize=(36,6))\n",
        "ax = f.add_subplot(131)\n",
        "\n",
        "# PNL Plot for the Test Period\n",
        "PNLTable[\"PNL\"].plot()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[12  8]\n",
            " [10 12]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f160dc8d6a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAGPCAYAAABbFLtAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwDKEHwAsQ/gBwDKEHwAs8/8BhEUmpJfJv04AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f160347a4e0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAncAAAGWCAYAAAAaOqXYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHQAAgIlQ7gAAAEyEcgcAAGAi/w/YVJKNYUTw3QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f16033d3978>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "PI1G2mO9bdAE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.1 Evaluation\n",
        "\n",
        "**Good**\n",
        "\n",
        "In the experiment, 8 LSTM models were trained to predict 1 week of USD price movement ahead. The LSTM model fitted the train set as much as it could and was used to predict the next week's USD price movements. \n",
        "\n",
        "The above confusion matrix shows a good spread of predicted Buy and Sell decisions with a accuracy of 0.57 and MCC of 0.15.\n",
        "\n",
        "The above PNL plot also shows that the 8 LSTM trained at one go, without any cherry picking of LSTM models, gives rise to 3% profit in 2 months.\n",
        "\n",
        "**Bad**\n",
        "\n",
        "However, more testing is required to prove that the above methodology is not one-off event. \n",
        "More testings with longer rolling periods of test sets are required. \n",
        "Repeated testing of the same period of test sets are required to ensure that the repeated test will produce the same results.\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "p1nongBRVFzR"
      },
      "cell_type": "markdown",
      "source": [
        "##  3 Codes"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7qHBVmabJWCD",
        "outputId": "789a4722-e750-42e1-f149-b8369910336a",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bfd93b44-af96-4842-981e-9f68df984272\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-bfd93b44-af96-4842-981e-9f68df984272\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving USDSentiment.csv to USDSentiment.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "HGRXzvUR0X11",
        "outputId": "a434dc3d-ec82-46a7-b732-4df5467cafb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best.h5  sample_data  us500Prices.csv  USDprices.csv  USDSentiment.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "MxvJZZarlEe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e7e813c6-fc6e-49f3-ac64-b08fbcd28c76"
      },
      "cell_type": "code",
      "source": [
        "from math import sqrt\n",
        "from numpy import concatenate\n",
        "from matplotlib import pyplot as plt\n",
        "from pandas import read_csv\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import model_from_json\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from datetime import datetime\n",
        "import os\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from keras.layers import Input, LSTM, RepeatVector, Lambda, Dense, Flatten, Permute, merge, multiply\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.metrics import roc_auc_score as areauc\n",
        "import tensorflow as tf\n",
        "from datetime import timedelta, date"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "H0U6XhwlpraX"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.1 Data Preparation"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "CNggyVu2lcl4",
        "outputId": "edc97ff7-a534-4117-8288-b048b154683c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "cell_type": "code",
      "source": [
        "##Read in Price data, use date as index\n",
        "Prices = pd.read_csv(\"USDprices.csv\")\n",
        "Prices.head()\n",
        "Prices['Date'].dtype\n",
        "Prices['Date'] = pd.to_datetime(Prices['Date'], format='%Y-%m-%d')\n",
        "Prices.head()\n",
        "Prices.index = Prices['Date']\n",
        "# print(Prices.head())\n",
        "\n",
        "#Reading in Sentiment Data and selecting News-only-MarketRisk Sentiment\n",
        "Sent = pd.read_csv(\"USDSentiment.csv\")\n",
        "Sent = Sent[Sent.dataType=='News'][['Date', 'marketRisk']].fillna(method = \"ffill\")\n",
        "# Sent.head()\n",
        "Sent['Date'] = pd.to_datetime(Sent['Date'], format='%Y-%m-%d')\n",
        "Sent.index = Sent['Date']\n",
        "# print(Sent.head())\n",
        "\n",
        "#Combining Price table and Sentiment Table\n",
        "Ana = Prices.merge(Sent, left_index= True, right_index=True, how='inner')\n",
        "Ana.head()\n",
        "Ana.columns\n",
        "Ana = Ana.drop(columns=['Unnamed: 0', 'Volume', 'Asset', 'Date_x', 'Date_y', 'Open', 'High', 'Low', 'UnadjClose'])\n",
        "Ana.head()\n",
        "\n",
        "# calculating r, %tage change of closing price over previous time step\n",
        "Ana['returns'] = (Ana['Close']- Ana['Close'].shift(1))/Ana['Close'].shift(1)\n",
        "Ana.head()\n",
        "\n",
        "# calculating target price\n",
        "Ana['Target'] = Ana[\"Close\"].diff(1).shift(-1)\n",
        "Ana.head()\n",
        "\n",
        "# calculating %change\n",
        "Ana['Change'] = Ana[\"returns\"].shift(-1)\n",
        "Ana.head()\n",
        "\n",
        "\n",
        "## Label each time step False(no-buy) or True(buy) based on whether the price will rise at the next closing price\n",
        "timestep = 1\n",
        "labels = []\n",
        "for i in range(0, Ana.shape[0]):\n",
        "  if(i+timestep< Ana.shape[0]):\n",
        "    aheadGain = [Ana[\"returns\"][i+j] for j in range(1,timestep+1)]\n",
        "    labels+= [np.sum(aheadGain)> 0]\n",
        "\n",
        "print(len(labels))\n",
        "\n",
        "# Price Table with Labels as Signal\n",
        "Ana = Ana.iloc[:-timestep,:].copy()\n",
        "Ana.head()\n",
        "Ana['Signal'] = labels\n",
        "Ana.head()\n",
        "\n",
        "#Feature Engineering\n",
        "Ana[\"Close30\"] =  Ana[\"Close\"].rolling(30).mean()\n",
        "Ana[\"Close100\"] =  Ana[\"Close\"].rolling(100).mean()\n",
        "Ana[\"r1\"] =  Ana[\"Close\"].diff(1)\n",
        "Ana[\"r2\"] =  Ana[\"Close\"].diff(7)\n",
        "Ana[\"marketrisk_avg30\"] = Ana[\"marketRisk\"].rolling(30).mean()\n",
        "Ana[\"marketrisk_avg90\"] = Ana[\"marketRisk\"].rolling(90).mean()\n",
        "\n",
        "#Drop NA\n",
        "Ana = Ana.dropna(0)\n",
        "Ana.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5448\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Close</th>\n",
              "      <th>marketRisk</th>\n",
              "      <th>returns</th>\n",
              "      <th>Target</th>\n",
              "      <th>Change</th>\n",
              "      <th>Signal</th>\n",
              "      <th>Close30</th>\n",
              "      <th>Close100</th>\n",
              "      <th>r1</th>\n",
              "      <th>r2</th>\n",
              "      <th>marketrisk_avg30</th>\n",
              "      <th>marketrisk_avg90</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1998-05-21</th>\n",
              "      <td>0.894694</td>\n",
              "      <td>0.017921</td>\n",
              "      <td>-0.005816</td>\n",
              "      <td>-0.001439</td>\n",
              "      <td>-0.001608</td>\n",
              "      <td>False</td>\n",
              "      <td>0.905687</td>\n",
              "      <td>0.916241</td>\n",
              "      <td>-0.005234</td>\n",
              "      <td>-0.007833</td>\n",
              "      <td>0.030184</td>\n",
              "      <td>0.034995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998-05-22</th>\n",
              "      <td>0.893256</td>\n",
              "      <td>0.054887</td>\n",
              "      <td>-0.001608</td>\n",
              "      <td>0.004813</td>\n",
              "      <td>0.005388</td>\n",
              "      <td>True</td>\n",
              "      <td>0.904816</td>\n",
              "      <td>0.916045</td>\n",
              "      <td>-0.001439</td>\n",
              "      <td>-0.011721</td>\n",
              "      <td>0.030500</td>\n",
              "      <td>0.035283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998-05-25</th>\n",
              "      <td>0.898069</td>\n",
              "      <td>0.072076</td>\n",
              "      <td>0.005388</td>\n",
              "      <td>0.001373</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>True</td>\n",
              "      <td>0.904089</td>\n",
              "      <td>0.915792</td>\n",
              "      <td>0.004813</td>\n",
              "      <td>-0.005518</td>\n",
              "      <td>0.031202</td>\n",
              "      <td>0.035543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998-05-26</th>\n",
              "      <td>0.899442</td>\n",
              "      <td>0.120229</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.006027</td>\n",
              "      <td>0.006700</td>\n",
              "      <td>True</td>\n",
              "      <td>0.903746</td>\n",
              "      <td>0.915530</td>\n",
              "      <td>0.001373</td>\n",
              "      <td>-0.007587</td>\n",
              "      <td>0.034618</td>\n",
              "      <td>0.036311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998-05-27</th>\n",
              "      <td>0.905469</td>\n",
              "      <td>0.057863</td>\n",
              "      <td>0.006700</td>\n",
              "      <td>-0.001718</td>\n",
              "      <td>-0.001898</td>\n",
              "      <td>False</td>\n",
              "      <td>0.903617</td>\n",
              "      <td>0.915352</td>\n",
              "      <td>0.006027</td>\n",
              "      <td>-0.001643</td>\n",
              "      <td>0.036330</td>\n",
              "      <td>0.036533</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Close  marketRisk   returns    Target    Change  Signal  \\\n",
              "Date                                                                     \n",
              "1998-05-21  0.894694    0.017921 -0.005816 -0.001439 -0.001608   False   \n",
              "1998-05-22  0.893256    0.054887 -0.001608  0.004813  0.005388    True   \n",
              "1998-05-25  0.898069    0.072076  0.005388  0.001373  0.001529    True   \n",
              "1998-05-26  0.899442    0.120229  0.001529  0.006027  0.006700    True   \n",
              "1998-05-27  0.905469    0.057863  0.006700 -0.001718 -0.001898   False   \n",
              "\n",
              "             Close30  Close100        r1        r2  marketrisk_avg30  \\\n",
              "Date                                                                   \n",
              "1998-05-21  0.905687  0.916241 -0.005234 -0.007833          0.030184   \n",
              "1998-05-22  0.904816  0.916045 -0.001439 -0.011721          0.030500   \n",
              "1998-05-25  0.904089  0.915792  0.004813 -0.005518          0.031202   \n",
              "1998-05-26  0.903746  0.915530  0.001373 -0.007587          0.034618   \n",
              "1998-05-27  0.903617  0.915352  0.006027 -0.001643          0.036330   \n",
              "\n",
              "            marketrisk_avg90  \n",
              "Date                          \n",
              "1998-05-21          0.034995  \n",
              "1998-05-22          0.035283  \n",
              "1998-05-25          0.035543  \n",
              "1998-05-26          0.036311  \n",
              "1998-05-27          0.036533  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "HInEXnin29AV",
        "outputId": "e18483d4-f281-458d-c3ae-a9f1db4d049e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        }
      },
      "cell_type": "code",
      "source": [
        "#Define function to convert dataframe to training, validating and testing examples\n",
        "def series_to_supervised(df, n_in=1, n_out=1):\n",
        "\tn_vars = 1 if type(df) is list else df.shape[1]\n",
        "  \n",
        "  \n",
        "\tcols, names = list(), list()\n",
        "\t# input sequence (t-n, ... t-1)\n",
        "\tfor i in range(n_in, -1, -1):\n",
        "\t\tcols.append(df.shift(i))\n",
        "\t\tnames += [(df.columns[j]+'(t-%d)' % (i)) for j in range(n_vars)]\n",
        "\n",
        "\t# put it all together\n",
        "\tagg = concat(cols, axis=1)\n",
        "\tagg.columns = names\n",
        "\treturn agg\n",
        "\n",
        "#Features Selection\n",
        "Set =  Ana[['Close', 'Close100', 'marketrisk_avg90', 'marketrisk_avg30']]\n",
        "# Set =  Ana[['r1', 'r2']]\n",
        "FEATURES_SHAPE =  Set.shape[1]\n",
        "print(FEATURES_SHAPE)\n",
        "\n",
        "#Forming examples\n",
        "SEQ_LEN = 5\n",
        "Set =  series_to_supervised(Set, SEQ_LEN-1, 0)\n",
        "Set[[\"Change\", \"Signal\"]] = Ana[[\"Change\", \"Signal\"]]\n",
        "Set = Set.dropna()\n",
        "print(Set.shape)\n",
        "Set.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n",
            "(5345, 22)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Close(t-4)</th>\n",
              "      <th>Close100(t-4)</th>\n",
              "      <th>marketrisk_avg90(t-4)</th>\n",
              "      <th>marketrisk_avg30(t-4)</th>\n",
              "      <th>Close(t-3)</th>\n",
              "      <th>Close100(t-3)</th>\n",
              "      <th>marketrisk_avg90(t-3)</th>\n",
              "      <th>marketrisk_avg30(t-3)</th>\n",
              "      <th>Close(t-2)</th>\n",
              "      <th>Close100(t-2)</th>\n",
              "      <th>...</th>\n",
              "      <th>Close(t-1)</th>\n",
              "      <th>Close100(t-1)</th>\n",
              "      <th>marketrisk_avg90(t-1)</th>\n",
              "      <th>marketrisk_avg30(t-1)</th>\n",
              "      <th>Close(t-0)</th>\n",
              "      <th>Close100(t-0)</th>\n",
              "      <th>marketrisk_avg90(t-0)</th>\n",
              "      <th>marketrisk_avg30(t-0)</th>\n",
              "      <th>Change</th>\n",
              "      <th>Signal</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1998-05-27</th>\n",
              "      <td>0.894694</td>\n",
              "      <td>0.916241</td>\n",
              "      <td>0.034995</td>\n",
              "      <td>0.030184</td>\n",
              "      <td>0.893256</td>\n",
              "      <td>0.916045</td>\n",
              "      <td>0.035283</td>\n",
              "      <td>0.030500</td>\n",
              "      <td>0.898069</td>\n",
              "      <td>0.915792</td>\n",
              "      <td>...</td>\n",
              "      <td>0.899442</td>\n",
              "      <td>0.915530</td>\n",
              "      <td>0.036311</td>\n",
              "      <td>0.034618</td>\n",
              "      <td>0.905469</td>\n",
              "      <td>0.915352</td>\n",
              "      <td>0.036533</td>\n",
              "      <td>0.036330</td>\n",
              "      <td>-0.001898</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998-05-28</th>\n",
              "      <td>0.893256</td>\n",
              "      <td>0.916045</td>\n",
              "      <td>0.035283</td>\n",
              "      <td>0.030500</td>\n",
              "      <td>0.898069</td>\n",
              "      <td>0.915792</td>\n",
              "      <td>0.035543</td>\n",
              "      <td>0.031202</td>\n",
              "      <td>0.899442</td>\n",
              "      <td>0.915530</td>\n",
              "      <td>...</td>\n",
              "      <td>0.905469</td>\n",
              "      <td>0.915352</td>\n",
              "      <td>0.036533</td>\n",
              "      <td>0.036330</td>\n",
              "      <td>0.903751</td>\n",
              "      <td>0.915163</td>\n",
              "      <td>0.036895</td>\n",
              "      <td>0.035828</td>\n",
              "      <td>0.003719</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998-05-29</th>\n",
              "      <td>0.898069</td>\n",
              "      <td>0.915792</td>\n",
              "      <td>0.035543</td>\n",
              "      <td>0.031202</td>\n",
              "      <td>0.899442</td>\n",
              "      <td>0.915530</td>\n",
              "      <td>0.036311</td>\n",
              "      <td>0.034618</td>\n",
              "      <td>0.905469</td>\n",
              "      <td>0.915352</td>\n",
              "      <td>...</td>\n",
              "      <td>0.903751</td>\n",
              "      <td>0.915163</td>\n",
              "      <td>0.036895</td>\n",
              "      <td>0.035828</td>\n",
              "      <td>0.907112</td>\n",
              "      <td>0.915000</td>\n",
              "      <td>0.037338</td>\n",
              "      <td>0.036237</td>\n",
              "      <td>-0.004515</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998-06-01</th>\n",
              "      <td>0.899442</td>\n",
              "      <td>0.915530</td>\n",
              "      <td>0.036311</td>\n",
              "      <td>0.034618</td>\n",
              "      <td>0.905469</td>\n",
              "      <td>0.915352</td>\n",
              "      <td>0.036533</td>\n",
              "      <td>0.036330</td>\n",
              "      <td>0.903751</td>\n",
              "      <td>0.915163</td>\n",
              "      <td>...</td>\n",
              "      <td>0.907112</td>\n",
              "      <td>0.915000</td>\n",
              "      <td>0.037338</td>\n",
              "      <td>0.036237</td>\n",
              "      <td>0.903016</td>\n",
              "      <td>0.914801</td>\n",
              "      <td>0.037892</td>\n",
              "      <td>0.038099</td>\n",
              "      <td>0.000813</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998-06-02</th>\n",
              "      <td>0.905469</td>\n",
              "      <td>0.915352</td>\n",
              "      <td>0.036533</td>\n",
              "      <td>0.036330</td>\n",
              "      <td>0.903751</td>\n",
              "      <td>0.915163</td>\n",
              "      <td>0.036895</td>\n",
              "      <td>0.035828</td>\n",
              "      <td>0.907112</td>\n",
              "      <td>0.915000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.903016</td>\n",
              "      <td>0.914801</td>\n",
              "      <td>0.037892</td>\n",
              "      <td>0.038099</td>\n",
              "      <td>0.903751</td>\n",
              "      <td>0.914626</td>\n",
              "      <td>0.038163</td>\n",
              "      <td>0.038561</td>\n",
              "      <td>-0.005840</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 22 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Close(t-4)  Close100(t-4)  marketrisk_avg90(t-4)  \\\n",
              "Date                                                           \n",
              "1998-05-27    0.894694       0.916241               0.034995   \n",
              "1998-05-28    0.893256       0.916045               0.035283   \n",
              "1998-05-29    0.898069       0.915792               0.035543   \n",
              "1998-06-01    0.899442       0.915530               0.036311   \n",
              "1998-06-02    0.905469       0.915352               0.036533   \n",
              "\n",
              "            marketrisk_avg30(t-4)  Close(t-3)  Close100(t-3)  \\\n",
              "Date                                                           \n",
              "1998-05-27               0.030184    0.893256       0.916045   \n",
              "1998-05-28               0.030500    0.898069       0.915792   \n",
              "1998-05-29               0.031202    0.899442       0.915530   \n",
              "1998-06-01               0.034618    0.905469       0.915352   \n",
              "1998-06-02               0.036330    0.903751       0.915163   \n",
              "\n",
              "            marketrisk_avg90(t-3)  marketrisk_avg30(t-3)  Close(t-2)  \\\n",
              "Date                                                                   \n",
              "1998-05-27               0.035283               0.030500    0.898069   \n",
              "1998-05-28               0.035543               0.031202    0.899442   \n",
              "1998-05-29               0.036311               0.034618    0.905469   \n",
              "1998-06-01               0.036533               0.036330    0.903751   \n",
              "1998-06-02               0.036895               0.035828    0.907112   \n",
              "\n",
              "            Close100(t-2)   ...    Close(t-1)  Close100(t-1)  \\\n",
              "Date                        ...                                \n",
              "1998-05-27       0.915792   ...      0.899442       0.915530   \n",
              "1998-05-28       0.915530   ...      0.905469       0.915352   \n",
              "1998-05-29       0.915352   ...      0.903751       0.915163   \n",
              "1998-06-01       0.915163   ...      0.907112       0.915000   \n",
              "1998-06-02       0.915000   ...      0.903016       0.914801   \n",
              "\n",
              "            marketrisk_avg90(t-1)  marketrisk_avg30(t-1)  Close(t-0)  \\\n",
              "Date                                                                   \n",
              "1998-05-27               0.036311               0.034618    0.905469   \n",
              "1998-05-28               0.036533               0.036330    0.903751   \n",
              "1998-05-29               0.036895               0.035828    0.907112   \n",
              "1998-06-01               0.037338               0.036237    0.903016   \n",
              "1998-06-02               0.037892               0.038099    0.903751   \n",
              "\n",
              "            Close100(t-0)  marketrisk_avg90(t-0)  marketrisk_avg30(t-0)  \\\n",
              "Date                                                                      \n",
              "1998-05-27       0.915352               0.036533               0.036330   \n",
              "1998-05-28       0.915163               0.036895               0.035828   \n",
              "1998-05-29       0.915000               0.037338               0.036237   \n",
              "1998-06-01       0.914801               0.037892               0.038099   \n",
              "1998-06-02       0.914626               0.038163               0.038561   \n",
              "\n",
              "              Change  Signal  \n",
              "Date                          \n",
              "1998-05-27 -0.001898   False  \n",
              "1998-05-28  0.003719    True  \n",
              "1998-05-29 -0.004515   False  \n",
              "1998-06-01  0.000813    True  \n",
              "1998-06-02 -0.005840   False  \n",
              "\n",
              "[5 rows x 22 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "9ar94vy_ip_7",
        "colab_type": "code",
        "outputId": "febe6cf6-5cdc-4b75-f9d6-992fbc0c140e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "cell_type": "code",
      "source": [
        "#Remove bad training and validating examples\n",
        "SetAdjusted = Set.copy()\n",
        "# SetAdjusted = Set.loc[(Set.Change< -0.0015)| (Set.Change> 0.003)]\n",
        "SetAdjusted.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Close(t-4)</th>\n",
              "      <th>Close100(t-4)</th>\n",
              "      <th>marketrisk_avg90(t-4)</th>\n",
              "      <th>marketrisk_avg30(t-4)</th>\n",
              "      <th>Close(t-3)</th>\n",
              "      <th>Close100(t-3)</th>\n",
              "      <th>marketrisk_avg90(t-3)</th>\n",
              "      <th>marketrisk_avg30(t-3)</th>\n",
              "      <th>Close(t-2)</th>\n",
              "      <th>Close100(t-2)</th>\n",
              "      <th>...</th>\n",
              "      <th>Close(t-1)</th>\n",
              "      <th>Close100(t-1)</th>\n",
              "      <th>marketrisk_avg90(t-1)</th>\n",
              "      <th>marketrisk_avg30(t-1)</th>\n",
              "      <th>Close(t-0)</th>\n",
              "      <th>Close100(t-0)</th>\n",
              "      <th>marketrisk_avg90(t-0)</th>\n",
              "      <th>marketrisk_avg30(t-0)</th>\n",
              "      <th>Change</th>\n",
              "      <th>Signal</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1998-05-27</th>\n",
              "      <td>0.894694</td>\n",
              "      <td>0.916241</td>\n",
              "      <td>0.034995</td>\n",
              "      <td>0.030184</td>\n",
              "      <td>0.893256</td>\n",
              "      <td>0.916045</td>\n",
              "      <td>0.035283</td>\n",
              "      <td>0.030500</td>\n",
              "      <td>0.898069</td>\n",
              "      <td>0.915792</td>\n",
              "      <td>...</td>\n",
              "      <td>0.899442</td>\n",
              "      <td>0.915530</td>\n",
              "      <td>0.036311</td>\n",
              "      <td>0.034618</td>\n",
              "      <td>0.905469</td>\n",
              "      <td>0.915352</td>\n",
              "      <td>0.036533</td>\n",
              "      <td>0.036330</td>\n",
              "      <td>-0.001898</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998-05-28</th>\n",
              "      <td>0.893256</td>\n",
              "      <td>0.916045</td>\n",
              "      <td>0.035283</td>\n",
              "      <td>0.030500</td>\n",
              "      <td>0.898069</td>\n",
              "      <td>0.915792</td>\n",
              "      <td>0.035543</td>\n",
              "      <td>0.031202</td>\n",
              "      <td>0.899442</td>\n",
              "      <td>0.915530</td>\n",
              "      <td>...</td>\n",
              "      <td>0.905469</td>\n",
              "      <td>0.915352</td>\n",
              "      <td>0.036533</td>\n",
              "      <td>0.036330</td>\n",
              "      <td>0.903751</td>\n",
              "      <td>0.915163</td>\n",
              "      <td>0.036895</td>\n",
              "      <td>0.035828</td>\n",
              "      <td>0.003719</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998-05-29</th>\n",
              "      <td>0.898069</td>\n",
              "      <td>0.915792</td>\n",
              "      <td>0.035543</td>\n",
              "      <td>0.031202</td>\n",
              "      <td>0.899442</td>\n",
              "      <td>0.915530</td>\n",
              "      <td>0.036311</td>\n",
              "      <td>0.034618</td>\n",
              "      <td>0.905469</td>\n",
              "      <td>0.915352</td>\n",
              "      <td>...</td>\n",
              "      <td>0.903751</td>\n",
              "      <td>0.915163</td>\n",
              "      <td>0.036895</td>\n",
              "      <td>0.035828</td>\n",
              "      <td>0.907112</td>\n",
              "      <td>0.915000</td>\n",
              "      <td>0.037338</td>\n",
              "      <td>0.036237</td>\n",
              "      <td>-0.004515</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998-06-01</th>\n",
              "      <td>0.899442</td>\n",
              "      <td>0.915530</td>\n",
              "      <td>0.036311</td>\n",
              "      <td>0.034618</td>\n",
              "      <td>0.905469</td>\n",
              "      <td>0.915352</td>\n",
              "      <td>0.036533</td>\n",
              "      <td>0.036330</td>\n",
              "      <td>0.903751</td>\n",
              "      <td>0.915163</td>\n",
              "      <td>...</td>\n",
              "      <td>0.907112</td>\n",
              "      <td>0.915000</td>\n",
              "      <td>0.037338</td>\n",
              "      <td>0.036237</td>\n",
              "      <td>0.903016</td>\n",
              "      <td>0.914801</td>\n",
              "      <td>0.037892</td>\n",
              "      <td>0.038099</td>\n",
              "      <td>0.000813</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998-06-02</th>\n",
              "      <td>0.905469</td>\n",
              "      <td>0.915352</td>\n",
              "      <td>0.036533</td>\n",
              "      <td>0.036330</td>\n",
              "      <td>0.903751</td>\n",
              "      <td>0.915163</td>\n",
              "      <td>0.036895</td>\n",
              "      <td>0.035828</td>\n",
              "      <td>0.907112</td>\n",
              "      <td>0.915000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.903016</td>\n",
              "      <td>0.914801</td>\n",
              "      <td>0.037892</td>\n",
              "      <td>0.038099</td>\n",
              "      <td>0.903751</td>\n",
              "      <td>0.914626</td>\n",
              "      <td>0.038163</td>\n",
              "      <td>0.038561</td>\n",
              "      <td>-0.005840</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 22 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Close(t-4)  Close100(t-4)  marketrisk_avg90(t-4)  \\\n",
              "Date                                                           \n",
              "1998-05-27    0.894694       0.916241               0.034995   \n",
              "1998-05-28    0.893256       0.916045               0.035283   \n",
              "1998-05-29    0.898069       0.915792               0.035543   \n",
              "1998-06-01    0.899442       0.915530               0.036311   \n",
              "1998-06-02    0.905469       0.915352               0.036533   \n",
              "\n",
              "            marketrisk_avg30(t-4)  Close(t-3)  Close100(t-3)  \\\n",
              "Date                                                           \n",
              "1998-05-27               0.030184    0.893256       0.916045   \n",
              "1998-05-28               0.030500    0.898069       0.915792   \n",
              "1998-05-29               0.031202    0.899442       0.915530   \n",
              "1998-06-01               0.034618    0.905469       0.915352   \n",
              "1998-06-02               0.036330    0.903751       0.915163   \n",
              "\n",
              "            marketrisk_avg90(t-3)  marketrisk_avg30(t-3)  Close(t-2)  \\\n",
              "Date                                                                   \n",
              "1998-05-27               0.035283               0.030500    0.898069   \n",
              "1998-05-28               0.035543               0.031202    0.899442   \n",
              "1998-05-29               0.036311               0.034618    0.905469   \n",
              "1998-06-01               0.036533               0.036330    0.903751   \n",
              "1998-06-02               0.036895               0.035828    0.907112   \n",
              "\n",
              "            Close100(t-2)   ...    Close(t-1)  Close100(t-1)  \\\n",
              "Date                        ...                                \n",
              "1998-05-27       0.915792   ...      0.899442       0.915530   \n",
              "1998-05-28       0.915530   ...      0.905469       0.915352   \n",
              "1998-05-29       0.915352   ...      0.903751       0.915163   \n",
              "1998-06-01       0.915163   ...      0.907112       0.915000   \n",
              "1998-06-02       0.915000   ...      0.903016       0.914801   \n",
              "\n",
              "            marketrisk_avg90(t-1)  marketrisk_avg30(t-1)  Close(t-0)  \\\n",
              "Date                                                                   \n",
              "1998-05-27               0.036311               0.034618    0.905469   \n",
              "1998-05-28               0.036533               0.036330    0.903751   \n",
              "1998-05-29               0.036895               0.035828    0.907112   \n",
              "1998-06-01               0.037338               0.036237    0.903016   \n",
              "1998-06-02               0.037892               0.038099    0.903751   \n",
              "\n",
              "            Close100(t-0)  marketrisk_avg90(t-0)  marketrisk_avg30(t-0)  \\\n",
              "Date                                                                      \n",
              "1998-05-27       0.915352               0.036533               0.036330   \n",
              "1998-05-28       0.915163               0.036895               0.035828   \n",
              "1998-05-29       0.915000               0.037338               0.036237   \n",
              "1998-06-01       0.914801               0.037892               0.038099   \n",
              "1998-06-02       0.914626               0.038163               0.038561   \n",
              "\n",
              "              Change  Signal  \n",
              "Date                          \n",
              "1998-05-27 -0.001898   False  \n",
              "1998-05-28  0.003719    True  \n",
              "1998-05-29 -0.004515   False  \n",
              "1998-06-01  0.000813    True  \n",
              "1998-06-02 -0.005840   False  \n",
              "\n",
              "[5 rows x 22 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Ovn_3v_G1Xuq"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.2 Train-Validate-Test Split"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "JvOKmcgM1da_",
        "outputId": "167e34a5-1b0a-4408-f159-c666805ff6ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82662
        }
      },
      "cell_type": "code",
      "source": [
        "daterange = pd.date_range(start='1/1/2017', periods=13, freq='1W')\n",
        "print(daterange)\n",
        "tradeTable= Ana.loc[\"2017-01-01\":\"2018-01-01\",:].copy()\n",
        "tradeTable[\"predictSignal\"] = 0\n",
        "# tradeTable.head()\n",
        "\n",
        "for single_date in daterange:\n",
        "  TRAIN_START = (single_date - timedelta(days=90)).strftime(\"%Y-%m-%d\")\n",
        "  TRAIN_END = (single_date - timedelta(days=1)).strftime(\"%Y-%m-%d\") \n",
        "  TEST_START = (single_date).strftime(\"%Y-%m-%d\")\n",
        "  TEST_END = (single_date + timedelta(days=6)).strftime(\"%Y-%m-%d\")\n",
        "  print(TEST_START)\n",
        "  print(TEST_END)\n",
        "\n",
        "  #Set x and y variables\n",
        "  FIRSTTRAINVAR = SetAdjusted.columns[0]\n",
        "  print(FIRSTTRAINVAR)\n",
        "  LASTTRAINVARIABLE = SetAdjusted.columns[-3]\n",
        "  print(LASTTRAINVARIABLE)\n",
        "  YVAR= \"Signal\"\n",
        "\n",
        "#   #Split train and test set at TRAIN_DATE_END and TEST_DATE_START\n",
        "#   TRAIN_START = \"2017-01-01\"\n",
        "#   TRAIN_END = \"2017-03-31\"\n",
        "#   # VALIDATE_START = \"2018-01-01\"\n",
        "#   # VALIDATE_END = \"2018-05-31\"\n",
        "#   TEST_START = \"2017-01-01\"\n",
        "#   TEST_END = \"2017-04-30\"\n",
        "\n",
        "  #Specify LSTM input shape\n",
        "  X_SHAPE1 = 5\n",
        "  X_SHAPE2 = FEATURES_SHAPE\n",
        "\n",
        "  #train_x\n",
        "  train_x = SetAdjusted.loc[TRAIN_START:TRAIN_END, FIRSTTRAINVAR:LASTTRAINVARIABLE]\n",
        "  # Normalised train_x to [0,1]\n",
        "  # print(train_x.head())\n",
        "  scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "  train_x = scaler.fit_transform(train_x)\n",
        "#   print(train_x.shape)\n",
        "  #reshape train_x for RNN input\n",
        "  train_x = train_x.reshape(train_x.shape[0], X_SHAPE1, X_SHAPE2)\n",
        "#   print(train_x.shape)\n",
        "\n",
        "  #train_y\n",
        "  train_y = SetAdjusted.loc[TRAIN_START:TRAIN_END, YVAR]\n",
        "  train_y = train_y.values.reshape((train_y.shape[0], 1))\n",
        "  yscaler = MinMaxScaler(feature_range=(0, 1))\n",
        "  # train_y = yscaler.fit_transform(train_y)\n",
        "#   print(train_y.shape)\n",
        "\n",
        "  #test_x\n",
        "  test_x = Set.loc[TEST_START:TEST_END, FIRSTTRAINVAR:LASTTRAINVARIABLE]\n",
        "  # Normalised train_x to [0,1]\n",
        "  test_x = scaler.transform(test_x)\n",
        "  #reshape test_x for RNN input\n",
        "  test_x = test_x.reshape(test_x.shape[0], X_SHAPE1, X_SHAPE2)\n",
        "#   print(test_x.shape)\n",
        "\n",
        "  #test_y\n",
        "  test_y = Set.loc[TEST_START:TEST_END, YVAR]\n",
        "  test_y = test_y.values.reshape((test_y.shape[0], 1))\n",
        "  # test_y = yscaler.transform(test_y)\n",
        "\n",
        "#   print(test_y.shape)\n",
        "\n",
        "\n",
        "  TESTEND = test_x.shape[0]\n",
        "  # TESTSTART = train_x.shape[0] + validate_x.shape[0]\n",
        "  TESTSTART = train_x.shape[0]\n",
        "  # VALIDATESTART = train_x.shape[0]\n",
        "  print(TESTSTART)\n",
        "  \n",
        "  ## Specifying Batch Size\n",
        "  n_batch = 2\n",
        "  \n",
        "  ## Only to use when removing model \n",
        "  !rm best.h5\n",
        "  \n",
        "  #Check train_y statistic\n",
        "  trainStats = np.unique(train_y, return_counts=True)\n",
        "  print(trainStats)\n",
        "\n",
        "  #Set Classweight\n",
        "  if trainStats[1][0] > trainStats[1][1]: \n",
        "    classweight = {0:1, 1:trainStats[1][0]/trainStats[1][1]}\n",
        "  else:\n",
        "    classweight = {0:trainStats[1][1]/trainStats[1][0], 1:1}\n",
        "\n",
        "  print(classweight)\n",
        "\n",
        "  units = 20\n",
        "  inputs = Input(shape=(train_x.shape[1], train_x.shape[2]))\n",
        "  secondary = LSTM(units, return_sequences=True)(inputs)\n",
        "  activations = LSTM(units, return_sequences=True)(secondary)\n",
        "  # out = Dense(1, activation='sigmoid')(activations)\n",
        "\n",
        "\n",
        "  attention = Dense(1, activation='tanh')(activations)\n",
        "  attention = Flatten()(attention)\n",
        "  attention = Activation('softmax')(attention)\n",
        "  attention = RepeatVector(units)(attention)\n",
        "  attention = Permute([2, 1])(attention)\n",
        "\n",
        "\n",
        "  sent_representation = multiply([activations, attention])\n",
        "  sent_representation = Lambda(lambda xin: K.sum(xin, axis=-2), output_shape=(units,))(sent_representation)\n",
        "\n",
        "  probabilities = Dense(1, activation='sigmoid')(sent_representation)\n",
        "\n",
        "\n",
        "  model = Model(inputs=inputs, outputs=probabilities)\n",
        "  model.compile(optimizer='rmsprop',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=[\"binary_accuracy\"])\n",
        "\n",
        "  # Checkpoint\n",
        "  checkpoint = ModelCheckpoint(\"best.h5\", monitor='binary_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='max')\n",
        "  early = EarlyStopping(monitor='binary_accuracy', min_delta=0, patience=40, verbose=0, mode='auto')\n",
        "  callbacks_list = [checkpoint]\n",
        "\n",
        "  # Fit model\n",
        "  if os.path.isfile(\"best.h5\"):\n",
        "    model.load_weights(\"best.h5\")\n",
        "\n",
        "  history = model.fit(train_x, train_y, epochs=300,  batch_size=n_batch, callbacks=[checkpoint, early],\n",
        "#                       validation_data=(validate_x, validate_y), \n",
        "                      shuffle=False, verbose =  1,\n",
        "                        class_weight= classweight\n",
        "                        )\n",
        "  \n",
        "  # Make predictions, yhat, on test set\n",
        "  if os.path.isfile(\"best.h5\"):\n",
        "    model.load_weights(\"best.h5\")\n",
        "  yhat = model.predict(test_x, batch_size=n_batch)\n",
        "  # yhat = yscaler.inverse_transform(yhat)\n",
        "\n",
        "  print(yhat.shape)\n",
        "\n",
        "  ## Reading yhat as predictSignal\n",
        "  tradeTable.loc[TEST_START:TEST_END, \"predictSignal\"] = yhat>0.5\n",
        "  # tradeTable[\"predictPrice\"] = yhat\n",
        "  # tradeTable[\"predictSignal\"]= tradeTable[\"predictPrice\"]>0\n",
        "#   tradeTable.head()\n",
        "\n",
        "#Changing Signals to Weights\n",
        "tradeTable['predictedweight'] = np.where(tradeTable['predictSignal'] == True, 1, -1)\n",
        "tradeTable['labelledweight'] = np.where(tradeTable['Signal'] == True, 1, -1)\n",
        "tradeTable.head()\n",
        "\n",
        "# calculating r, %tage change of closing price over previous time step\n",
        "tradeTable['r'] = (tradeTable['Close']- tradeTable['Close'].shift(1))/tradeTable['Close'].shift(1)\n",
        "tradeTable.head()\n",
        "\n",
        "# calculating predicted and labelled trade returns over each time step\n",
        "tradeTable['predictedreturns'] = tradeTable['r']*tradeTable['predictedweight'].shift(1)\n",
        "tradeTable['labelledreturns'] = tradeTable['r']*tradeTable['labelledweight'].shift(1)\n",
        "\n",
        "tradeTable\n",
        "\n",
        "# # calculating cumulative predicted and labelled trade returns over each time step\n",
        "# TrainTable =  tradeTable.loc[TRAIN_START:TRAIN_END,:].copy()\n",
        "# TrainTable[\"labelledPNL\"] =  (1+TrainTable['labelledreturns']).cumprod()\n",
        "# TrainTable[\"PNL\"] = (1+TrainTable['predictedreturns']).cumprod()\n",
        "# TrainTable.head()\n",
        "\n",
        "# # # calculating cumulative predicted and labelled trade returns over each time step\n",
        "# # ValTable =  tradeTable.loc[VALIDATE_START:VALIDATE_END,:].copy()\n",
        "# # ValTable[\"labelledPNL\"] =  (1+ValTable['labelledreturns']).cumprod()\n",
        "# # ValTable[\"PNL\"] = (1+ValTable['predictedreturns']).cumprod()\n",
        "# # ValTable.head()\n",
        "\n",
        "# # calculating cumulative predicted and labelled trade returns over each time step\n",
        "# PNLTable =  tradeTable.loc[TEST_START:TEST_END,:].copy()\n",
        "# PNLTable[\"labelledPNL\"] =  (1+PNLTable['labelledreturns']).cumprod()\n",
        "# PNLTable[\"PNL\"] = (1+PNLTable['predictedreturns']).cumprod()\n",
        "# PNLTable.head()\n",
        "\n",
        "# #plotting Confusion Matrix\n",
        "# f = plt.figure(figsize=(15,7))\n",
        "# ax = f.add_subplot(131)\n",
        "# plotConfusion(TRAIN_START, TRAIN_END, \"Train\")\n",
        "\n",
        "# #   plt.subplot(132)\n",
        "# #   plotConfusion(VALIDATE_START, VALIDATE_END, \"Validate\")\n",
        "\n",
        "# plt.subplot(133)\n",
        "# plotConfusion(TEST_START, TEST_END, \"Test\")\n",
        "\n",
        "# plt.show()\n",
        "\n",
        "# f = plt.figure(figsize=(36,6))\n",
        "# ax = f.add_subplot(131)\n",
        "\n",
        "# # PNL Plot for the Test Period\n",
        "# PNLTable[\"PNL\"].plot()\n",
        "\n",
        "# # PNL Plot for the Test Period\n",
        "# TrainTable[\"PNL\"].plot()\n",
        "\n",
        "# plt.show()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DatetimeIndex(['2017-01-01', '2017-01-08', '2017-01-15', '2017-01-22',\n",
            "               '2017-01-29', '2017-02-05', '2017-02-12', '2017-02-19',\n",
            "               '2017-02-26', '2017-03-05', '2017-03-12', '2017-03-19',\n",
            "               '2017-03-26'],\n",
            "              dtype='datetime64[ns]', freq='W-SUN')\n",
            "2017-01-01\n",
            "2017-01-07\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([27, 38]))\n",
            "{0: 1.4074074074074074, 1: 1}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 8s 121ms/step - loss: 0.8137 - binary_accuracy: 0.4154\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.41538, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8123 - binary_accuracy: 0.4000\n",
            "\n",
            "Epoch 00002: binary_accuracy did not improve from 0.41538\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8119 - binary_accuracy: 0.4154\n",
            "\n",
            "Epoch 00003: binary_accuracy did not improve from 0.41538\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8115 - binary_accuracy: 0.4615\n",
            "\n",
            "Epoch 00004: binary_accuracy improved from 0.41538 to 0.46154, saving model to best.h5\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8112 - binary_accuracy: 0.4615\n",
            "\n",
            "Epoch 00005: binary_accuracy did not improve from 0.46154\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8109 - binary_accuracy: 0.4769\n",
            "\n",
            "Epoch 00006: binary_accuracy improved from 0.46154 to 0.47692, saving model to best.h5\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8106 - binary_accuracy: 0.4615\n",
            "\n",
            "Epoch 00007: binary_accuracy did not improve from 0.47692\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8103 - binary_accuracy: 0.4462\n",
            "\n",
            "Epoch 00008: binary_accuracy did not improve from 0.47692\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8100 - binary_accuracy: 0.4615\n",
            "\n",
            "Epoch 00009: binary_accuracy did not improve from 0.47692\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8097 - binary_accuracy: 0.4769\n",
            "\n",
            "Epoch 00010: binary_accuracy did not improve from 0.47692\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8094 - binary_accuracy: 0.4769\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.47692\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8091 - binary_accuracy: 0.5077\n",
            "\n",
            "Epoch 00012: binary_accuracy improved from 0.47692 to 0.50769, saving model to best.h5\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8087 - binary_accuracy: 0.4769\n",
            "\n",
            "Epoch 00013: binary_accuracy did not improve from 0.50769\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8084 - binary_accuracy: 0.4769\n",
            "\n",
            "Epoch 00014: binary_accuracy did not improve from 0.50769\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8081 - binary_accuracy: 0.4769\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.50769\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8077 - binary_accuracy: 0.4769\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.50769\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8074 - binary_accuracy: 0.4769\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.50769\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8070 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.50769\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8066 - binary_accuracy: 0.5077\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.50769\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8062 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00020: binary_accuracy improved from 0.50769 to 0.52308, saving model to best.h5\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8059 - binary_accuracy: 0.5077\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.52308\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8055 - binary_accuracy: 0.5077\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.52308\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8051 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00023: binary_accuracy improved from 0.52308 to 0.55385, saving model to best.h5\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8046 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.55385\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8042 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00025: binary_accuracy improved from 0.55385 to 0.56923, saving model to best.h5\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8038 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00026: binary_accuracy improved from 0.56923 to 0.58462, saving model to best.h5\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8034 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.58462\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8029 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.58462\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8025 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.58462\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8021 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.58462\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8016 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.58462\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8012 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.58462\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8008 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.58462\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8003 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.58462\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7999 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.58462\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7995 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.58462\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7991 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.58462\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7987 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.58462\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7983 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.58462\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7979 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00040: binary_accuracy improved from 0.58462 to 0.60000, saving model to best.h5\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7975 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.60000\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7971 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.60000\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7968 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.60000\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7964 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.60000\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7961 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.60000\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7957 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.60000\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7954 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.60000\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7951 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.60000\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7948 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.60000\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7944 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.60000\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7941 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00051: binary_accuracy did not improve from 0.60000\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7938 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.60000\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7935 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00053: binary_accuracy did not improve from 0.60000\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7932 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.60000\n",
            "Epoch 55/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7929 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00055: binary_accuracy did not improve from 0.60000\n",
            "Epoch 56/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7926 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00056: binary_accuracy did not improve from 0.60000\n",
            "Epoch 57/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7923 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00057: binary_accuracy did not improve from 0.60000\n",
            "Epoch 58/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7921 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00058: binary_accuracy did not improve from 0.60000\n",
            "Epoch 59/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7918 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00059: binary_accuracy did not improve from 0.60000\n",
            "Epoch 60/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7915 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00060: binary_accuracy did not improve from 0.60000\n",
            "Epoch 61/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7912 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00061: binary_accuracy did not improve from 0.60000\n",
            "Epoch 62/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7909 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00062: binary_accuracy did not improve from 0.60000\n",
            "Epoch 63/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7907 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00063: binary_accuracy did not improve from 0.60000\n",
            "Epoch 64/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7904 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00064: binary_accuracy did not improve from 0.60000\n",
            "Epoch 65/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7901 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00065: binary_accuracy did not improve from 0.60000\n",
            "Epoch 66/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7898 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00066: binary_accuracy did not improve from 0.60000\n",
            "Epoch 67/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7895 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00067: binary_accuracy did not improve from 0.60000\n",
            "Epoch 68/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7892 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00068: binary_accuracy did not improve from 0.60000\n",
            "Epoch 69/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7889 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00069: binary_accuracy did not improve from 0.60000\n",
            "Epoch 70/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7886 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00070: binary_accuracy did not improve from 0.60000\n",
            "Epoch 71/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7882 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00071: binary_accuracy did not improve from 0.60000\n",
            "Epoch 72/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7879 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00072: binary_accuracy did not improve from 0.60000\n",
            "Epoch 73/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7876 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00073: binary_accuracy did not improve from 0.60000\n",
            "Epoch 74/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7872 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00074: binary_accuracy did not improve from 0.60000\n",
            "Epoch 75/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7868 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00075: binary_accuracy did not improve from 0.60000\n",
            "Epoch 76/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7865 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00076: binary_accuracy did not improve from 0.60000\n",
            "Epoch 77/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7861 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00077: binary_accuracy did not improve from 0.60000\n",
            "Epoch 78/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7857 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00078: binary_accuracy did not improve from 0.60000\n",
            "Epoch 79/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7853 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00079: binary_accuracy did not improve from 0.60000\n",
            "Epoch 80/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7849 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00080: binary_accuracy did not improve from 0.60000\n",
            "(5, 1)\n",
            "2017-01-08\n",
            "2017-01-14\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([28, 37]))\n",
            "{0: 1.3214285714285714, 1: 1}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 8s 122ms/step - loss: 0.7906 - binary_accuracy: 0.4769\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.47692, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7886 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00002: binary_accuracy improved from 0.47692 to 0.49231, saving model to best.h5\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7876 - binary_accuracy: 0.4769\n",
            "\n",
            "Epoch 00003: binary_accuracy did not improve from 0.49231\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7868 - binary_accuracy: 0.4615\n",
            "\n",
            "Epoch 00004: binary_accuracy did not improve from 0.49231\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7861 - binary_accuracy: 0.4615\n",
            "\n",
            "Epoch 00005: binary_accuracy did not improve from 0.49231\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7853 - binary_accuracy: 0.4769\n",
            "\n",
            "Epoch 00006: binary_accuracy did not improve from 0.49231\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7846 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00007: binary_accuracy did not improve from 0.49231\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7838 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00008: binary_accuracy did not improve from 0.49231\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7830 - binary_accuracy: 0.5077\n",
            "\n",
            "Epoch 00009: binary_accuracy improved from 0.49231 to 0.50769, saving model to best.h5\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7822 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00010: binary_accuracy did not improve from 0.50769\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7813 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.50769\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7803 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00012: binary_accuracy did not improve from 0.50769\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7794 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00013: binary_accuracy did not improve from 0.50769\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7784 - binary_accuracy: 0.5077\n",
            "\n",
            "Epoch 00014: binary_accuracy did not improve from 0.50769\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7773 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.50769\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7763 - binary_accuracy: 0.5077\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.50769\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7752 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00017: binary_accuracy improved from 0.50769 to 0.53846, saving model to best.h5\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7742 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.53846\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7732 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00019: binary_accuracy improved from 0.53846 to 0.56923, saving model to best.h5\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7721 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.56923\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7712 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00021: binary_accuracy improved from 0.56923 to 0.60000, saving model to best.h5\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7702 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.60000\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7692 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.60000\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7684 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.60000\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7675 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.60000\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7667 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.60000\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7659 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.60000\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7652 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.60000\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7645 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.60000\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7638 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.60000\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7632 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.60000\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7626 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.60000\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7620 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.60000\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7614 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.60000\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7609 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.60000\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7603 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.60000\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7598 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.60000\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7593 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.60000\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7588 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.60000\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7582 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.60000\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7577 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.60000\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7572 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.60000\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7567 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.60000\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7562 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.60000\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7557 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.60000\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7552 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.60000\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7547 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.60000\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7542 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.60000\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7537 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.60000\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7532 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.60000\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7527 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00051: binary_accuracy did not improve from 0.60000\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7522 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.60000\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7517 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00053: binary_accuracy did not improve from 0.60000\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7512 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.60000\n",
            "Epoch 55/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7507 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00055: binary_accuracy did not improve from 0.60000\n",
            "Epoch 56/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7502 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00056: binary_accuracy did not improve from 0.60000\n",
            "Epoch 57/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7498 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00057: binary_accuracy did not improve from 0.60000\n",
            "Epoch 58/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7493 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00058: binary_accuracy did not improve from 0.60000\n",
            "Epoch 59/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7489 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00059: binary_accuracy improved from 0.60000 to 0.61538, saving model to best.h5\n",
            "Epoch 60/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7484 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00060: binary_accuracy did not improve from 0.61538\n",
            "Epoch 61/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7480 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00061: binary_accuracy did not improve from 0.61538\n",
            "Epoch 62/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7476 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00062: binary_accuracy did not improve from 0.61538\n",
            "Epoch 63/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7472 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00063: binary_accuracy did not improve from 0.61538\n",
            "Epoch 64/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7468 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00064: binary_accuracy did not improve from 0.61538\n",
            "Epoch 65/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7464 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00065: binary_accuracy did not improve from 0.61538\n",
            "Epoch 66/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7460 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00066: binary_accuracy did not improve from 0.61538\n",
            "Epoch 67/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7456 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00067: binary_accuracy did not improve from 0.61538\n",
            "Epoch 68/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7452 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00068: binary_accuracy did not improve from 0.61538\n",
            "Epoch 69/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7448 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00069: binary_accuracy did not improve from 0.61538\n",
            "Epoch 70/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7444 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00070: binary_accuracy did not improve from 0.61538\n",
            "Epoch 71/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7440 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00071: binary_accuracy improved from 0.61538 to 0.64615, saving model to best.h5\n",
            "Epoch 72/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7437 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00072: binary_accuracy did not improve from 0.64615\n",
            "Epoch 73/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7433 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00073: binary_accuracy did not improve from 0.64615\n",
            "Epoch 74/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7429 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00074: binary_accuracy did not improve from 0.64615\n",
            "Epoch 75/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7425 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00075: binary_accuracy did not improve from 0.64615\n",
            "Epoch 76/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7421 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00076: binary_accuracy did not improve from 0.64615\n",
            "Epoch 77/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7417 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00077: binary_accuracy did not improve from 0.64615\n",
            "Epoch 78/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7412 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00078: binary_accuracy did not improve from 0.64615\n",
            "Epoch 79/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7408 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00079: binary_accuracy did not improve from 0.64615\n",
            "Epoch 80/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7404 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00080: binary_accuracy did not improve from 0.64615\n",
            "Epoch 81/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7399 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00081: binary_accuracy did not improve from 0.64615\n",
            "Epoch 82/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7395 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00082: binary_accuracy did not improve from 0.64615\n",
            "Epoch 83/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7390 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00083: binary_accuracy did not improve from 0.64615\n",
            "Epoch 84/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7386 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00084: binary_accuracy did not improve from 0.64615\n",
            "Epoch 85/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7381 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00085: binary_accuracy did not improve from 0.64615\n",
            "Epoch 86/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7376 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00086: binary_accuracy did not improve from 0.64615\n",
            "Epoch 87/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7371 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00087: binary_accuracy did not improve from 0.64615\n",
            "Epoch 88/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7366 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00088: binary_accuracy did not improve from 0.64615\n",
            "Epoch 89/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7361 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00089: binary_accuracy did not improve from 0.64615\n",
            "Epoch 90/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7356 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00090: binary_accuracy did not improve from 0.64615\n",
            "Epoch 91/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7351 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00091: binary_accuracy did not improve from 0.64615\n",
            "Epoch 92/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7346 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00092: binary_accuracy did not improve from 0.64615\n",
            "Epoch 93/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7341 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00093: binary_accuracy did not improve from 0.64615\n",
            "Epoch 94/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7335 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00094: binary_accuracy did not improve from 0.64615\n",
            "Epoch 95/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7330 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00095: binary_accuracy did not improve from 0.64615\n",
            "Epoch 96/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7324 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00096: binary_accuracy did not improve from 0.64615\n",
            "Epoch 97/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7319 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00097: binary_accuracy did not improve from 0.64615\n",
            "Epoch 98/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7313 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00098: binary_accuracy did not improve from 0.64615\n",
            "Epoch 99/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7307 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00099: binary_accuracy did not improve from 0.64615\n",
            "Epoch 100/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7302 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00100: binary_accuracy did not improve from 0.64615\n",
            "Epoch 101/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7296 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00101: binary_accuracy did not improve from 0.64615\n",
            "Epoch 102/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7290 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00102: binary_accuracy did not improve from 0.64615\n",
            "Epoch 103/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7284 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00103: binary_accuracy did not improve from 0.64615\n",
            "Epoch 104/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7279 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00104: binary_accuracy did not improve from 0.64615\n",
            "Epoch 105/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7273 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00105: binary_accuracy did not improve from 0.64615\n",
            "Epoch 106/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7267 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00106: binary_accuracy did not improve from 0.64615\n",
            "Epoch 107/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7261 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00107: binary_accuracy did not improve from 0.64615\n",
            "Epoch 108/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7255 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00108: binary_accuracy did not improve from 0.64615\n",
            "Epoch 109/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7250 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00109: binary_accuracy did not improve from 0.64615\n",
            "Epoch 110/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7244 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00110: binary_accuracy did not improve from 0.64615\n",
            "Epoch 111/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7238 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00111: binary_accuracy did not improve from 0.64615\n",
            "(5, 1)\n",
            "2017-01-15\n",
            "2017-01-21\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([29, 36]))\n",
            "{0: 1.2413793103448276, 1: 1}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 8s 128ms/step - loss: 0.7738 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.52308, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7709 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00002: binary_accuracy improved from 0.52308 to 0.53846, saving model to best.h5\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7697 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00003: binary_accuracy did not improve from 0.53846\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7688 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00004: binary_accuracy did not improve from 0.53846\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7681 - binary_accuracy: 0.4769\n",
            "\n",
            "Epoch 00005: binary_accuracy did not improve from 0.53846\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7674 - binary_accuracy: 0.4615\n",
            "\n",
            "Epoch 00006: binary_accuracy did not improve from 0.53846\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7668 - binary_accuracy: 0.4615\n",
            "\n",
            "Epoch 00007: binary_accuracy did not improve from 0.53846\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7663 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00008: binary_accuracy did not improve from 0.53846\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7658 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00009: binary_accuracy did not improve from 0.53846\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7652 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00010: binary_accuracy did not improve from 0.53846\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7647 - binary_accuracy: 0.5077\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.53846\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7642 - binary_accuracy: 0.5077\n",
            "\n",
            "Epoch 00012: binary_accuracy did not improve from 0.53846\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7636 - binary_accuracy: 0.5077\n",
            "\n",
            "Epoch 00013: binary_accuracy did not improve from 0.53846\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7630 - binary_accuracy: 0.5077\n",
            "\n",
            "Epoch 00014: binary_accuracy did not improve from 0.53846\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7623 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00015: binary_accuracy improved from 0.53846 to 0.55385, saving model to best.h5\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7617 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.55385\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7609 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.55385\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7602 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.55385\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7593 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.55385\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7584 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.55385\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7574 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00021: binary_accuracy improved from 0.55385 to 0.58462, saving model to best.h5\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7564 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.58462\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7553 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.58462\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7541 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.58462\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7529 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.58462\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7516 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00026: binary_accuracy improved from 0.58462 to 0.60000, saving model to best.h5\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7502 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.60000\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7488 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.60000\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7473 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.60000\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7458 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.60000\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7443 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.60000\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7428 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.60000\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7414 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.60000\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7400 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.60000\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7387 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.60000\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7375 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.60000\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7363 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.60000\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7352 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.60000\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7343 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.60000\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7334 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.60000\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7326 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.60000\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7318 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.60000\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7312 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.60000\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7305 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00044: binary_accuracy improved from 0.60000 to 0.61538, saving model to best.h5\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7299 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.61538\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7294 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.61538\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7289 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.61538\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7284 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.61538\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7279 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.61538\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7274 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.61538\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7269 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00051: binary_accuracy improved from 0.61538 to 0.63077, saving model to best.h5\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7264 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.63077\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7259 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00053: binary_accuracy did not improve from 0.63077\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7254 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.63077\n",
            "Epoch 55/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7249 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00055: binary_accuracy did not improve from 0.63077\n",
            "Epoch 56/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7244 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00056: binary_accuracy did not improve from 0.63077\n",
            "Epoch 57/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7239 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00057: binary_accuracy did not improve from 0.63077\n",
            "Epoch 58/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7233 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00058: binary_accuracy did not improve from 0.63077\n",
            "Epoch 59/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7227 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00059: binary_accuracy did not improve from 0.63077\n",
            "Epoch 60/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7221 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00060: binary_accuracy did not improve from 0.63077\n",
            "Epoch 61/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7215 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00061: binary_accuracy did not improve from 0.63077\n",
            "Epoch 62/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7208 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00062: binary_accuracy did not improve from 0.63077\n",
            "Epoch 63/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7201 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00063: binary_accuracy did not improve from 0.63077\n",
            "Epoch 64/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7193 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00064: binary_accuracy did not improve from 0.63077\n",
            "Epoch 65/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7185 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00065: binary_accuracy did not improve from 0.63077\n",
            "Epoch 66/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7177 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00066: binary_accuracy did not improve from 0.63077\n",
            "Epoch 67/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7168 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00067: binary_accuracy did not improve from 0.63077\n",
            "Epoch 68/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7159 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00068: binary_accuracy did not improve from 0.63077\n",
            "Epoch 69/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7149 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00069: binary_accuracy did not improve from 0.63077\n",
            "Epoch 70/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7139 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00070: binary_accuracy did not improve from 0.63077\n",
            "Epoch 71/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7129 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00071: binary_accuracy did not improve from 0.63077\n",
            "Epoch 72/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7118 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00072: binary_accuracy did not improve from 0.63077\n",
            "Epoch 73/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7107 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00073: binary_accuracy did not improve from 0.63077\n",
            "Epoch 74/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7096 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00074: binary_accuracy did not improve from 0.63077\n",
            "Epoch 75/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7085 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00075: binary_accuracy did not improve from 0.63077\n",
            "Epoch 76/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7073 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00076: binary_accuracy did not improve from 0.63077\n",
            "Epoch 77/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7061 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00077: binary_accuracy did not improve from 0.63077\n",
            "Epoch 78/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7050 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00078: binary_accuracy did not improve from 0.63077\n",
            "Epoch 79/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7038 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00079: binary_accuracy did not improve from 0.63077\n",
            "Epoch 80/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7026 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00080: binary_accuracy did not improve from 0.63077\n",
            "Epoch 81/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7014 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00081: binary_accuracy did not improve from 0.63077\n",
            "Epoch 82/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7002 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00082: binary_accuracy did not improve from 0.63077\n",
            "Epoch 83/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6991 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00083: binary_accuracy did not improve from 0.63077\n",
            "Epoch 84/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6980 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00084: binary_accuracy did not improve from 0.63077\n",
            "Epoch 85/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6969 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00085: binary_accuracy did not improve from 0.63077\n",
            "Epoch 86/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6958 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00086: binary_accuracy did not improve from 0.63077\n",
            "Epoch 87/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6947 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00087: binary_accuracy did not improve from 0.63077\n",
            "Epoch 88/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6936 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00088: binary_accuracy did not improve from 0.63077\n",
            "Epoch 89/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6926 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00089: binary_accuracy did not improve from 0.63077\n",
            "Epoch 90/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6915 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00090: binary_accuracy did not improve from 0.63077\n",
            "Epoch 91/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6905 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00091: binary_accuracy did not improve from 0.63077\n",
            "(5, 1)\n",
            "2017-01-22\n",
            "2017-01-28\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([33, 32]))\n",
            "{0: 1, 1: 1.03125}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 9s 135ms/step - loss: 0.7077 - binary_accuracy: 0.4462\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.44615, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7056 - binary_accuracy: 0.4615\n",
            "\n",
            "Epoch 00002: binary_accuracy improved from 0.44615 to 0.46154, saving model to best.h5\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7047 - binary_accuracy: 0.4769\n",
            "\n",
            "Epoch 00003: binary_accuracy improved from 0.46154 to 0.47692, saving model to best.h5\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7039 - binary_accuracy: 0.4769\n",
            "\n",
            "Epoch 00004: binary_accuracy did not improve from 0.47692\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7030 - binary_accuracy: 0.4769\n",
            "\n",
            "Epoch 00005: binary_accuracy did not improve from 0.47692\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7021 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00006: binary_accuracy improved from 0.47692 to 0.49231, saving model to best.h5\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7012 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00007: binary_accuracy did not improve from 0.49231\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7001 - binary_accuracy: 0.5077\n",
            "\n",
            "Epoch 00008: binary_accuracy improved from 0.49231 to 0.50769, saving model to best.h5\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6990 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00009: binary_accuracy improved from 0.50769 to 0.55385, saving model to best.h5\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6977 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00010: binary_accuracy improved from 0.55385 to 0.56923, saving model to best.h5\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6964 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00011: binary_accuracy improved from 0.56923 to 0.58462, saving model to best.h5\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6950 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00012: binary_accuracy did not improve from 0.58462\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6937 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00013: binary_accuracy did not improve from 0.58462\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6923 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00014: binary_accuracy did not improve from 0.58462\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6909 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.58462\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6897 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.58462\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6885 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.58462\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6874 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.58462\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6864 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.58462\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6854 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.58462\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6845 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.58462\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6837 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.58462\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6828 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.58462\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6820 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.58462\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6811 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.58462\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6803 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.58462\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6794 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.58462\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6785 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.58462\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6775 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.58462\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6765 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.58462\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6755 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.58462\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6745 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.58462\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6734 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.58462\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6723 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.58462\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6713 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00035: binary_accuracy improved from 0.58462 to 0.60000, saving model to best.h5\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6702 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.60000\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6691 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.60000\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6680 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.60000\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6669 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.60000\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6659 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.60000\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6648 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.60000\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6639 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00042: binary_accuracy improved from 0.60000 to 0.61538, saving model to best.h5\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6629 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.61538\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6620 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.61538\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6611 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.61538\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6602 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.61538\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6594 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.61538\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6585 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.61538\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6577 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.61538\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6569 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.61538\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6562 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00051: binary_accuracy did not improve from 0.61538\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6554 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.61538\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6547 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00053: binary_accuracy improved from 0.61538 to 0.63077, saving model to best.h5\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6539 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.63077\n",
            "Epoch 55/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6532 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00055: binary_accuracy did not improve from 0.63077\n",
            "Epoch 56/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6526 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00056: binary_accuracy did not improve from 0.63077\n",
            "Epoch 57/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6519 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00057: binary_accuracy did not improve from 0.63077\n",
            "Epoch 58/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6513 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00058: binary_accuracy did not improve from 0.63077\n",
            "Epoch 59/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6507 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00059: binary_accuracy did not improve from 0.63077\n",
            "Epoch 60/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6501 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00060: binary_accuracy did not improve from 0.63077\n",
            "Epoch 61/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6496 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00061: binary_accuracy did not improve from 0.63077\n",
            "Epoch 62/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6490 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00062: binary_accuracy did not improve from 0.63077\n",
            "Epoch 63/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6485 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00063: binary_accuracy did not improve from 0.63077\n",
            "Epoch 64/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6480 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00064: binary_accuracy did not improve from 0.63077\n",
            "Epoch 65/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6475 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00065: binary_accuracy did not improve from 0.63077\n",
            "Epoch 66/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6471 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00066: binary_accuracy did not improve from 0.63077\n",
            "Epoch 67/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6466 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00067: binary_accuracy did not improve from 0.63077\n",
            "Epoch 68/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6461 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00068: binary_accuracy did not improve from 0.63077\n",
            "Epoch 69/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6456 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00069: binary_accuracy did not improve from 0.63077\n",
            "Epoch 70/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6451 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00070: binary_accuracy did not improve from 0.63077\n",
            "Epoch 71/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6446 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00071: binary_accuracy did not improve from 0.63077\n",
            "Epoch 72/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6441 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00072: binary_accuracy did not improve from 0.63077\n",
            "Epoch 73/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6435 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00073: binary_accuracy did not improve from 0.63077\n",
            "Epoch 74/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6430 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00074: binary_accuracy did not improve from 0.63077\n",
            "Epoch 75/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6424 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00075: binary_accuracy did not improve from 0.63077\n",
            "Epoch 76/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6419 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00076: binary_accuracy did not improve from 0.63077\n",
            "Epoch 77/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6413 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00077: binary_accuracy did not improve from 0.63077\n",
            "Epoch 78/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6407 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00078: binary_accuracy did not improve from 0.63077\n",
            "Epoch 79/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6401 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00079: binary_accuracy did not improve from 0.63077\n",
            "Epoch 80/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6396 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00080: binary_accuracy did not improve from 0.63077\n",
            "Epoch 81/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6389 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00081: binary_accuracy did not improve from 0.63077\n",
            "Epoch 82/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6384 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00082: binary_accuracy did not improve from 0.63077\n",
            "Epoch 83/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6378 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00083: binary_accuracy did not improve from 0.63077\n",
            "Epoch 84/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6371 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00084: binary_accuracy did not improve from 0.63077\n",
            "Epoch 85/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6365 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00085: binary_accuracy did not improve from 0.63077\n",
            "Epoch 86/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6359 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00086: binary_accuracy did not improve from 0.63077\n",
            "Epoch 87/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6353 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00087: binary_accuracy did not improve from 0.63077\n",
            "Epoch 88/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6346 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00088: binary_accuracy did not improve from 0.63077\n",
            "Epoch 89/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6340 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00089: binary_accuracy did not improve from 0.63077\n",
            "Epoch 90/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6333 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00090: binary_accuracy did not improve from 0.63077\n",
            "Epoch 91/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6327 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00091: binary_accuracy did not improve from 0.63077\n",
            "Epoch 92/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6323 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00092: binary_accuracy did not improve from 0.63077\n",
            "Epoch 93/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6318 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00093: binary_accuracy did not improve from 0.63077\n",
            "(5, 1)\n",
            "2017-01-29\n",
            "2017-02-04\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([32, 33]))\n",
            "{0: 1.03125, 1: 1}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 9s 136ms/step - loss: 0.7112 - binary_accuracy: 0.4615\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.46154, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7083 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00002: binary_accuracy improved from 0.46154 to 0.49231, saving model to best.h5\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7072 - binary_accuracy: 0.4769\n",
            "\n",
            "Epoch 00003: binary_accuracy did not improve from 0.49231\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7064 - binary_accuracy: 0.4462\n",
            "\n",
            "Epoch 00004: binary_accuracy did not improve from 0.49231\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7057 - binary_accuracy: 0.4154\n",
            "\n",
            "Epoch 00005: binary_accuracy did not improve from 0.49231\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7051 - binary_accuracy: 0.4308\n",
            "\n",
            "Epoch 00006: binary_accuracy did not improve from 0.49231\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7046 - binary_accuracy: 0.4615\n",
            "\n",
            "Epoch 00007: binary_accuracy did not improve from 0.49231\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7041 - binary_accuracy: 0.4769\n",
            "\n",
            "Epoch 00008: binary_accuracy did not improve from 0.49231\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7037 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00009: binary_accuracy did not improve from 0.49231\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7032 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00010: binary_accuracy improved from 0.49231 to 0.56923, saving model to best.h5\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7028 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.56923\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7024 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00012: binary_accuracy did not improve from 0.56923\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7020 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00013: binary_accuracy did not improve from 0.56923\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7016 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00014: binary_accuracy did not improve from 0.56923\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7012 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.56923\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7008 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.56923\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7003 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.56923\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6999 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.56923\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6994 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.56923\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6990 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.56923\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6985 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.56923\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6980 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.56923\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6975 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.56923\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6970 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.56923\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6964 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.56923\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6959 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.56923\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6953 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.56923\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6948 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.56923\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6942 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.56923\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6936 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.56923\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6930 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.56923\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6925 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.56923\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6919 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.56923\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6913 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.56923\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6908 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.56923\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6902 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.56923\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6897 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.56923\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6891 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.56923\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6886 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.56923\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6881 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.56923\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6877 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.56923\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6872 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.56923\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6868 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.56923\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6863 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.56923\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6859 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.56923\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6855 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.56923\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6851 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.56923\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6847 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.56923\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6843 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.56923\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6840 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.56923\n",
            "(5, 1)\n",
            "2017-02-05\n",
            "2017-02-11\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([30, 35]))\n",
            "{0: 1.1666666666666667, 1: 1}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 9s 143ms/step - loss: 0.7479 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.52308, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7438 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00002: binary_accuracy improved from 0.52308 to 0.56923, saving model to best.h5\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7414 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00003: binary_accuracy did not improve from 0.56923\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7392 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00004: binary_accuracy did not improve from 0.56923\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7369 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00005: binary_accuracy did not improve from 0.56923\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7346 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00006: binary_accuracy did not improve from 0.56923\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7320 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00007: binary_accuracy did not improve from 0.56923\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7291 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00008: binary_accuracy did not improve from 0.56923\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7258 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00009: binary_accuracy did not improve from 0.56923\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7219 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00010: binary_accuracy did not improve from 0.56923\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7173 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.56923\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7121 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00012: binary_accuracy did not improve from 0.56923\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7064 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00013: binary_accuracy did not improve from 0.56923\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7006 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00014: binary_accuracy did not improve from 0.56923\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6951 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.56923\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6901 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.56923\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6856 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.56923\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6816 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.56923\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6780 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.56923\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6748 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.56923\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6721 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00021: binary_accuracy improved from 0.56923 to 0.58462, saving model to best.h5\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6699 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.58462\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6681 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.58462\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6665 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.58462\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6652 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.58462\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6640 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.58462\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6629 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00027: binary_accuracy improved from 0.58462 to 0.60000, saving model to best.h5\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6619 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.60000\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6609 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.60000\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6598 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00030: binary_accuracy improved from 0.60000 to 0.61538, saving model to best.h5\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6588 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.61538\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6579 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.61538\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6568 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.61538\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6557 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.61538\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6547 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.61538\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6537 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.61538\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6528 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.61538\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6519 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.61538\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6511 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.61538\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6502 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.61538\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6494 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.61538\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6486 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.61538\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6479 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.61538\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6472 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.61538\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6465 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.61538\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6459 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.61538\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6453 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.61538\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6447 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.61538\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6441 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.61538\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6436 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.61538\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6430 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00051: binary_accuracy did not improve from 0.61538\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6425 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.61538\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6420 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00053: binary_accuracy did not improve from 0.61538\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6415 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.61538\n",
            "Epoch 55/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6411 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00055: binary_accuracy did not improve from 0.61538\n",
            "Epoch 56/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6406 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00056: binary_accuracy did not improve from 0.61538\n",
            "Epoch 57/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6402 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00057: binary_accuracy did not improve from 0.61538\n",
            "Epoch 58/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6397 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00058: binary_accuracy did not improve from 0.61538\n",
            "Epoch 59/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6393 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00059: binary_accuracy did not improve from 0.61538\n",
            "Epoch 60/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6389 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00060: binary_accuracy did not improve from 0.61538\n",
            "Epoch 61/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6385 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00061: binary_accuracy did not improve from 0.61538\n",
            "Epoch 62/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6382 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00062: binary_accuracy did not improve from 0.61538\n",
            "Epoch 63/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6367 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00063: binary_accuracy did not improve from 0.61538\n",
            "Epoch 64/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6455 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00064: binary_accuracy did not improve from 0.61538\n",
            "Epoch 65/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6358 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00065: binary_accuracy did not improve from 0.61538\n",
            "Epoch 66/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6372 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00066: binary_accuracy did not improve from 0.61538\n",
            "Epoch 67/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6349 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00067: binary_accuracy improved from 0.61538 to 0.63077, saving model to best.h5\n",
            "Epoch 68/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6534 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00068: binary_accuracy did not improve from 0.63077\n",
            "Epoch 69/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6348 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00069: binary_accuracy did not improve from 0.63077\n",
            "Epoch 70/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6361 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00070: binary_accuracy did not improve from 0.63077\n",
            "Epoch 71/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6355 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00071: binary_accuracy did not improve from 0.63077\n",
            "Epoch 72/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6375 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00072: binary_accuracy did not improve from 0.63077\n",
            "Epoch 73/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6332 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00073: binary_accuracy did not improve from 0.63077\n",
            "Epoch 74/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6455 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00074: binary_accuracy did not improve from 0.63077\n",
            "Epoch 75/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6334 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00075: binary_accuracy did not improve from 0.63077\n",
            "Epoch 76/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6350 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00076: binary_accuracy did not improve from 0.63077\n",
            "Epoch 77/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6336 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00077: binary_accuracy did not improve from 0.63077\n",
            "Epoch 78/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6415 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00078: binary_accuracy did not improve from 0.63077\n",
            "Epoch 79/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6327 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00079: binary_accuracy did not improve from 0.63077\n",
            "Epoch 80/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6346 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00080: binary_accuracy did not improve from 0.63077\n",
            "Epoch 81/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6320 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00081: binary_accuracy did not improve from 0.63077\n",
            "Epoch 82/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6459 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00082: binary_accuracy did not improve from 0.63077\n",
            "Epoch 83/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6318 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00083: binary_accuracy did not improve from 0.63077\n",
            "Epoch 84/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6336 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00084: binary_accuracy did not improve from 0.63077\n",
            "Epoch 85/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6330 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00085: binary_accuracy did not improve from 0.63077\n",
            "Epoch 86/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6342 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00086: binary_accuracy did not improve from 0.63077\n",
            "Epoch 87/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6310 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00087: binary_accuracy did not improve from 0.63077\n",
            "Epoch 88/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6393 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00088: binary_accuracy did not improve from 0.63077\n",
            "Epoch 89/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6309 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00089: binary_accuracy did not improve from 0.63077\n",
            "Epoch 90/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6327 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00090: binary_accuracy did not improve from 0.63077\n",
            "Epoch 91/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6315 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00091: binary_accuracy did not improve from 0.63077\n",
            "Epoch 92/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6336 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00092: binary_accuracy did not improve from 0.63077\n",
            "Epoch 93/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6301 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00093: binary_accuracy did not improve from 0.63077\n",
            "Epoch 94/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6335 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00094: binary_accuracy did not improve from 0.63077\n",
            "Epoch 95/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6299 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00095: binary_accuracy did not improve from 0.63077\n",
            "Epoch 96/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6320 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00096: binary_accuracy did not improve from 0.63077\n",
            "Epoch 97/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6299 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00097: binary_accuracy did not improve from 0.63077\n",
            "Epoch 98/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6317 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00098: binary_accuracy did not improve from 0.63077\n",
            "Epoch 99/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6294 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00099: binary_accuracy did not improve from 0.63077\n",
            "Epoch 100/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6310 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00100: binary_accuracy did not improve from 0.63077\n",
            "Epoch 101/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6293 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00101: binary_accuracy did not improve from 0.63077\n",
            "Epoch 102/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6303 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00102: binary_accuracy did not improve from 0.63077\n",
            "Epoch 103/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6293 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00103: binary_accuracy did not improve from 0.63077\n",
            "Epoch 104/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6297 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00104: binary_accuracy did not improve from 0.63077\n",
            "Epoch 105/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6290 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00105: binary_accuracy did not improve from 0.63077\n",
            "Epoch 106/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6292 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00106: binary_accuracy did not improve from 0.63077\n",
            "Epoch 107/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6287 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00107: binary_accuracy did not improve from 0.63077\n",
            "(5, 1)\n",
            "2017-02-12\n",
            "2017-02-18\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([31, 34]))\n",
            "{0: 1.096774193548387, 1: 1}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 9s 144ms/step - loss: 0.7322 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.52308, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7291 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00002: binary_accuracy did not improve from 0.52308\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7276 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00003: binary_accuracy did not improve from 0.52308\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7265 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00004: binary_accuracy did not improve from 0.52308\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7255 - binary_accuracy: 0.4308\n",
            "\n",
            "Epoch 00005: binary_accuracy did not improve from 0.52308\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7246 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00006: binary_accuracy improved from 0.52308 to 0.56923, saving model to best.h5\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7238 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00007: binary_accuracy improved from 0.56923 to 0.60000, saving model to best.h5\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7230 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00008: binary_accuracy did not improve from 0.60000\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7222 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00009: binary_accuracy did not improve from 0.60000\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7213 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00010: binary_accuracy did not improve from 0.60000\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7204 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.60000\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7195 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00012: binary_accuracy did not improve from 0.60000\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7185 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00013: binary_accuracy did not improve from 0.60000\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7174 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00014: binary_accuracy did not improve from 0.60000\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7162 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.60000\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7150 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.60000\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7138 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.60000\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7124 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.60000\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7111 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.60000\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7097 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.60000\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7083 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.60000\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7069 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.60000\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7055 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.60000\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7041 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.60000\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7027 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.60000\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7013 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.60000\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6999 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00027: binary_accuracy improved from 0.60000 to 0.61538, saving model to best.h5\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6985 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.61538\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6971 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.61538\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6957 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.61538\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6943 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.61538\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6929 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.61538\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6915 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.61538\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6901 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.61538\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6887 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.61538\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6873 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.61538\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6859 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.61538\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6845 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.61538\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6831 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.61538\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6818 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.61538\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6805 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.61538\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6793 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.61538\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6782 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.61538\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6772 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.61538\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6762 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.61538\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6754 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.61538\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6747 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.61538\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6741 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.61538\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6735 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.61538\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6730 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.61538\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6726 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00051: binary_accuracy did not improve from 0.61538\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6722 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.61538\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6719 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00053: binary_accuracy did not improve from 0.61538\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6716 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.61538\n",
            "Epoch 55/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6713 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00055: binary_accuracy did not improve from 0.61538\n",
            "Epoch 56/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6711 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00056: binary_accuracy did not improve from 0.61538\n",
            "Epoch 57/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6708 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00057: binary_accuracy did not improve from 0.61538\n",
            "Epoch 58/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6706 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00058: binary_accuracy did not improve from 0.61538\n",
            "Epoch 59/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6704 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00059: binary_accuracy did not improve from 0.61538\n",
            "Epoch 60/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6702 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00060: binary_accuracy did not improve from 0.61538\n",
            "Epoch 61/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6701 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00061: binary_accuracy did not improve from 0.61538\n",
            "Epoch 62/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6699 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00062: binary_accuracy did not improve from 0.61538\n",
            "Epoch 63/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6697 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00063: binary_accuracy did not improve from 0.61538\n",
            "Epoch 64/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6695 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00064: binary_accuracy did not improve from 0.61538\n",
            "Epoch 65/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6694 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00065: binary_accuracy did not improve from 0.61538\n",
            "Epoch 66/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6692 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00066: binary_accuracy did not improve from 0.61538\n",
            "Epoch 67/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6691 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00067: binary_accuracy did not improve from 0.61538\n",
            "(5, 1)\n",
            "2017-02-19\n",
            "2017-02-25\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([32, 33]))\n",
            "{0: 1.03125, 1: 1}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 10s 149ms/step - loss: 0.7055 - binary_accuracy: 0.4615\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.46154, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7037 - binary_accuracy: 0.5077\n",
            "\n",
            "Epoch 00002: binary_accuracy improved from 0.46154 to 0.50769, saving model to best.h5\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7026 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00003: binary_accuracy improved from 0.50769 to 0.53846, saving model to best.h5\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7014 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00004: binary_accuracy did not improve from 0.53846\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7001 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00005: binary_accuracy improved from 0.53846 to 0.56923, saving model to best.h5\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6989 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00006: binary_accuracy improved from 0.56923 to 0.60000, saving model to best.h5\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6976 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00007: binary_accuracy did not improve from 0.60000\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6962 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00008: binary_accuracy did not improve from 0.60000\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6949 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00009: binary_accuracy did not improve from 0.60000\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6936 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00010: binary_accuracy did not improve from 0.60000\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6924 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.60000\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6913 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00012: binary_accuracy did not improve from 0.60000\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6903 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00013: binary_accuracy did not improve from 0.60000\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6895 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00014: binary_accuracy did not improve from 0.60000\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6888 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.60000\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6882 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.60000\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6877 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.60000\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6872 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.60000\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6869 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.60000\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6865 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.60000\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6862 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.60000\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6860 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.60000\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6857 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.60000\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6855 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.60000\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6852 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.60000\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6850 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.60000\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6848 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.60000\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6846 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.60000\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6844 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.60000\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6842 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.60000\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6840 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.60000\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6838 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.60000\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6836 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.60000\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6835 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.60000\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6833 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.60000\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6831 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.60000\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6830 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.60000\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6828 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.60000\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6827 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.60000\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6825 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.60000\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6824 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.60000\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6822 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.60000\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6821 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00043: binary_accuracy improved from 0.60000 to 0.61538, saving model to best.h5\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6819 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.61538\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6818 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00045: binary_accuracy improved from 0.61538 to 0.63077, saving model to best.h5\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6817 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.63077\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6815 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.63077\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6814 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.63077\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6813 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00049: binary_accuracy improved from 0.63077 to 0.64615, saving model to best.h5\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6812 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.64615\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6810 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00051: binary_accuracy did not improve from 0.64615\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6809 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.64615\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6808 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00053: binary_accuracy did not improve from 0.64615\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6807 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.64615\n",
            "Epoch 55/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6805 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00055: binary_accuracy did not improve from 0.64615\n",
            "Epoch 56/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6804 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00056: binary_accuracy did not improve from 0.64615\n",
            "Epoch 57/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6803 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00057: binary_accuracy did not improve from 0.64615\n",
            "Epoch 58/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6802 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00058: binary_accuracy improved from 0.64615 to 0.67692, saving model to best.h5\n",
            "Epoch 59/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6800 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00059: binary_accuracy did not improve from 0.67692\n",
            "Epoch 60/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6799 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00060: binary_accuracy did not improve from 0.67692\n",
            "Epoch 61/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6798 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00061: binary_accuracy did not improve from 0.67692\n",
            "Epoch 62/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6797 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00062: binary_accuracy did not improve from 0.67692\n",
            "Epoch 63/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6795 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00063: binary_accuracy did not improve from 0.67692\n",
            "Epoch 64/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6794 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00064: binary_accuracy did not improve from 0.67692\n",
            "Epoch 65/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6793 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00065: binary_accuracy improved from 0.67692 to 0.69231, saving model to best.h5\n",
            "Epoch 66/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6791 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00066: binary_accuracy did not improve from 0.69231\n",
            "Epoch 67/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6790 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00067: binary_accuracy did not improve from 0.69231\n",
            "Epoch 68/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6789 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00068: binary_accuracy did not improve from 0.69231\n",
            "Epoch 69/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6787 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00069: binary_accuracy did not improve from 0.69231\n",
            "Epoch 70/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6786 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00070: binary_accuracy did not improve from 0.69231\n",
            "Epoch 71/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6785 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00071: binary_accuracy did not improve from 0.69231\n",
            "Epoch 72/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6783 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00072: binary_accuracy did not improve from 0.69231\n",
            "Epoch 73/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6782 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00073: binary_accuracy did not improve from 0.69231\n",
            "Epoch 74/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6781 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00074: binary_accuracy did not improve from 0.69231\n",
            "Epoch 75/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6779 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00075: binary_accuracy did not improve from 0.69231\n",
            "Epoch 76/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6778 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00076: binary_accuracy did not improve from 0.69231\n",
            "Epoch 77/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6776 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00077: binary_accuracy did not improve from 0.69231\n",
            "Epoch 78/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6775 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00078: binary_accuracy did not improve from 0.69231\n",
            "Epoch 79/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6774 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00079: binary_accuracy did not improve from 0.69231\n",
            "Epoch 80/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6772 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00080: binary_accuracy did not improve from 0.69231\n",
            "Epoch 81/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6771 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00081: binary_accuracy did not improve from 0.69231\n",
            "Epoch 82/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6769 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00082: binary_accuracy did not improve from 0.69231\n",
            "Epoch 83/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6767 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00083: binary_accuracy did not improve from 0.69231\n",
            "Epoch 84/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6766 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00084: binary_accuracy did not improve from 0.69231\n",
            "Epoch 85/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6764 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00085: binary_accuracy did not improve from 0.69231\n",
            "Epoch 86/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6763 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00086: binary_accuracy did not improve from 0.69231\n",
            "Epoch 87/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6761 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00087: binary_accuracy did not improve from 0.69231\n",
            "Epoch 88/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6759 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00088: binary_accuracy did not improve from 0.69231\n",
            "Epoch 89/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6758 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00089: binary_accuracy did not improve from 0.69231\n",
            "Epoch 90/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6756 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00090: binary_accuracy did not improve from 0.69231\n",
            "Epoch 91/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6754 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00091: binary_accuracy did not improve from 0.69231\n",
            "Epoch 92/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6752 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00092: binary_accuracy did not improve from 0.69231\n",
            "Epoch 93/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6751 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00093: binary_accuracy did not improve from 0.69231\n",
            "Epoch 94/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6749 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00094: binary_accuracy did not improve from 0.69231\n",
            "Epoch 95/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6747 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00095: binary_accuracy did not improve from 0.69231\n",
            "Epoch 96/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6745 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00096: binary_accuracy did not improve from 0.69231\n",
            "Epoch 97/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6743 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00097: binary_accuracy did not improve from 0.69231\n",
            "Epoch 98/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6741 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00098: binary_accuracy did not improve from 0.69231\n",
            "Epoch 99/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6739 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00099: binary_accuracy did not improve from 0.69231\n",
            "Epoch 100/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6737 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00100: binary_accuracy did not improve from 0.69231\n",
            "Epoch 101/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6735 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00101: binary_accuracy did not improve from 0.69231\n",
            "Epoch 102/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6733 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00102: binary_accuracy did not improve from 0.69231\n",
            "Epoch 103/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6730 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00103: binary_accuracy did not improve from 0.69231\n",
            "Epoch 104/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6728 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00104: binary_accuracy did not improve from 0.69231\n",
            "Epoch 105/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6726 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00105: binary_accuracy did not improve from 0.69231\n",
            "(5, 1)\n",
            "2017-02-26\n",
            "2017-03-04\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([33, 32]))\n",
            "{0: 1, 1: 1.03125}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 10s 153ms/step - loss: 0.7075 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.49231, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7061 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00002: binary_accuracy did not improve from 0.49231\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7054 - binary_accuracy: 0.4769\n",
            "\n",
            "Epoch 00003: binary_accuracy did not improve from 0.49231\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7049 - binary_accuracy: 0.4308\n",
            "\n",
            "Epoch 00004: binary_accuracy did not improve from 0.49231\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7044 - binary_accuracy: 0.4769\n",
            "\n",
            "Epoch 00005: binary_accuracy did not improve from 0.49231\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7040 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00006: binary_accuracy improved from 0.49231 to 0.52308, saving model to best.h5\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7036 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00007: binary_accuracy improved from 0.52308 to 0.55385, saving model to best.h5\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7032 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00008: binary_accuracy did not improve from 0.55385\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7028 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00009: binary_accuracy did not improve from 0.55385\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7025 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00010: binary_accuracy did not improve from 0.55385\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7021 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.55385\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7017 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00012: binary_accuracy improved from 0.55385 to 0.56923, saving model to best.h5\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7013 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00013: binary_accuracy improved from 0.56923 to 0.58462, saving model to best.h5\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7009 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00014: binary_accuracy did not improve from 0.58462\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7005 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.58462\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7000 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.58462\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6996 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.58462\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6992 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.58462\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6987 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.58462\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6983 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.58462\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6978 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.58462\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6974 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.58462\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6970 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.58462\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6966 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.58462\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6961 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.58462\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6957 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.58462\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6953 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.58462\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6949 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.58462\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6945 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.58462\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6942 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.58462\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6938 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.58462\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6934 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.58462\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6931 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.58462\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6927 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.58462\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6924 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.58462\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6921 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.58462\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6918 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.58462\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6915 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.58462\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6912 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.58462\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6909 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.58462\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6906 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.58462\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6904 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.58462\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6901 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.58462\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6899 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.58462\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6897 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.58462\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6894 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.58462\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6892 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.58462\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6890 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.58462\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6888 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.58462\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6887 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.58462\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6885 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00051: binary_accuracy did not improve from 0.58462\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6883 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.58462\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6881 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00053: binary_accuracy did not improve from 0.58462\n",
            "(5, 1)\n",
            "2017-03-05\n",
            "2017-03-11\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([30, 35]))\n",
            "{0: 1.1666666666666667, 1: 1}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 10s 158ms/step - loss: 0.7484 - binary_accuracy: 0.4769\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.47692, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7463 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00002: binary_accuracy improved from 0.47692 to 0.52308, saving model to best.h5\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7450 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00003: binary_accuracy improved from 0.52308 to 0.53846, saving model to best.h5\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7439 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00004: binary_accuracy did not improve from 0.53846\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7428 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00005: binary_accuracy improved from 0.53846 to 0.55385, saving model to best.h5\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7418 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00006: binary_accuracy did not improve from 0.55385\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7408 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00007: binary_accuracy improved from 0.55385 to 0.56923, saving model to best.h5\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7398 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00008: binary_accuracy did not improve from 0.56923\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7389 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00009: binary_accuracy did not improve from 0.56923\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7380 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00010: binary_accuracy did not improve from 0.56923\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7373 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.56923\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7365 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00012: binary_accuracy improved from 0.56923 to 0.58462, saving model to best.h5\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7359 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00013: binary_accuracy improved from 0.58462 to 0.60000, saving model to best.h5\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7353 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00014: binary_accuracy did not improve from 0.60000\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7349 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.60000\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7344 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.60000\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7341 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.60000\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7337 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.60000\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7335 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.60000\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7332 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.60000\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7330 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.60000\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7327 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.60000\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7325 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.60000\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7323 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.60000\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7321 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.60000\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7318 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.60000\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7316 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.60000\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7314 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.60000\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7312 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.60000\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7309 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.60000\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7307 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.60000\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7305 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.60000\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7303 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.60000\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7301 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.60000\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7298 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.60000\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7296 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.60000\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7294 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.60000\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7292 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.60000\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7290 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.60000\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7288 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00040: binary_accuracy improved from 0.60000 to 0.61538, saving model to best.h5\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7287 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00041: binary_accuracy improved from 0.61538 to 0.63077, saving model to best.h5\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7285 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.63077\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7283 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.63077\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7281 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.63077\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7280 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.63077\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7278 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.63077\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7276 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.63077\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7275 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00048: binary_accuracy improved from 0.63077 to 0.64615, saving model to best.h5\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7273 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.64615\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7272 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.64615\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7270 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00051: binary_accuracy did not improve from 0.64615\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7268 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.64615\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7266 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00053: binary_accuracy did not improve from 0.64615\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7265 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.64615\n",
            "Epoch 55/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7263 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00055: binary_accuracy did not improve from 0.64615\n",
            "Epoch 56/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7261 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00056: binary_accuracy did not improve from 0.64615\n",
            "Epoch 57/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7259 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00057: binary_accuracy did not improve from 0.64615\n",
            "Epoch 58/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7257 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00058: binary_accuracy improved from 0.64615 to 0.66154, saving model to best.h5\n",
            "Epoch 59/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7256 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00059: binary_accuracy did not improve from 0.66154\n",
            "Epoch 60/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7254 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00060: binary_accuracy did not improve from 0.66154\n",
            "Epoch 61/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7252 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00061: binary_accuracy did not improve from 0.66154\n",
            "Epoch 62/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7250 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00062: binary_accuracy did not improve from 0.66154\n",
            "Epoch 63/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7249 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00063: binary_accuracy did not improve from 0.66154\n",
            "Epoch 64/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7247 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00064: binary_accuracy did not improve from 0.66154\n",
            "Epoch 65/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7245 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00065: binary_accuracy did not improve from 0.66154\n",
            "Epoch 66/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7243 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00066: binary_accuracy did not improve from 0.66154\n",
            "Epoch 67/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7242 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00067: binary_accuracy did not improve from 0.66154\n",
            "Epoch 68/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7240 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00068: binary_accuracy did not improve from 0.66154\n",
            "Epoch 69/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7238 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00069: binary_accuracy did not improve from 0.66154\n",
            "Epoch 70/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7237 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00070: binary_accuracy did not improve from 0.66154\n",
            "Epoch 71/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7235 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00071: binary_accuracy did not improve from 0.66154\n",
            "Epoch 72/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7233 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00072: binary_accuracy did not improve from 0.66154\n",
            "Epoch 73/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7231 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00073: binary_accuracy did not improve from 0.66154\n",
            "Epoch 74/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7230 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00074: binary_accuracy did not improve from 0.66154\n",
            "Epoch 75/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7228 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00075: binary_accuracy did not improve from 0.66154\n",
            "Epoch 76/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7226 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00076: binary_accuracy did not improve from 0.66154\n",
            "Epoch 77/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7225 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00077: binary_accuracy did not improve from 0.66154\n",
            "Epoch 78/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7222 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00078: binary_accuracy did not improve from 0.66154\n",
            "Epoch 79/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7221 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00079: binary_accuracy did not improve from 0.66154\n",
            "Epoch 80/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7219 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00080: binary_accuracy did not improve from 0.66154\n",
            "Epoch 81/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7217 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00081: binary_accuracy did not improve from 0.66154\n",
            "Epoch 82/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7215 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00082: binary_accuracy did not improve from 0.66154\n",
            "Epoch 83/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7214 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00083: binary_accuracy did not improve from 0.66154\n",
            "Epoch 84/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7211 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00084: binary_accuracy did not improve from 0.66154\n",
            "Epoch 85/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7210 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00085: binary_accuracy did not improve from 0.66154\n",
            "Epoch 86/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7208 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00086: binary_accuracy did not improve from 0.66154\n",
            "Epoch 87/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7206 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00087: binary_accuracy did not improve from 0.66154\n",
            "Epoch 88/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7204 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00088: binary_accuracy did not improve from 0.66154\n",
            "Epoch 89/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7202 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00089: binary_accuracy did not improve from 0.66154\n",
            "Epoch 90/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7200 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00090: binary_accuracy did not improve from 0.66154\n",
            "Epoch 91/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7198 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00091: binary_accuracy did not improve from 0.66154\n",
            "Epoch 92/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7196 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00092: binary_accuracy did not improve from 0.66154\n",
            "Epoch 93/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7194 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00093: binary_accuracy did not improve from 0.66154\n",
            "Epoch 94/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7192 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00094: binary_accuracy did not improve from 0.66154\n",
            "Epoch 95/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7190 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00095: binary_accuracy did not improve from 0.66154\n",
            "Epoch 96/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7188 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00096: binary_accuracy did not improve from 0.66154\n",
            "Epoch 97/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7186 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00097: binary_accuracy did not improve from 0.66154\n",
            "Epoch 98/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7184 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00098: binary_accuracy did not improve from 0.66154\n",
            "(5, 1)\n",
            "2017-03-12\n",
            "2017-03-18\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([30, 35]))\n",
            "{0: 1.1666666666666667, 1: 1}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 11s 164ms/step - loss: 0.7510 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.53846, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7486 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00002: binary_accuracy did not improve from 0.53846\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7476 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00003: binary_accuracy did not improve from 0.53846\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7466 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00004: binary_accuracy did not improve from 0.53846\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7458 - binary_accuracy: 0.4615\n",
            "\n",
            "Epoch 00005: binary_accuracy did not improve from 0.53846\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7449 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00006: binary_accuracy improved from 0.53846 to 0.56923, saving model to best.h5\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7441 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00007: binary_accuracy improved from 0.56923 to 0.58462, saving model to best.h5\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7434 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00008: binary_accuracy did not improve from 0.58462\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7426 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00009: binary_accuracy did not improve from 0.58462\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7419 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00010: binary_accuracy did not improve from 0.58462\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7412 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.58462\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7405 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00012: binary_accuracy did not improve from 0.58462\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7398 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00013: binary_accuracy did not improve from 0.58462\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7392 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00014: binary_accuracy did not improve from 0.58462\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7386 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.58462\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7380 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.58462\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7375 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.58462\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7371 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.58462\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7367 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.58462\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7363 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.58462\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7361 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.58462\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7358 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.58462\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7356 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.58462\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7355 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.58462\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7353 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.58462\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7352 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.58462\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7351 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.58462\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7351 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.58462\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7350 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.58462\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7349 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.58462\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7349 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.58462\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7349 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.58462\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7348 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.58462\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7348 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.58462\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7347 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.58462\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7347 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.58462\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7347 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.58462\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7346 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.58462\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7346 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.58462\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7346 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.58462\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7345 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.58462\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7345 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.58462\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7345 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.58462\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7344 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.58462\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7344 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.58462\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7344 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.58462\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7344 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00047: binary_accuracy improved from 0.58462 to 0.60000, saving model to best.h5\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7343 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.60000\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7343 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.60000\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7343 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.60000\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7342 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00051: binary_accuracy did not improve from 0.60000\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7342 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.60000\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7342 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00053: binary_accuracy did not improve from 0.60000\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7341 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.60000\n",
            "Epoch 55/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7341 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00055: binary_accuracy did not improve from 0.60000\n",
            "Epoch 56/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7341 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00056: binary_accuracy did not improve from 0.60000\n",
            "Epoch 57/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7340 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00057: binary_accuracy did not improve from 0.60000\n",
            "Epoch 58/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7340 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00058: binary_accuracy did not improve from 0.60000\n",
            "Epoch 59/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7340 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00059: binary_accuracy did not improve from 0.60000\n",
            "Epoch 60/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7339 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00060: binary_accuracy did not improve from 0.60000\n",
            "Epoch 61/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7339 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00061: binary_accuracy did not improve from 0.60000\n",
            "Epoch 62/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7339 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00062: binary_accuracy did not improve from 0.60000\n",
            "Epoch 63/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7338 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00063: binary_accuracy did not improve from 0.60000\n",
            "Epoch 64/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7338 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00064: binary_accuracy did not improve from 0.60000\n",
            "Epoch 65/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7338 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00065: binary_accuracy did not improve from 0.60000\n",
            "Epoch 66/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7337 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00066: binary_accuracy did not improve from 0.60000\n",
            "Epoch 67/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7337 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00067: binary_accuracy did not improve from 0.60000\n",
            "Epoch 68/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7337 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00068: binary_accuracy did not improve from 0.60000\n",
            "Epoch 69/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7336 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00069: binary_accuracy did not improve from 0.60000\n",
            "Epoch 70/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7336 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00070: binary_accuracy did not improve from 0.60000\n",
            "Epoch 71/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7336 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00071: binary_accuracy did not improve from 0.60000\n",
            "Epoch 72/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7335 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00072: binary_accuracy did not improve from 0.60000\n",
            "Epoch 73/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7335 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00073: binary_accuracy did not improve from 0.60000\n",
            "Epoch 74/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7335 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00074: binary_accuracy did not improve from 0.60000\n",
            "Epoch 75/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7334 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00075: binary_accuracy did not improve from 0.60000\n",
            "Epoch 76/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7334 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00076: binary_accuracy did not improve from 0.60000\n",
            "Epoch 77/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7334 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00077: binary_accuracy did not improve from 0.60000\n",
            "Epoch 78/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7333 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00078: binary_accuracy did not improve from 0.60000\n",
            "Epoch 79/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7333 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00079: binary_accuracy did not improve from 0.60000\n",
            "Epoch 80/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7333 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00080: binary_accuracy did not improve from 0.60000\n",
            "Epoch 81/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7332 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00081: binary_accuracy did not improve from 0.60000\n",
            "Epoch 82/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7332 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00082: binary_accuracy did not improve from 0.60000\n",
            "Epoch 83/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7331 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00083: binary_accuracy did not improve from 0.60000\n",
            "Epoch 84/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7331 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00084: binary_accuracy did not improve from 0.60000\n",
            "Epoch 85/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7330 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00085: binary_accuracy did not improve from 0.60000\n",
            "Epoch 86/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7330 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00086: binary_accuracy did not improve from 0.60000\n",
            "Epoch 87/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7329 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00087: binary_accuracy did not improve from 0.60000\n",
            "(5, 1)\n",
            "2017-03-19\n",
            "2017-03-25\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([32, 33]))\n",
            "{0: 1.03125, 1: 1}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 11s 175ms/step - loss: 0.7023 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.49231, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7004 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00002: binary_accuracy improved from 0.49231 to 0.52308, saving model to best.h5\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6989 - binary_accuracy: 0.4615\n",
            "\n",
            "Epoch 00003: binary_accuracy did not improve from 0.52308\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6974 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00004: binary_accuracy improved from 0.52308 to 0.53846, saving model to best.h5\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6959 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00005: binary_accuracy improved from 0.53846 to 0.56923, saving model to best.h5\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6944 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00006: binary_accuracy did not improve from 0.56923\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6930 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00007: binary_accuracy improved from 0.56923 to 0.58462, saving model to best.h5\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6917 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00008: binary_accuracy did not improve from 0.58462\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6905 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00009: binary_accuracy improved from 0.58462 to 0.60000, saving model to best.h5\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6894 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00010: binary_accuracy did not improve from 0.60000\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6884 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.60000\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6876 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00012: binary_accuracy did not improve from 0.60000\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6869 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00013: binary_accuracy did not improve from 0.60000\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6863 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00014: binary_accuracy did not improve from 0.60000\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6857 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.60000\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6852 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.60000\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6847 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00017: binary_accuracy improved from 0.60000 to 0.61538, saving model to best.h5\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6843 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.61538\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6838 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.61538\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6834 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.61538\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6831 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.61538\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6827 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.61538\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6823 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.61538\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6820 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.61538\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6817 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.61538\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6814 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.61538\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6811 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.61538\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6808 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.61538\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6806 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.61538\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6803 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.61538\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6801 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.61538\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6799 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.61538\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6797 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.61538\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6796 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.61538\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6794 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.61538\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6793 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00036: binary_accuracy improved from 0.61538 to 0.63077, saving model to best.h5\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6791 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.63077\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6790 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.63077\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6789 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.63077\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6788 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.63077\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6787 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.63077\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6786 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.63077\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6785 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.63077\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6784 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.63077\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6784 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.63077\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6783 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.63077\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6782 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.63077\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6781 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.63077\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6780 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.63077\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6780 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.63077\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6779 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00051: binary_accuracy did not improve from 0.63077\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6778 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.63077\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6777 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00053: binary_accuracy did not improve from 0.63077\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6776 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.63077\n",
            "Epoch 55/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6775 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00055: binary_accuracy did not improve from 0.63077\n",
            "Epoch 56/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6774 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00056: binary_accuracy did not improve from 0.63077\n",
            "Epoch 57/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6773 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00057: binary_accuracy did not improve from 0.63077\n",
            "Epoch 58/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6772 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00058: binary_accuracy did not improve from 0.63077\n",
            "Epoch 59/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6771 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00059: binary_accuracy did not improve from 0.63077\n",
            "Epoch 60/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6770 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00060: binary_accuracy did not improve from 0.63077\n",
            "Epoch 61/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6769 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00061: binary_accuracy did not improve from 0.63077\n",
            "Epoch 62/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6768 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00062: binary_accuracy did not improve from 0.63077\n",
            "Epoch 63/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6767 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00063: binary_accuracy did not improve from 0.63077\n",
            "Epoch 64/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6766 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00064: binary_accuracy did not improve from 0.63077\n",
            "Epoch 65/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6765 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00065: binary_accuracy did not improve from 0.63077\n",
            "Epoch 66/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6764 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00066: binary_accuracy did not improve from 0.63077\n",
            "Epoch 67/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6762 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00067: binary_accuracy did not improve from 0.63077\n",
            "Epoch 68/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6761 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00068: binary_accuracy did not improve from 0.63077\n",
            "Epoch 69/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6760 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00069: binary_accuracy did not improve from 0.63077\n",
            "Epoch 70/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6758 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00070: binary_accuracy did not improve from 0.63077\n",
            "Epoch 71/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6757 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00071: binary_accuracy did not improve from 0.63077\n",
            "Epoch 72/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6756 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00072: binary_accuracy did not improve from 0.63077\n",
            "Epoch 73/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6754 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00073: binary_accuracy did not improve from 0.63077\n",
            "Epoch 74/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6753 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00074: binary_accuracy did not improve from 0.63077\n",
            "Epoch 75/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6752 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00075: binary_accuracy did not improve from 0.63077\n",
            "Epoch 76/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6751 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00076: binary_accuracy did not improve from 0.63077\n",
            "(5, 1)\n",
            "2017-03-26\n",
            "2017-04-01\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([32, 33]))\n",
            "{0: 1.03125, 1: 1}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 11s 176ms/step - loss: 0.7050 - binary_accuracy: 0.4769\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.47692, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7035 - binary_accuracy: 0.4769\n",
            "\n",
            "Epoch 00002: binary_accuracy did not improve from 0.47692\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7025 - binary_accuracy: 0.4769\n",
            "\n",
            "Epoch 00003: binary_accuracy did not improve from 0.47692\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7014 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00004: binary_accuracy improved from 0.47692 to 0.55385, saving model to best.h5\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7003 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00005: binary_accuracy improved from 0.55385 to 0.56923, saving model to best.h5\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6990 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00006: binary_accuracy improved from 0.56923 to 0.58462, saving model to best.h5\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6977 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00007: binary_accuracy improved from 0.58462 to 0.61538, saving model to best.h5\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6962 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00008: binary_accuracy did not improve from 0.61538\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6947 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00009: binary_accuracy did not improve from 0.61538\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6932 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00010: binary_accuracy did not improve from 0.61538\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6916 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.61538\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6902 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00012: binary_accuracy did not improve from 0.61538\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6888 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00013: binary_accuracy did not improve from 0.61538\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6876 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00014: binary_accuracy did not improve from 0.61538\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6866 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.61538\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6857 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.61538\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6849 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.61538\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6843 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.61538\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6838 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.61538\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6834 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.61538\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6831 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.61538\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6828 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.61538\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6826 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.61538\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6824 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.61538\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6822 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.61538\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6821 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.61538\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6819 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.61538\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6818 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.61538\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6817 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.61538\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6815 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.61538\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6814 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.61538\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6813 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.61538\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6812 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.61538\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6811 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.61538\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6810 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.61538\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6810 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00036: binary_accuracy improved from 0.61538 to 0.63077, saving model to best.h5\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6809 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.63077\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6808 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.63077\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6807 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.63077\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6806 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.63077\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6806 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.63077\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6805 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.63077\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6804 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.63077\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6804 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.63077\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6803 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.63077\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6802 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.63077\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6802 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.63077\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6801 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.63077\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6801 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.63077\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6800 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.63077\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6800 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00051: binary_accuracy did not improve from 0.63077\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6799 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.63077\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6799 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00053: binary_accuracy did not improve from 0.63077\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6798 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.63077\n",
            "Epoch 55/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6798 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00055: binary_accuracy did not improve from 0.63077\n",
            "Epoch 56/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6797 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00056: binary_accuracy did not improve from 0.63077\n",
            "Epoch 57/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6797 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00057: binary_accuracy did not improve from 0.63077\n",
            "Epoch 58/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6796 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00058: binary_accuracy did not improve from 0.63077\n",
            "Epoch 59/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6796 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00059: binary_accuracy did not improve from 0.63077\n",
            "Epoch 60/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6796 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00060: binary_accuracy did not improve from 0.63077\n",
            "Epoch 61/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6795 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00061: binary_accuracy did not improve from 0.63077\n",
            "Epoch 62/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6795 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00062: binary_accuracy did not improve from 0.63077\n",
            "Epoch 63/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6794 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00063: binary_accuracy did not improve from 0.63077\n",
            "Epoch 64/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6794 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00064: binary_accuracy did not improve from 0.63077\n",
            "Epoch 65/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6794 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00065: binary_accuracy did not improve from 0.63077\n",
            "Epoch 66/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6793 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00066: binary_accuracy did not improve from 0.63077\n",
            "Epoch 67/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6793 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00067: binary_accuracy did not improve from 0.63077\n",
            "Epoch 68/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6793 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00068: binary_accuracy did not improve from 0.63077\n",
            "Epoch 69/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6792 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00069: binary_accuracy did not improve from 0.63077\n",
            "Epoch 70/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6792 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00070: binary_accuracy did not improve from 0.63077\n",
            "Epoch 71/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6791 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00071: binary_accuracy did not improve from 0.63077\n",
            "Epoch 72/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6791 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00072: binary_accuracy did not improve from 0.63077\n",
            "Epoch 73/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6791 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00073: binary_accuracy did not improve from 0.63077\n",
            "Epoch 74/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6790 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00074: binary_accuracy did not improve from 0.63077\n",
            "Epoch 75/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6790 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00075: binary_accuracy did not improve from 0.63077\n",
            "Epoch 76/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6790 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00076: binary_accuracy did not improve from 0.63077\n",
            "(5, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Close</th>\n",
              "      <th>marketRisk</th>\n",
              "      <th>returns</th>\n",
              "      <th>Target</th>\n",
              "      <th>Change</th>\n",
              "      <th>Signal</th>\n",
              "      <th>Close30</th>\n",
              "      <th>Close100</th>\n",
              "      <th>r1</th>\n",
              "      <th>r2</th>\n",
              "      <th>marketrisk_avg30</th>\n",
              "      <th>marketrisk_avg90</th>\n",
              "      <th>predictSignal</th>\n",
              "      <th>predictedweight</th>\n",
              "      <th>labelledweight</th>\n",
              "      <th>r</th>\n",
              "      <th>predictedreturns</th>\n",
              "      <th>labelledreturns</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2017-01-02</th>\n",
              "      <td>0.956297</td>\n",
              "      <td>0.074303</td>\n",
              "      <td>0.005355</td>\n",
              "      <td>0.004872</td>\n",
              "      <td>0.005094</td>\n",
              "      <td>True</td>\n",
              "      <td>0.948200</td>\n",
              "      <td>0.915872</td>\n",
              "      <td>0.005094</td>\n",
              "      <td>-0.002016</td>\n",
              "      <td>0.066802</td>\n",
              "      <td>0.065010</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-03</th>\n",
              "      <td>0.961169</td>\n",
              "      <td>0.101960</td>\n",
              "      <td>0.005094</td>\n",
              "      <td>-0.007516</td>\n",
              "      <td>-0.007820</td>\n",
              "      <td>False</td>\n",
              "      <td>0.948863</td>\n",
              "      <td>0.916616</td>\n",
              "      <td>0.004872</td>\n",
              "      <td>0.004597</td>\n",
              "      <td>0.068402</td>\n",
              "      <td>0.065185</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.005094</td>\n",
              "      <td>-0.005094</td>\n",
              "      <td>0.005094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-04</th>\n",
              "      <td>0.953652</td>\n",
              "      <td>0.058255</td>\n",
              "      <td>-0.007820</td>\n",
              "      <td>-0.010523</td>\n",
              "      <td>-0.011035</td>\n",
              "      <td>False</td>\n",
              "      <td>0.949074</td>\n",
              "      <td>0.917292</td>\n",
              "      <td>-0.007516</td>\n",
              "      <td>-0.003011</td>\n",
              "      <td>0.067669</td>\n",
              "      <td>0.064691</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.007820</td>\n",
              "      <td>0.007820</td>\n",
              "      <td>0.007820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-05</th>\n",
              "      <td>0.943129</td>\n",
              "      <td>0.049752</td>\n",
              "      <td>-0.011035</td>\n",
              "      <td>0.006538</td>\n",
              "      <td>0.006933</td>\n",
              "      <td>True</td>\n",
              "      <td>0.948922</td>\n",
              "      <td>0.917914</td>\n",
              "      <td>-0.010523</td>\n",
              "      <td>-0.013351</td>\n",
              "      <td>0.066586</td>\n",
              "      <td>0.064512</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.011035</td>\n",
              "      <td>0.011035</td>\n",
              "      <td>0.011035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-06</th>\n",
              "      <td>0.949668</td>\n",
              "      <td>0.064650</td>\n",
              "      <td>0.006933</td>\n",
              "      <td>-0.003773</td>\n",
              "      <td>-0.003973</td>\n",
              "      <td>False</td>\n",
              "      <td>0.949083</td>\n",
              "      <td>0.918581</td>\n",
              "      <td>0.006538</td>\n",
              "      <td>-0.011224</td>\n",
              "      <td>0.066382</td>\n",
              "      <td>0.064718</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.006933</td>\n",
              "      <td>-0.006933</td>\n",
              "      <td>0.006933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-09</th>\n",
              "      <td>0.945895</td>\n",
              "      <td>0.059655</td>\n",
              "      <td>-0.003973</td>\n",
              "      <td>0.001703</td>\n",
              "      <td>0.001800</td>\n",
              "      <td>True</td>\n",
              "      <td>0.949205</td>\n",
              "      <td>0.919202</td>\n",
              "      <td>-0.003773</td>\n",
              "      <td>-0.007394</td>\n",
              "      <td>0.066756</td>\n",
              "      <td>0.064769</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.003973</td>\n",
              "      <td>0.003973</td>\n",
              "      <td>0.003973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-10</th>\n",
              "      <td>0.947598</td>\n",
              "      <td>0.033261</td>\n",
              "      <td>0.001800</td>\n",
              "      <td>-0.002418</td>\n",
              "      <td>-0.002552</td>\n",
              "      <td>False</td>\n",
              "      <td>0.949484</td>\n",
              "      <td>0.919830</td>\n",
              "      <td>0.001703</td>\n",
              "      <td>-0.003605</td>\n",
              "      <td>0.065723</td>\n",
              "      <td>0.064691</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.001800</td>\n",
              "      <td>-0.001800</td>\n",
              "      <td>0.001800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-11</th>\n",
              "      <td>0.945180</td>\n",
              "      <td>0.062518</td>\n",
              "      <td>-0.002552</td>\n",
              "      <td>-0.002673</td>\n",
              "      <td>-0.002828</td>\n",
              "      <td>False</td>\n",
              "      <td>0.949499</td>\n",
              "      <td>0.920402</td>\n",
              "      <td>-0.002418</td>\n",
              "      <td>-0.011118</td>\n",
              "      <td>0.065729</td>\n",
              "      <td>0.064932</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.002552</td>\n",
              "      <td>0.002552</td>\n",
              "      <td>0.002552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-12</th>\n",
              "      <td>0.942507</td>\n",
              "      <td>0.036572</td>\n",
              "      <td>-0.002828</td>\n",
              "      <td>-0.002746</td>\n",
              "      <td>-0.002913</td>\n",
              "      <td>False</td>\n",
              "      <td>0.949646</td>\n",
              "      <td>0.920961</td>\n",
              "      <td>-0.002673</td>\n",
              "      <td>-0.018662</td>\n",
              "      <td>0.064580</td>\n",
              "      <td>0.064800</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.002828</td>\n",
              "      <td>-0.002828</td>\n",
              "      <td>0.002828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-13</th>\n",
              "      <td>0.939761</td>\n",
              "      <td>0.053134</td>\n",
              "      <td>-0.002913</td>\n",
              "      <td>0.003635</td>\n",
              "      <td>0.003868</td>\n",
              "      <td>True</td>\n",
              "      <td>0.949731</td>\n",
              "      <td>0.921426</td>\n",
              "      <td>-0.002746</td>\n",
              "      <td>-0.013891</td>\n",
              "      <td>0.064965</td>\n",
              "      <td>0.064543</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.002913</td>\n",
              "      <td>-0.002913</td>\n",
              "      <td>0.002913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-16</th>\n",
              "      <td>0.943396</td>\n",
              "      <td>0.043191</td>\n",
              "      <td>0.003868</td>\n",
              "      <td>-0.009864</td>\n",
              "      <td>-0.010456</td>\n",
              "      <td>False</td>\n",
              "      <td>0.950205</td>\n",
              "      <td>0.921922</td>\n",
              "      <td>0.003635</td>\n",
              "      <td>0.000267</td>\n",
              "      <td>0.064260</td>\n",
              "      <td>0.064488</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.003868</td>\n",
              "      <td>0.003868</td>\n",
              "      <td>0.003868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-17</th>\n",
              "      <td>0.933532</td>\n",
              "      <td>0.039432</td>\n",
              "      <td>-0.010456</td>\n",
              "      <td>0.007201</td>\n",
              "      <td>0.007714</td>\n",
              "      <td>True</td>\n",
              "      <td>0.950219</td>\n",
              "      <td>0.922281</td>\n",
              "      <td>-0.009864</td>\n",
              "      <td>-0.016135</td>\n",
              "      <td>0.063088</td>\n",
              "      <td>0.064473</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.010456</td>\n",
              "      <td>0.010456</td>\n",
              "      <td>0.010456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-18</th>\n",
              "      <td>0.940734</td>\n",
              "      <td>0.050054</td>\n",
              "      <td>0.007714</td>\n",
              "      <td>-0.002911</td>\n",
              "      <td>-0.003095</td>\n",
              "      <td>False</td>\n",
              "      <td>0.950572</td>\n",
              "      <td>0.922725</td>\n",
              "      <td>0.007201</td>\n",
              "      <td>-0.005161</td>\n",
              "      <td>0.062997</td>\n",
              "      <td>0.064406</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.007714</td>\n",
              "      <td>-0.007714</td>\n",
              "      <td>0.007714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-19</th>\n",
              "      <td>0.937822</td>\n",
              "      <td>0.068003</td>\n",
              "      <td>-0.003095</td>\n",
              "      <td>-0.003156</td>\n",
              "      <td>-0.003365</td>\n",
              "      <td>False</td>\n",
              "      <td>0.950431</td>\n",
              "      <td>0.923171</td>\n",
              "      <td>-0.002911</td>\n",
              "      <td>-0.009775</td>\n",
              "      <td>0.063504</td>\n",
              "      <td>0.064415</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.003095</td>\n",
              "      <td>-0.003095</td>\n",
              "      <td>0.003095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-20</th>\n",
              "      <td>0.934667</td>\n",
              "      <td>0.036442</td>\n",
              "      <td>-0.003365</td>\n",
              "      <td>-0.005385</td>\n",
              "      <td>-0.005762</td>\n",
              "      <td>False</td>\n",
              "      <td>0.950018</td>\n",
              "      <td>0.923552</td>\n",
              "      <td>-0.003156</td>\n",
              "      <td>-0.010513</td>\n",
              "      <td>0.062047</td>\n",
              "      <td>0.064140</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.003365</td>\n",
              "      <td>-0.003365</td>\n",
              "      <td>0.003365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-23</th>\n",
              "      <td>0.929282</td>\n",
              "      <td>0.037652</td>\n",
              "      <td>-0.005762</td>\n",
              "      <td>0.002685</td>\n",
              "      <td>0.002889</td>\n",
              "      <td>True</td>\n",
              "      <td>0.949648</td>\n",
              "      <td>0.923871</td>\n",
              "      <td>-0.005385</td>\n",
              "      <td>-0.013225</td>\n",
              "      <td>0.060675</td>\n",
              "      <td>0.063988</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.005762</td>\n",
              "      <td>-0.005762</td>\n",
              "      <td>0.005762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-24</th>\n",
              "      <td>0.931966</td>\n",
              "      <td>0.038307</td>\n",
              "      <td>0.002889</td>\n",
              "      <td>-0.001214</td>\n",
              "      <td>-0.001303</td>\n",
              "      <td>False</td>\n",
              "      <td>0.949341</td>\n",
              "      <td>0.924304</td>\n",
              "      <td>0.002685</td>\n",
              "      <td>-0.007795</td>\n",
              "      <td>0.060226</td>\n",
              "      <td>0.064062</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.002889</td>\n",
              "      <td>-0.002889</td>\n",
              "      <td>0.002889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-25</th>\n",
              "      <td>0.930752</td>\n",
              "      <td>0.054899</td>\n",
              "      <td>-0.001303</td>\n",
              "      <td>0.005578</td>\n",
              "      <td>0.005993</td>\n",
              "      <td>True</td>\n",
              "      <td>0.948725</td>\n",
              "      <td>0.924712</td>\n",
              "      <td>-0.001214</td>\n",
              "      <td>-0.012644</td>\n",
              "      <td>0.059850</td>\n",
              "      <td>0.064147</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.001303</td>\n",
              "      <td>0.001303</td>\n",
              "      <td>0.001303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-26</th>\n",
              "      <td>0.936330</td>\n",
              "      <td>0.040985</td>\n",
              "      <td>0.005993</td>\n",
              "      <td>-0.001226</td>\n",
              "      <td>-0.001309</td>\n",
              "      <td>False</td>\n",
              "      <td>0.947922</td>\n",
              "      <td>0.925193</td>\n",
              "      <td>0.005578</td>\n",
              "      <td>0.002797</td>\n",
              "      <td>0.057834</td>\n",
              "      <td>0.064095</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.005993</td>\n",
              "      <td>-0.005993</td>\n",
              "      <td>0.005993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-27</th>\n",
              "      <td>0.935104</td>\n",
              "      <td>0.076645</td>\n",
              "      <td>-0.001309</td>\n",
              "      <td>0.000175</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>True</td>\n",
              "      <td>0.947185</td>\n",
              "      <td>0.925638</td>\n",
              "      <td>-0.001226</td>\n",
              "      <td>-0.005630</td>\n",
              "      <td>0.058025</td>\n",
              "      <td>0.064731</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.001309</td>\n",
              "      <td>0.001309</td>\n",
              "      <td>0.001309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-30</th>\n",
              "      <td>0.935279</td>\n",
              "      <td>0.022058</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>-0.008924</td>\n",
              "      <td>-0.009541</td>\n",
              "      <td>False</td>\n",
              "      <td>0.946316</td>\n",
              "      <td>0.926088</td>\n",
              "      <td>0.000175</td>\n",
              "      <td>-0.002544</td>\n",
              "      <td>0.056695</td>\n",
              "      <td>0.064603</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>-0.000187</td>\n",
              "      <td>0.000187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-31</th>\n",
              "      <td>0.926355</td>\n",
              "      <td>0.031282</td>\n",
              "      <td>-0.009541</td>\n",
              "      <td>0.002409</td>\n",
              "      <td>0.002601</td>\n",
              "      <td>True</td>\n",
              "      <td>0.945097</td>\n",
              "      <td>0.926436</td>\n",
              "      <td>-0.008924</td>\n",
              "      <td>-0.008312</td>\n",
              "      <td>0.054650</td>\n",
              "      <td>0.064372</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.009541</td>\n",
              "      <td>0.009541</td>\n",
              "      <td>0.009541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-02-01</th>\n",
              "      <td>0.928764</td>\n",
              "      <td>0.039968</td>\n",
              "      <td>0.002601</td>\n",
              "      <td>0.000777</td>\n",
              "      <td>0.000837</td>\n",
              "      <td>True</td>\n",
              "      <td>0.944075</td>\n",
              "      <td>0.926834</td>\n",
              "      <td>0.002409</td>\n",
              "      <td>-0.000518</td>\n",
              "      <td>0.054057</td>\n",
              "      <td>0.064102</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.002601</td>\n",
              "      <td>-0.002601</td>\n",
              "      <td>0.002601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-02-02</th>\n",
              "      <td>0.929541</td>\n",
              "      <td>0.026518</td>\n",
              "      <td>0.000837</td>\n",
              "      <td>-0.001983</td>\n",
              "      <td>-0.002133</td>\n",
              "      <td>False</td>\n",
              "      <td>0.943116</td>\n",
              "      <td>0.927234</td>\n",
              "      <td>0.000777</td>\n",
              "      <td>-0.002426</td>\n",
              "      <td>0.053253</td>\n",
              "      <td>0.063772</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.000837</td>\n",
              "      <td>-0.000837</td>\n",
              "      <td>0.000837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-02-03</th>\n",
              "      <td>0.927558</td>\n",
              "      <td>0.041529</td>\n",
              "      <td>-0.002133</td>\n",
              "      <td>0.002761</td>\n",
              "      <td>0.002977</td>\n",
              "      <td>True</td>\n",
              "      <td>0.942149</td>\n",
              "      <td>0.927543</td>\n",
              "      <td>-0.001983</td>\n",
              "      <td>-0.003194</td>\n",
              "      <td>0.053013</td>\n",
              "      <td>0.063526</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.002133</td>\n",
              "      <td>0.002133</td>\n",
              "      <td>0.002133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-02-06</th>\n",
              "      <td>0.930319</td>\n",
              "      <td>0.028982</td>\n",
              "      <td>0.002977</td>\n",
              "      <td>0.006010</td>\n",
              "      <td>0.006461</td>\n",
              "      <td>True</td>\n",
              "      <td>0.941271</td>\n",
              "      <td>0.927896</td>\n",
              "      <td>0.002761</td>\n",
              "      <td>-0.006010</td>\n",
              "      <td>0.052926</td>\n",
              "      <td>0.063153</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.002977</td>\n",
              "      <td>-0.002977</td>\n",
              "      <td>0.002977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-02-07</th>\n",
              "      <td>0.936330</td>\n",
              "      <td>0.056471</td>\n",
              "      <td>0.006461</td>\n",
              "      <td>-0.001488</td>\n",
              "      <td>-0.001589</td>\n",
              "      <td>False</td>\n",
              "      <td>0.940599</td>\n",
              "      <td>0.928292</td>\n",
              "      <td>0.006010</td>\n",
              "      <td>0.001226</td>\n",
              "      <td>0.052020</td>\n",
              "      <td>0.062957</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.006461</td>\n",
              "      <td>0.006461</td>\n",
              "      <td>0.006461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-02-08</th>\n",
              "      <td>0.934842</td>\n",
              "      <td>0.038375</td>\n",
              "      <td>-0.001589</td>\n",
              "      <td>0.003773</td>\n",
              "      <td>0.004036</td>\n",
              "      <td>True</td>\n",
              "      <td>0.939731</td>\n",
              "      <td>0.928700</td>\n",
              "      <td>-0.001488</td>\n",
              "      <td>-0.000437</td>\n",
              "      <td>0.049903</td>\n",
              "      <td>0.062438</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.001589</td>\n",
              "      <td>-0.001589</td>\n",
              "      <td>0.001589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-02-09</th>\n",
              "      <td>0.938615</td>\n",
              "      <td>0.051965</td>\n",
              "      <td>0.004036</td>\n",
              "      <td>0.001412</td>\n",
              "      <td>0.001504</td>\n",
              "      <td>True</td>\n",
              "      <td>0.939242</td>\n",
              "      <td>0.929163</td>\n",
              "      <td>0.003773</td>\n",
              "      <td>0.012260</td>\n",
              "      <td>0.049062</td>\n",
              "      <td>0.061897</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.004036</td>\n",
              "      <td>0.004036</td>\n",
              "      <td>0.004036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-02-10</th>\n",
              "      <td>0.940026</td>\n",
              "      <td>0.085581</td>\n",
              "      <td>0.001504</td>\n",
              "      <td>0.003637</td>\n",
              "      <td>0.003869</td>\n",
              "      <td>True</td>\n",
              "      <td>0.938869</td>\n",
              "      <td>0.929653</td>\n",
              "      <td>0.001412</td>\n",
              "      <td>0.011263</td>\n",
              "      <td>0.050080</td>\n",
              "      <td>0.062181</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.001504</td>\n",
              "      <td>0.001504</td>\n",
              "      <td>0.001504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-11-21</th>\n",
              "      <td>0.852006</td>\n",
              "      <td>0.032757</td>\n",
              "      <td>-0.000426</td>\n",
              "      <td>-0.005983</td>\n",
              "      <td>-0.007022</td>\n",
              "      <td>False</td>\n",
              "      <td>0.853261</td>\n",
              "      <td>0.851177</td>\n",
              "      <td>-0.000363</td>\n",
              "      <td>-0.005406</td>\n",
              "      <td>0.046575</td>\n",
              "      <td>0.046909</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.000426</td>\n",
              "      <td>0.000426</td>\n",
              "      <td>0.000426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-11-22</th>\n",
              "      <td>0.846024</td>\n",
              "      <td>0.022044</td>\n",
              "      <td>-0.007022</td>\n",
              "      <td>-0.002142</td>\n",
              "      <td>-0.002532</td>\n",
              "      <td>False</td>\n",
              "      <td>0.853349</td>\n",
              "      <td>0.850828</td>\n",
              "      <td>-0.005983</td>\n",
              "      <td>-0.011242</td>\n",
              "      <td>0.046114</td>\n",
              "      <td>0.046822</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.007022</td>\n",
              "      <td>0.007022</td>\n",
              "      <td>0.007022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-11-23</th>\n",
              "      <td>0.843882</td>\n",
              "      <td>0.018941</td>\n",
              "      <td>-0.002532</td>\n",
              "      <td>-0.005659</td>\n",
              "      <td>-0.006706</td>\n",
              "      <td>False</td>\n",
              "      <td>0.853299</td>\n",
              "      <td>0.850512</td>\n",
              "      <td>-0.002142</td>\n",
              "      <td>-0.003863</td>\n",
              "      <td>0.045806</td>\n",
              "      <td>0.046652</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.002532</td>\n",
              "      <td>0.002532</td>\n",
              "      <td>0.002532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-11-24</th>\n",
              "      <td>0.838223</td>\n",
              "      <td>0.030054</td>\n",
              "      <td>-0.006706</td>\n",
              "      <td>0.002396</td>\n",
              "      <td>0.002858</td>\n",
              "      <td>True</td>\n",
              "      <td>0.853044</td>\n",
              "      <td>0.850122</td>\n",
              "      <td>-0.005659</td>\n",
              "      <td>-0.009882</td>\n",
              "      <td>0.045548</td>\n",
              "      <td>0.046685</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.006706</td>\n",
              "      <td>0.006706</td>\n",
              "      <td>0.006706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-11-27</th>\n",
              "      <td>0.840619</td>\n",
              "      <td>0.032492</td>\n",
              "      <td>0.002858</td>\n",
              "      <td>0.004047</td>\n",
              "      <td>0.004815</td>\n",
              "      <td>True</td>\n",
              "      <td>0.852804</td>\n",
              "      <td>0.849755</td>\n",
              "      <td>0.002396</td>\n",
              "      <td>-0.009071</td>\n",
              "      <td>0.045056</td>\n",
              "      <td>0.046735</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.002858</td>\n",
              "      <td>-0.002858</td>\n",
              "      <td>0.002858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-11-28</th>\n",
              "      <td>0.844666</td>\n",
              "      <td>0.044063</td>\n",
              "      <td>0.004815</td>\n",
              "      <td>-0.000570</td>\n",
              "      <td>-0.000675</td>\n",
              "      <td>False</td>\n",
              "      <td>0.852629</td>\n",
              "      <td>0.849480</td>\n",
              "      <td>0.004047</td>\n",
              "      <td>-0.003295</td>\n",
              "      <td>0.044239</td>\n",
              "      <td>0.046910</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.004815</td>\n",
              "      <td>-0.004815</td>\n",
              "      <td>0.004815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-11-29</th>\n",
              "      <td>0.844096</td>\n",
              "      <td>0.060237</td>\n",
              "      <td>-0.000675</td>\n",
              "      <td>-0.003901</td>\n",
              "      <td>-0.004621</td>\n",
              "      <td>False</td>\n",
              "      <td>0.852486</td>\n",
              "      <td>0.849157</td>\n",
              "      <td>-0.000570</td>\n",
              "      <td>-0.008274</td>\n",
              "      <td>0.044113</td>\n",
              "      <td>0.047140</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.000675</td>\n",
              "      <td>0.000675</td>\n",
              "      <td>0.000675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-11-30</th>\n",
              "      <td>0.840195</td>\n",
              "      <td>0.053413</td>\n",
              "      <td>-0.004621</td>\n",
              "      <td>0.000919</td>\n",
              "      <td>0.001093</td>\n",
              "      <td>True</td>\n",
              "      <td>0.852363</td>\n",
              "      <td>0.848783</td>\n",
              "      <td>-0.003901</td>\n",
              "      <td>-0.011812</td>\n",
              "      <td>0.043694</td>\n",
              "      <td>0.047326</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.004621</td>\n",
              "      <td>0.004621</td>\n",
              "      <td>0.004621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-01</th>\n",
              "      <td>0.841114</td>\n",
              "      <td>0.049116</td>\n",
              "      <td>0.001093</td>\n",
              "      <td>0.001772</td>\n",
              "      <td>0.002107</td>\n",
              "      <td>True</td>\n",
              "      <td>0.852111</td>\n",
              "      <td>0.848473</td>\n",
              "      <td>0.000919</td>\n",
              "      <td>-0.004910</td>\n",
              "      <td>0.042181</td>\n",
              "      <td>0.047390</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.001093</td>\n",
              "      <td>-0.001093</td>\n",
              "      <td>0.001093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-04</th>\n",
              "      <td>0.842886</td>\n",
              "      <td>0.090952</td>\n",
              "      <td>0.002107</td>\n",
              "      <td>0.002851</td>\n",
              "      <td>0.003383</td>\n",
              "      <td>True</td>\n",
              "      <td>0.851831</td>\n",
              "      <td>0.848190</td>\n",
              "      <td>0.001772</td>\n",
              "      <td>-0.000996</td>\n",
              "      <td>0.042438</td>\n",
              "      <td>0.047974</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.002107</td>\n",
              "      <td>-0.002107</td>\n",
              "      <td>0.002107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-05</th>\n",
              "      <td>0.845737</td>\n",
              "      <td>0.072858</td>\n",
              "      <td>0.003383</td>\n",
              "      <td>0.002079</td>\n",
              "      <td>0.002459</td>\n",
              "      <td>True</td>\n",
              "      <td>0.851675</td>\n",
              "      <td>0.847991</td>\n",
              "      <td>0.002851</td>\n",
              "      <td>0.007515</td>\n",
              "      <td>0.043048</td>\n",
              "      <td>0.048544</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.003383</td>\n",
              "      <td>-0.003383</td>\n",
              "      <td>0.003383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-06</th>\n",
              "      <td>0.847817</td>\n",
              "      <td>0.033286</td>\n",
              "      <td>0.002459</td>\n",
              "      <td>0.001729</td>\n",
              "      <td>0.002039</td>\n",
              "      <td>True</td>\n",
              "      <td>0.851716</td>\n",
              "      <td>0.847784</td>\n",
              "      <td>0.002079</td>\n",
              "      <td>0.007198</td>\n",
              "      <td>0.042090</td>\n",
              "      <td>0.048541</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.002459</td>\n",
              "      <td>-0.002459</td>\n",
              "      <td>0.002459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-07</th>\n",
              "      <td>0.849545</td>\n",
              "      <td>0.065278</td>\n",
              "      <td>0.002039</td>\n",
              "      <td>0.000506</td>\n",
              "      <td>0.000595</td>\n",
              "      <td>True</td>\n",
              "      <td>0.851422</td>\n",
              "      <td>0.847680</td>\n",
              "      <td>0.001729</td>\n",
              "      <td>0.004880</td>\n",
              "      <td>0.042056</td>\n",
              "      <td>0.048816</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.002039</td>\n",
              "      <td>-0.002039</td>\n",
              "      <td>0.002039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-08</th>\n",
              "      <td>0.850051</td>\n",
              "      <td>0.085969</td>\n",
              "      <td>0.000595</td>\n",
              "      <td>-0.000289</td>\n",
              "      <td>-0.000340</td>\n",
              "      <td>False</td>\n",
              "      <td>0.851041</td>\n",
              "      <td>0.847605</td>\n",
              "      <td>0.000506</td>\n",
              "      <td>0.005955</td>\n",
              "      <td>0.042609</td>\n",
              "      <td>0.049215</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.000595</td>\n",
              "      <td>-0.000595</td>\n",
              "      <td>0.000595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-11</th>\n",
              "      <td>0.849762</td>\n",
              "      <td>0.036393</td>\n",
              "      <td>-0.000340</td>\n",
              "      <td>0.002027</td>\n",
              "      <td>0.002385</td>\n",
              "      <td>True</td>\n",
              "      <td>0.850752</td>\n",
              "      <td>0.847510</td>\n",
              "      <td>-0.000289</td>\n",
              "      <td>0.009567</td>\n",
              "      <td>0.042542</td>\n",
              "      <td>0.048944</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.000340</td>\n",
              "      <td>0.000340</td>\n",
              "      <td>0.000340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-12</th>\n",
              "      <td>0.851789</td>\n",
              "      <td>0.081947</td>\n",
              "      <td>0.002385</td>\n",
              "      <td>-0.006123</td>\n",
              "      <td>-0.007188</td>\n",
              "      <td>False</td>\n",
              "      <td>0.850517</td>\n",
              "      <td>0.847445</td>\n",
              "      <td>0.002027</td>\n",
              "      <td>0.010675</td>\n",
              "      <td>0.044133</td>\n",
              "      <td>0.049259</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.002385</td>\n",
              "      <td>-0.002385</td>\n",
              "      <td>0.002385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-13</th>\n",
              "      <td>0.845666</td>\n",
              "      <td>0.032359</td>\n",
              "      <td>-0.007188</td>\n",
              "      <td>0.003447</td>\n",
              "      <td>0.004076</td>\n",
              "      <td>True</td>\n",
              "      <td>0.850013</td>\n",
              "      <td>0.847378</td>\n",
              "      <td>-0.006123</td>\n",
              "      <td>0.002780</td>\n",
              "      <td>0.042946</td>\n",
              "      <td>0.049144</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.007188</td>\n",
              "      <td>0.007188</td>\n",
              "      <td>0.007188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-14</th>\n",
              "      <td>0.849113</td>\n",
              "      <td>0.040492</td>\n",
              "      <td>0.004076</td>\n",
              "      <td>0.001806</td>\n",
              "      <td>0.002127</td>\n",
              "      <td>True</td>\n",
              "      <td>0.849719</td>\n",
              "      <td>0.847304</td>\n",
              "      <td>0.003447</td>\n",
              "      <td>0.003375</td>\n",
              "      <td>0.043390</td>\n",
              "      <td>0.049264</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.004076</td>\n",
              "      <td>-0.004076</td>\n",
              "      <td>0.004076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-15</th>\n",
              "      <td>0.850919</td>\n",
              "      <td>0.047422</td>\n",
              "      <td>0.002127</td>\n",
              "      <td>-0.002095</td>\n",
              "      <td>-0.002462</td>\n",
              "      <td>False</td>\n",
              "      <td>0.849365</td>\n",
              "      <td>0.847303</td>\n",
              "      <td>0.001806</td>\n",
              "      <td>0.003102</td>\n",
              "      <td>0.043575</td>\n",
              "      <td>0.049625</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.002127</td>\n",
              "      <td>-0.002127</td>\n",
              "      <td>0.002127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-18</th>\n",
              "      <td>0.848824</td>\n",
              "      <td>0.046910</td>\n",
              "      <td>-0.002462</td>\n",
              "      <td>-0.004087</td>\n",
              "      <td>-0.004815</td>\n",
              "      <td>False</td>\n",
              "      <td>0.848945</td>\n",
              "      <td>0.847345</td>\n",
              "      <td>-0.002095</td>\n",
              "      <td>-0.000721</td>\n",
              "      <td>0.042431</td>\n",
              "      <td>0.049620</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.002462</td>\n",
              "      <td>0.002462</td>\n",
              "      <td>0.002462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-19</th>\n",
              "      <td>0.844737</td>\n",
              "      <td>0.044138</td>\n",
              "      <td>-0.004815</td>\n",
              "      <td>-0.002206</td>\n",
              "      <td>-0.002612</td>\n",
              "      <td>False</td>\n",
              "      <td>0.848330</td>\n",
              "      <td>0.847319</td>\n",
              "      <td>-0.004087</td>\n",
              "      <td>-0.005314</td>\n",
              "      <td>0.041673</td>\n",
              "      <td>0.049359</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.004815</td>\n",
              "      <td>0.004815</td>\n",
              "      <td>0.004815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-20</th>\n",
              "      <td>0.842531</td>\n",
              "      <td>0.065079</td>\n",
              "      <td>-0.002612</td>\n",
              "      <td>-0.000284</td>\n",
              "      <td>-0.000337</td>\n",
              "      <td>False</td>\n",
              "      <td>0.847662</td>\n",
              "      <td>0.847308</td>\n",
              "      <td>-0.002206</td>\n",
              "      <td>-0.007231</td>\n",
              "      <td>0.043813</td>\n",
              "      <td>0.049362</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.002612</td>\n",
              "      <td>0.002612</td>\n",
              "      <td>0.002612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-21</th>\n",
              "      <td>0.842247</td>\n",
              "      <td>0.053057</td>\n",
              "      <td>-0.000337</td>\n",
              "      <td>0.001137</td>\n",
              "      <td>0.001349</td>\n",
              "      <td>True</td>\n",
              "      <td>0.847100</td>\n",
              "      <td>0.847305</td>\n",
              "      <td>-0.000284</td>\n",
              "      <td>-0.009542</td>\n",
              "      <td>0.045579</td>\n",
              "      <td>0.049593</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.000337</td>\n",
              "      <td>0.000337</td>\n",
              "      <td>0.000337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-22</th>\n",
              "      <td>0.843384</td>\n",
              "      <td>0.050513</td>\n",
              "      <td>0.001349</td>\n",
              "      <td>-0.000782</td>\n",
              "      <td>-0.000927</td>\n",
              "      <td>False</td>\n",
              "      <td>0.846632</td>\n",
              "      <td>0.847241</td>\n",
              "      <td>0.001137</td>\n",
              "      <td>-0.002282</td>\n",
              "      <td>0.046792</td>\n",
              "      <td>0.049929</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.001349</td>\n",
              "      <td>-0.001349</td>\n",
              "      <td>0.001349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-25</th>\n",
              "      <td>0.842602</td>\n",
              "      <td>0.055977</td>\n",
              "      <td>-0.000927</td>\n",
              "      <td>0.000711</td>\n",
              "      <td>0.000843</td>\n",
              "      <td>True</td>\n",
              "      <td>0.846143</td>\n",
              "      <td>0.847187</td>\n",
              "      <td>-0.000782</td>\n",
              "      <td>-0.006511</td>\n",
              "      <td>0.048229</td>\n",
              "      <td>0.050399</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.000927</td>\n",
              "      <td>0.000927</td>\n",
              "      <td>0.000927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-26</th>\n",
              "      <td>0.843313</td>\n",
              "      <td>0.053010</td>\n",
              "      <td>0.000843</td>\n",
              "      <td>-0.001987</td>\n",
              "      <td>-0.002356</td>\n",
              "      <td>False</td>\n",
              "      <td>0.845996</td>\n",
              "      <td>0.847109</td>\n",
              "      <td>0.000711</td>\n",
              "      <td>-0.007606</td>\n",
              "      <td>0.048635</td>\n",
              "      <td>0.050599</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.000843</td>\n",
              "      <td>-0.000843</td>\n",
              "      <td>0.000843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-27</th>\n",
              "      <td>0.841326</td>\n",
              "      <td>0.039179</td>\n",
              "      <td>-0.002356</td>\n",
              "      <td>-0.003875</td>\n",
              "      <td>-0.004606</td>\n",
              "      <td>False</td>\n",
              "      <td>0.845770</td>\n",
              "      <td>0.847017</td>\n",
              "      <td>-0.001987</td>\n",
              "      <td>-0.007498</td>\n",
              "      <td>0.048731</td>\n",
              "      <td>0.050775</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.002356</td>\n",
              "      <td>0.002356</td>\n",
              "      <td>0.002356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-28</th>\n",
              "      <td>0.837451</td>\n",
              "      <td>0.041405</td>\n",
              "      <td>-0.004606</td>\n",
              "      <td>-0.003840</td>\n",
              "      <td>-0.004585</td>\n",
              "      <td>False</td>\n",
              "      <td>0.845362</td>\n",
              "      <td>0.846895</td>\n",
              "      <td>-0.003875</td>\n",
              "      <td>-0.007286</td>\n",
              "      <td>0.048284</td>\n",
              "      <td>0.050967</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.004606</td>\n",
              "      <td>0.004606</td>\n",
              "      <td>0.004606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-29</th>\n",
              "      <td>0.833611</td>\n",
              "      <td>0.033295</td>\n",
              "      <td>-0.004585</td>\n",
              "      <td>-0.000833</td>\n",
              "      <td>-0.000999</td>\n",
              "      <td>False</td>\n",
              "      <td>0.844883</td>\n",
              "      <td>0.846771</td>\n",
              "      <td>-0.003840</td>\n",
              "      <td>-0.008920</td>\n",
              "      <td>0.048487</td>\n",
              "      <td>0.050887</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.004585</td>\n",
              "      <td>0.004585</td>\n",
              "      <td>0.004585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01</th>\n",
              "      <td>0.832778</td>\n",
              "      <td>0.051245</td>\n",
              "      <td>-0.000999</td>\n",
              "      <td>-0.003384</td>\n",
              "      <td>-0.004064</td>\n",
              "      <td>False</td>\n",
              "      <td>0.844230</td>\n",
              "      <td>0.846608</td>\n",
              "      <td>-0.000833</td>\n",
              "      <td>-0.009469</td>\n",
              "      <td>0.048796</td>\n",
              "      <td>0.051115</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.000999</td>\n",
              "      <td>0.000999</td>\n",
              "      <td>0.000999</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>261 rows × 18 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               Close  marketRisk   returns    Target    Change  Signal  \\\n",
              "Date                                                                     \n",
              "2017-01-02  0.956297    0.074303  0.005355  0.004872  0.005094    True   \n",
              "2017-01-03  0.961169    0.101960  0.005094 -0.007516 -0.007820   False   \n",
              "2017-01-04  0.953652    0.058255 -0.007820 -0.010523 -0.011035   False   \n",
              "2017-01-05  0.943129    0.049752 -0.011035  0.006538  0.006933    True   \n",
              "2017-01-06  0.949668    0.064650  0.006933 -0.003773 -0.003973   False   \n",
              "2017-01-09  0.945895    0.059655 -0.003973  0.001703  0.001800    True   \n",
              "2017-01-10  0.947598    0.033261  0.001800 -0.002418 -0.002552   False   \n",
              "2017-01-11  0.945180    0.062518 -0.002552 -0.002673 -0.002828   False   \n",
              "2017-01-12  0.942507    0.036572 -0.002828 -0.002746 -0.002913   False   \n",
              "2017-01-13  0.939761    0.053134 -0.002913  0.003635  0.003868    True   \n",
              "2017-01-16  0.943396    0.043191  0.003868 -0.009864 -0.010456   False   \n",
              "2017-01-17  0.933532    0.039432 -0.010456  0.007201  0.007714    True   \n",
              "2017-01-18  0.940734    0.050054  0.007714 -0.002911 -0.003095   False   \n",
              "2017-01-19  0.937822    0.068003 -0.003095 -0.003156 -0.003365   False   \n",
              "2017-01-20  0.934667    0.036442 -0.003365 -0.005385 -0.005762   False   \n",
              "2017-01-23  0.929282    0.037652 -0.005762  0.002685  0.002889    True   \n",
              "2017-01-24  0.931966    0.038307  0.002889 -0.001214 -0.001303   False   \n",
              "2017-01-25  0.930752    0.054899 -0.001303  0.005578  0.005993    True   \n",
              "2017-01-26  0.936330    0.040985  0.005993 -0.001226 -0.001309   False   \n",
              "2017-01-27  0.935104    0.076645 -0.001309  0.000175  0.000187    True   \n",
              "2017-01-30  0.935279    0.022058  0.000187 -0.008924 -0.009541   False   \n",
              "2017-01-31  0.926355    0.031282 -0.009541  0.002409  0.002601    True   \n",
              "2017-02-01  0.928764    0.039968  0.002601  0.000777  0.000837    True   \n",
              "2017-02-02  0.929541    0.026518  0.000837 -0.001983 -0.002133   False   \n",
              "2017-02-03  0.927558    0.041529 -0.002133  0.002761  0.002977    True   \n",
              "2017-02-06  0.930319    0.028982  0.002977  0.006010  0.006461    True   \n",
              "2017-02-07  0.936330    0.056471  0.006461 -0.001488 -0.001589   False   \n",
              "2017-02-08  0.934842    0.038375 -0.001589  0.003773  0.004036    True   \n",
              "2017-02-09  0.938615    0.051965  0.004036  0.001412  0.001504    True   \n",
              "2017-02-10  0.940026    0.085581  0.001504  0.003637  0.003869    True   \n",
              "...              ...         ...       ...       ...       ...     ...   \n",
              "2017-11-21  0.852006    0.032757 -0.000426 -0.005983 -0.007022   False   \n",
              "2017-11-22  0.846024    0.022044 -0.007022 -0.002142 -0.002532   False   \n",
              "2017-11-23  0.843882    0.018941 -0.002532 -0.005659 -0.006706   False   \n",
              "2017-11-24  0.838223    0.030054 -0.006706  0.002396  0.002858    True   \n",
              "2017-11-27  0.840619    0.032492  0.002858  0.004047  0.004815    True   \n",
              "2017-11-28  0.844666    0.044063  0.004815 -0.000570 -0.000675   False   \n",
              "2017-11-29  0.844096    0.060237 -0.000675 -0.003901 -0.004621   False   \n",
              "2017-11-30  0.840195    0.053413 -0.004621  0.000919  0.001093    True   \n",
              "2017-12-01  0.841114    0.049116  0.001093  0.001772  0.002107    True   \n",
              "2017-12-04  0.842886    0.090952  0.002107  0.002851  0.003383    True   \n",
              "2017-12-05  0.845737    0.072858  0.003383  0.002079  0.002459    True   \n",
              "2017-12-06  0.847817    0.033286  0.002459  0.001729  0.002039    True   \n",
              "2017-12-07  0.849545    0.065278  0.002039  0.000506  0.000595    True   \n",
              "2017-12-08  0.850051    0.085969  0.000595 -0.000289 -0.000340   False   \n",
              "2017-12-11  0.849762    0.036393 -0.000340  0.002027  0.002385    True   \n",
              "2017-12-12  0.851789    0.081947  0.002385 -0.006123 -0.007188   False   \n",
              "2017-12-13  0.845666    0.032359 -0.007188  0.003447  0.004076    True   \n",
              "2017-12-14  0.849113    0.040492  0.004076  0.001806  0.002127    True   \n",
              "2017-12-15  0.850919    0.047422  0.002127 -0.002095 -0.002462   False   \n",
              "2017-12-18  0.848824    0.046910 -0.002462 -0.004087 -0.004815   False   \n",
              "2017-12-19  0.844737    0.044138 -0.004815 -0.002206 -0.002612   False   \n",
              "2017-12-20  0.842531    0.065079 -0.002612 -0.000284 -0.000337   False   \n",
              "2017-12-21  0.842247    0.053057 -0.000337  0.001137  0.001349    True   \n",
              "2017-12-22  0.843384    0.050513  0.001349 -0.000782 -0.000927   False   \n",
              "2017-12-25  0.842602    0.055977 -0.000927  0.000711  0.000843    True   \n",
              "2017-12-26  0.843313    0.053010  0.000843 -0.001987 -0.002356   False   \n",
              "2017-12-27  0.841326    0.039179 -0.002356 -0.003875 -0.004606   False   \n",
              "2017-12-28  0.837451    0.041405 -0.004606 -0.003840 -0.004585   False   \n",
              "2017-12-29  0.833611    0.033295 -0.004585 -0.000833 -0.000999   False   \n",
              "2018-01-01  0.832778    0.051245 -0.000999 -0.003384 -0.004064   False   \n",
              "\n",
              "             Close30  Close100        r1        r2  marketrisk_avg30  \\\n",
              "Date                                                                   \n",
              "2017-01-02  0.948200  0.915872  0.005094 -0.002016          0.066802   \n",
              "2017-01-03  0.948863  0.916616  0.004872  0.004597          0.068402   \n",
              "2017-01-04  0.949074  0.917292 -0.007516 -0.003011          0.067669   \n",
              "2017-01-05  0.948922  0.917914 -0.010523 -0.013351          0.066586   \n",
              "2017-01-06  0.949083  0.918581  0.006538 -0.011224          0.066382   \n",
              "2017-01-09  0.949205  0.919202 -0.003773 -0.007394          0.066756   \n",
              "2017-01-10  0.949484  0.919830  0.001703 -0.003605          0.065723   \n",
              "2017-01-11  0.949499  0.920402 -0.002418 -0.011118          0.065729   \n",
              "2017-01-12  0.949646  0.920961 -0.002673 -0.018662          0.064580   \n",
              "2017-01-13  0.949731  0.921426 -0.002746 -0.013891          0.064965   \n",
              "2017-01-16  0.950205  0.921922  0.003635  0.000267          0.064260   \n",
              "2017-01-17  0.950219  0.922281 -0.009864 -0.016135          0.063088   \n",
              "2017-01-18  0.950572  0.922725  0.007201 -0.005161          0.062997   \n",
              "2017-01-19  0.950431  0.923171 -0.002911 -0.009775          0.063504   \n",
              "2017-01-20  0.950018  0.923552 -0.003156 -0.010513          0.062047   \n",
              "2017-01-23  0.949648  0.923871 -0.005385 -0.013225          0.060675   \n",
              "2017-01-24  0.949341  0.924304  0.002685 -0.007795          0.060226   \n",
              "2017-01-25  0.948725  0.924712 -0.001214 -0.012644          0.059850   \n",
              "2017-01-26  0.947922  0.925193  0.005578  0.002797          0.057834   \n",
              "2017-01-27  0.947185  0.925638 -0.001226 -0.005630          0.058025   \n",
              "2017-01-30  0.946316  0.926088  0.000175 -0.002544          0.056695   \n",
              "2017-01-31  0.945097  0.926436 -0.008924 -0.008312          0.054650   \n",
              "2017-02-01  0.944075  0.926834  0.002409 -0.000518          0.054057   \n",
              "2017-02-02  0.943116  0.927234  0.000777 -0.002426          0.053253   \n",
              "2017-02-03  0.942149  0.927543 -0.001983 -0.003194          0.053013   \n",
              "2017-02-06  0.941271  0.927896  0.002761 -0.006010          0.052926   \n",
              "2017-02-07  0.940599  0.928292  0.006010  0.001226          0.052020   \n",
              "2017-02-08  0.939731  0.928700 -0.001488 -0.000437          0.049903   \n",
              "2017-02-09  0.939242  0.929163  0.003773  0.012260          0.049062   \n",
              "2017-02-10  0.938869  0.929653  0.001412  0.011263          0.050080   \n",
              "...              ...       ...       ...       ...               ...   \n",
              "2017-11-21  0.853261  0.851177 -0.000363 -0.005406          0.046575   \n",
              "2017-11-22  0.853349  0.850828 -0.005983 -0.011242          0.046114   \n",
              "2017-11-23  0.853299  0.850512 -0.002142 -0.003863          0.045806   \n",
              "2017-11-24  0.853044  0.850122 -0.005659 -0.009882          0.045548   \n",
              "2017-11-27  0.852804  0.849755  0.002396 -0.009071          0.045056   \n",
              "2017-11-28  0.852629  0.849480  0.004047 -0.003295          0.044239   \n",
              "2017-11-29  0.852486  0.849157 -0.000570 -0.008274          0.044113   \n",
              "2017-11-30  0.852363  0.848783 -0.003901 -0.011812          0.043694   \n",
              "2017-12-01  0.852111  0.848473  0.000919 -0.004910          0.042181   \n",
              "2017-12-04  0.851831  0.848190  0.001772 -0.000996          0.042438   \n",
              "2017-12-05  0.851675  0.847991  0.002851  0.007515          0.043048   \n",
              "2017-12-06  0.851716  0.847784  0.002079  0.007198          0.042090   \n",
              "2017-12-07  0.851422  0.847680  0.001729  0.004880          0.042056   \n",
              "2017-12-08  0.851041  0.847605  0.000506  0.005955          0.042609   \n",
              "2017-12-11  0.850752  0.847510 -0.000289  0.009567          0.042542   \n",
              "2017-12-12  0.850517  0.847445  0.002027  0.010675          0.044133   \n",
              "2017-12-13  0.850013  0.847378 -0.006123  0.002780          0.042946   \n",
              "2017-12-14  0.849719  0.847304  0.003447  0.003375          0.043390   \n",
              "2017-12-15  0.849365  0.847303  0.001806  0.003102          0.043575   \n",
              "2017-12-18  0.848945  0.847345 -0.002095 -0.000721          0.042431   \n",
              "2017-12-19  0.848330  0.847319 -0.004087 -0.005314          0.041673   \n",
              "2017-12-20  0.847662  0.847308 -0.002206 -0.007231          0.043813   \n",
              "2017-12-21  0.847100  0.847305 -0.000284 -0.009542          0.045579   \n",
              "2017-12-22  0.846632  0.847241  0.001137 -0.002282          0.046792   \n",
              "2017-12-25  0.846143  0.847187 -0.000782 -0.006511          0.048229   \n",
              "2017-12-26  0.845996  0.847109  0.000711 -0.007606          0.048635   \n",
              "2017-12-27  0.845770  0.847017 -0.001987 -0.007498          0.048731   \n",
              "2017-12-28  0.845362  0.846895 -0.003875 -0.007286          0.048284   \n",
              "2017-12-29  0.844883  0.846771 -0.003840 -0.008920          0.048487   \n",
              "2018-01-01  0.844230  0.846608 -0.000833 -0.009469          0.048796   \n",
              "\n",
              "            marketrisk_avg90 predictSignal  predictedweight  labelledweight  \\\n",
              "Date                                                                          \n",
              "2017-01-02          0.065010         False               -1               1   \n",
              "2017-01-03          0.065185         False               -1              -1   \n",
              "2017-01-04          0.064691         False               -1              -1   \n",
              "2017-01-05          0.064512         False               -1               1   \n",
              "2017-01-06          0.064718         False               -1              -1   \n",
              "2017-01-09          0.064769         False               -1               1   \n",
              "2017-01-10          0.064691         False               -1              -1   \n",
              "2017-01-11          0.064932          True                1              -1   \n",
              "2017-01-12          0.064800          True                1              -1   \n",
              "2017-01-13          0.064543          True                1               1   \n",
              "2017-01-16          0.064488         False               -1              -1   \n",
              "2017-01-17          0.064473         False               -1               1   \n",
              "2017-01-18          0.064406          True                1              -1   \n",
              "2017-01-19          0.064415          True                1              -1   \n",
              "2017-01-20          0.064140          True                1              -1   \n",
              "2017-01-23          0.063988         False               -1               1   \n",
              "2017-01-24          0.064062         False               -1              -1   \n",
              "2017-01-25          0.064147         False               -1               1   \n",
              "2017-01-26          0.064095         False               -1              -1   \n",
              "2017-01-27          0.064731         False               -1               1   \n",
              "2017-01-30          0.064603         False               -1              -1   \n",
              "2017-01-31          0.064372         False               -1               1   \n",
              "2017-02-01          0.064102         False               -1               1   \n",
              "2017-02-02          0.063772         False               -1              -1   \n",
              "2017-02-03          0.063526         False               -1               1   \n",
              "2017-02-06          0.063153          True                1               1   \n",
              "2017-02-07          0.062957          True                1              -1   \n",
              "2017-02-08          0.062438          True                1               1   \n",
              "2017-02-09          0.061897          True                1               1   \n",
              "2017-02-10          0.062181          True                1               1   \n",
              "...                      ...           ...              ...             ...   \n",
              "2017-11-21          0.046909             0               -1              -1   \n",
              "2017-11-22          0.046822             0               -1              -1   \n",
              "2017-11-23          0.046652             0               -1              -1   \n",
              "2017-11-24          0.046685             0               -1               1   \n",
              "2017-11-27          0.046735             0               -1               1   \n",
              "2017-11-28          0.046910             0               -1              -1   \n",
              "2017-11-29          0.047140             0               -1              -1   \n",
              "2017-11-30          0.047326             0               -1               1   \n",
              "2017-12-01          0.047390             0               -1               1   \n",
              "2017-12-04          0.047974             0               -1               1   \n",
              "2017-12-05          0.048544             0               -1               1   \n",
              "2017-12-06          0.048541             0               -1               1   \n",
              "2017-12-07          0.048816             0               -1               1   \n",
              "2017-12-08          0.049215             0               -1              -1   \n",
              "2017-12-11          0.048944             0               -1               1   \n",
              "2017-12-12          0.049259             0               -1              -1   \n",
              "2017-12-13          0.049144             0               -1               1   \n",
              "2017-12-14          0.049264             0               -1               1   \n",
              "2017-12-15          0.049625             0               -1              -1   \n",
              "2017-12-18          0.049620             0               -1              -1   \n",
              "2017-12-19          0.049359             0               -1              -1   \n",
              "2017-12-20          0.049362             0               -1              -1   \n",
              "2017-12-21          0.049593             0               -1               1   \n",
              "2017-12-22          0.049929             0               -1              -1   \n",
              "2017-12-25          0.050399             0               -1               1   \n",
              "2017-12-26          0.050599             0               -1              -1   \n",
              "2017-12-27          0.050775             0               -1              -1   \n",
              "2017-12-28          0.050967             0               -1              -1   \n",
              "2017-12-29          0.050887             0               -1              -1   \n",
              "2018-01-01          0.051115             0               -1              -1   \n",
              "\n",
              "                   r  predictedreturns  labelledreturns  \n",
              "Date                                                     \n",
              "2017-01-02       NaN               NaN              NaN  \n",
              "2017-01-03  0.005094         -0.005094         0.005094  \n",
              "2017-01-04 -0.007820          0.007820         0.007820  \n",
              "2017-01-05 -0.011035          0.011035         0.011035  \n",
              "2017-01-06  0.006933         -0.006933         0.006933  \n",
              "2017-01-09 -0.003973          0.003973         0.003973  \n",
              "2017-01-10  0.001800         -0.001800         0.001800  \n",
              "2017-01-11 -0.002552          0.002552         0.002552  \n",
              "2017-01-12 -0.002828         -0.002828         0.002828  \n",
              "2017-01-13 -0.002913         -0.002913         0.002913  \n",
              "2017-01-16  0.003868          0.003868         0.003868  \n",
              "2017-01-17 -0.010456          0.010456         0.010456  \n",
              "2017-01-18  0.007714         -0.007714         0.007714  \n",
              "2017-01-19 -0.003095         -0.003095         0.003095  \n",
              "2017-01-20 -0.003365         -0.003365         0.003365  \n",
              "2017-01-23 -0.005762         -0.005762         0.005762  \n",
              "2017-01-24  0.002889         -0.002889         0.002889  \n",
              "2017-01-25 -0.001303          0.001303         0.001303  \n",
              "2017-01-26  0.005993         -0.005993         0.005993  \n",
              "2017-01-27 -0.001309          0.001309         0.001309  \n",
              "2017-01-30  0.000187         -0.000187         0.000187  \n",
              "2017-01-31 -0.009541          0.009541         0.009541  \n",
              "2017-02-01  0.002601         -0.002601         0.002601  \n",
              "2017-02-02  0.000837         -0.000837         0.000837  \n",
              "2017-02-03 -0.002133          0.002133         0.002133  \n",
              "2017-02-06  0.002977         -0.002977         0.002977  \n",
              "2017-02-07  0.006461          0.006461         0.006461  \n",
              "2017-02-08 -0.001589         -0.001589         0.001589  \n",
              "2017-02-09  0.004036          0.004036         0.004036  \n",
              "2017-02-10  0.001504          0.001504         0.001504  \n",
              "...              ...               ...              ...  \n",
              "2017-11-21 -0.000426          0.000426         0.000426  \n",
              "2017-11-22 -0.007022          0.007022         0.007022  \n",
              "2017-11-23 -0.002532          0.002532         0.002532  \n",
              "2017-11-24 -0.006706          0.006706         0.006706  \n",
              "2017-11-27  0.002858         -0.002858         0.002858  \n",
              "2017-11-28  0.004815         -0.004815         0.004815  \n",
              "2017-11-29 -0.000675          0.000675         0.000675  \n",
              "2017-11-30 -0.004621          0.004621         0.004621  \n",
              "2017-12-01  0.001093         -0.001093         0.001093  \n",
              "2017-12-04  0.002107         -0.002107         0.002107  \n",
              "2017-12-05  0.003383         -0.003383         0.003383  \n",
              "2017-12-06  0.002459         -0.002459         0.002459  \n",
              "2017-12-07  0.002039         -0.002039         0.002039  \n",
              "2017-12-08  0.000595         -0.000595         0.000595  \n",
              "2017-12-11 -0.000340          0.000340         0.000340  \n",
              "2017-12-12  0.002385         -0.002385         0.002385  \n",
              "2017-12-13 -0.007188          0.007188         0.007188  \n",
              "2017-12-14  0.004076         -0.004076         0.004076  \n",
              "2017-12-15  0.002127         -0.002127         0.002127  \n",
              "2017-12-18 -0.002462          0.002462         0.002462  \n",
              "2017-12-19 -0.004815          0.004815         0.004815  \n",
              "2017-12-20 -0.002612          0.002612         0.002612  \n",
              "2017-12-21 -0.000337          0.000337         0.000337  \n",
              "2017-12-22  0.001349         -0.001349         0.001349  \n",
              "2017-12-25 -0.000927          0.000927         0.000927  \n",
              "2017-12-26  0.000843         -0.000843         0.000843  \n",
              "2017-12-27 -0.002356          0.002356         0.002356  \n",
              "2017-12-28 -0.004606          0.004606         0.004606  \n",
              "2017-12-29 -0.004585          0.004585         0.004585  \n",
              "2018-01-01 -0.000999          0.000999         0.000999  \n",
              "\n",
              "[261 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "XP6EtlYYSjdd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define confusion matrix function\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "#     plt.figure(figsize= (10,10))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "#     plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"black\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    \n",
        "def plotConfusion(start, end, string):\n",
        "  real = tradeTable[\"labelledweight\"]\n",
        "  pred = tradeTable[\"predictedweight\"]\n",
        "  cnf_matrix = confusion_matrix(real[start:end], pred[start:end])\n",
        "  accuracy = np.around(accuracy_score(real[start:end], pred[start:end]),2)\n",
        "  f1= np.around(f1_score(real[start:end], pred[start:end], average='binary'), 2)\n",
        "  auc = np.around(areauc(real[start:end], pred[start:end]), 2)\n",
        "  prec = np.around(precision_score(real[start:end], pred[start:end], average='binary'), 2)\n",
        "  recall = np.around(recall_score(real[start:end], pred[start:end], average='binary'), 2)\n",
        "  mcc =  np.around(matthews_corrcoef(real[start:end], pred[start:end]), 2)\n",
        "  # np.set_printoptions(precision=2)\n",
        "\n",
        "\n",
        "  plot_confusion_matrix(cnf_matrix, classes=['Sell', 'buy'],\n",
        "                        title=string + ' Accuracy: ' +str(accuracy)\n",
        "                        + ', AUC:' + str(auc)\n",
        "#                         + ', F1 Score:' + str(f1)\n",
        "#                         + ', Precison:' + str(prec)\n",
        "#                         + ', Recall:' + str(recall)\n",
        "                        + ', MCC:' + str(mcc))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}