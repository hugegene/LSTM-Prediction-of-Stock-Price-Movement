{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentimentTestinUSDLoop.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hugegene/LSTM-Prediction-of-Stock-Price-Movement/blob/master/SentimentTestingUSDRollingWindow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "dIF_jv42m9pH"
      },
      "cell_type": "markdown",
      "source": [
        "# Rolling LSTM testing on USD price movements"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "hOXobOBeKwDe"
      },
      "cell_type": "markdown",
      "source": [
        "## 1 Objective and Overview\n",
        "\n",
        "A supervised learning was done to make trading decisions (Sell or Buy) for USD using USD's past price and MarketRisk TRMI.\n",
        "\n",
        "The supervised model used was LSTM.  \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "mPEtY17IU0Qs"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.1 Splitting Train-Validate-Test Set\n",
        "\n",
        "2016-11-01 data to 2017-12-31 USD price and TRMI MarketRisk data was used for this experiment.\n",
        "\n",
        "The data is plit to rolling train and test sets as shown:\n",
        "\n",
        "\n",
        "The effective test period is from 2017-01-01 to 2017-12-31\n",
        "\n",
        "The graph below shows the USD price over the effective test period.\n"
      ]
    },
    {
      "metadata": {
        "id": "OjlMoZGeFd32",
        "colab_type": "code",
        "outputId": "89bd273d-c246-451b-af3c-31221d4694e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        }
      },
      "cell_type": "code",
      "source": [
        "# Plotting USD prices\n",
        "ax = tradeTable[\"2017-01-01\":\"2017-12-31\"].plot(y=['Close'], figsize=(12,6), grid=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/plotting/_core.py:1716: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
            "  series.name = label\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsYAAAF/CAYAAABKRQ+VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCEASYyGEEEIIIQBJjIUQQgghhADg/wcJbaq6pgv24AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f16032137f0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "PCC2x8HuU4Y3"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.2 Feature Engineering"
      ]
    },
    {
      "metadata": {
        "id": "tYnpeunFweP9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://github.com/hugegene/LSTM-Prediction-of-Stock-Price-Movement/blob/master/USDprice.png?raw=1)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "NHFZ6CcMVDaX"
      },
      "cell_type": "markdown",
      "source": [
        "USD price was overlayed with various TRMIs moving averages and the 30-days MarketRisk moving average seems to co-related well with the price visually. As such, the following was feature-enginnered as the input to the LSTM model for all the experiments:\n",
        "\n",
        "1. 90-day MarketRisk moving-average\n",
        "2. 30-day MarketRisk moving-average\n",
        "3. Close\n",
        "4. 100-day Close moving-average\n",
        "\n",
        "## 1.3 Evaluation Metrics\n",
        "\n",
        "The evaluation metrics are:\n",
        "1. Matthews correlation coefficient (MCC)\n",
        "2. Accuracy\n",
        "3. Profit and Loss\n",
        "\n",
        "\n",
        "## 1.4 Labels\n",
        "\n",
        "Each timestep is labelled Sell(0) or Buy(1) depending on whether the next closing price is up or down.\n",
        "\n",
        "\n",
        "## 1.5 LSTM Set-Up\n",
        "\n",
        "The LSTM is set-up to predict the labels based on the sequential data of the past 5 time-steps.\n",
        "\n",
        "\n",
        "## 1.6 Trading\n",
        "\n",
        "Each LSTM prediction of Buy will result in a buying and holding of USD for 1 day  and each LSTM prediction of Sell will result in a short-sell and holding of USD for 1 day."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "2c-AjZEwWonj"
      },
      "cell_type": "markdown",
      "source": [
        "## 2 Results "
      ]
    },
    {
      "metadata": {
        "id": "9X8agEoisJKp",
        "colab_type": "code",
        "outputId": "624d7a3f-81e3-46d7-e7a6-3f6c57c8d463",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 893
        }
      },
      "cell_type": "code",
      "source": [
        "# calculating cumulative predicted and labelled trade returns over each time step\n",
        "PNLTable =  tradeTable.loc[\"2017-01-01\":\"2017-02-28\",:].copy()\n",
        "PNLTable[\"labelledPNL\"] =  (1+PNLTable['labelledreturns']).cumprod()\n",
        "PNLTable[\"PNL\"] = (1+PNLTable['predictedreturns']).cumprod()\n",
        "PNLTable.head()\n",
        "\n",
        "#plotting Confusion Matrix\n",
        "f = plt.figure(figsize=(15,7))\n",
        "ax = f.add_subplot(131)\n",
        "plotConfusion(\"2017-01-01\", \"2017-02-28\", \"\")\n",
        "\n",
        "f = plt.figure(figsize=(36,6))\n",
        "ax = f.add_subplot(131)\n",
        "\n",
        "# PNL Plot for the Test Period\n",
        "PNLTable[\"PNL\"].plot()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[ 9 11]\n",
            " [ 9 13]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f20f846dd68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAGPCAYAAABbFLtAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nw/w/bK4h9GscnesAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f20f6dfb9e8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAn0AAAGWCAYAAAANGDURAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f20cd066fd0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "PI1G2mO9bdAE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.1 Evaluation\n",
        "\n",
        "**Good**\n",
        "\n",
        "In the experiment, 8 LSTM models were trained to predict 1 week of USD price movement ahead. The LSTM model fitted the train set as much as it could and was used to predict the next week's USD price movements. \n",
        "\n",
        "The above confusion matrix shows a good spread of predicted Buy and Sell decisions with a accuracy of 0.57 and MCC of 0.15.\n",
        "\n",
        "The above PNL plot also shows that the 8 LSTM trained at one go, without any cherry picking of LSTM models, gives rise to 3% profit in 2 months.\n",
        "\n",
        "**Bad**\n",
        "\n",
        "However, more testing is required to prove that the above methodology is not one-off event. \n",
        "More testings with longer rolling periods of test sets are required. \n",
        "Repeated testing of the same period of test sets are required to ensure that the repeated test will produce the same results.\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "p1nongBRVFzR"
      },
      "cell_type": "markdown",
      "source": [
        "##  3 Codes"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7qHBVmabJWCD",
        "outputId": "9918a960-1287-4126-ed10-74ea8ea4b859",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 38
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d497adf4-c9af-464f-973f-7adfbf1e7472\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-d497adf4-c9af-464f-973f-7adfbf1e7472\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "HGRXzvUR0X11",
        "outputId": "980cec40-7c2c-4a8c-a0fb-a67b9aa296bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  USDprices.csv  USDSentiment.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "MxvJZZarlEe6",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from math import sqrt\n",
        "from numpy import concatenate\n",
        "from matplotlib import pyplot as plt\n",
        "from pandas import read_csv\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import model_from_json\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from datetime import datetime\n",
        "import os\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from keras.layers import Input, LSTM, RepeatVector, Lambda, Dense, Flatten, Permute, merge, multiply\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.metrics import roc_auc_score as areauc\n",
        "import tensorflow as tf\n",
        "from datetime import timedelta, date"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "H0U6XhwlpraX"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.1 Data Preparation"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "CNggyVu2lcl4",
        "outputId": "75ea6739-791a-4f86-e94c-2841ad6400ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "cell_type": "code",
      "source": [
        "##Read in Price data, use date as index\n",
        "Prices = pd.read_csv(\"USDprices.csv\")\n",
        "Prices.head()\n",
        "Prices['Date'].dtype\n",
        "Prices['Date'] = pd.to_datetime(Prices['Date'], format='%Y-%m-%d')\n",
        "Prices.head()\n",
        "Prices.index = Prices['Date']\n",
        "# print(Prices.head())\n",
        "\n",
        "#Reading in Sentiment Data and selecting News-only-MarketRisk Sentiment\n",
        "Sent = pd.read_csv(\"USDSentiment.csv\")\n",
        "Sent = Sent[Sent.dataType=='News'][['Date', 'marketRisk']].fillna(method = \"ffill\")\n",
        "# Sent.head()\n",
        "Sent['Date'] = pd.to_datetime(Sent['Date'], format='%Y-%m-%d')\n",
        "Sent.index = Sent['Date']\n",
        "# print(Sent.head())\n",
        "\n",
        "#Combining Price table and Sentiment Table\n",
        "Ana = Prices.merge(Sent, left_index= True, right_index=True, how='inner')\n",
        "Ana.head()\n",
        "Ana.columns\n",
        "Ana = Ana.drop(columns=['Unnamed: 0', 'Volume', 'Asset', 'Date_x', 'Date_y', 'Open', 'High', 'Low', 'UnadjClose'])\n",
        "Ana.head()\n",
        "\n",
        "# calculating r, %tage change of closing price over previous time step\n",
        "Ana['returns'] = (Ana['Close']- Ana['Close'].shift(1))/Ana['Close'].shift(1)\n",
        "Ana.head()\n",
        "\n",
        "# calculating target price\n",
        "Ana['Target'] = Ana[\"Close\"].diff(1).shift(-1)\n",
        "Ana.head()\n",
        "\n",
        "# calculating %change\n",
        "Ana['Change'] = Ana[\"returns\"].shift(-1)\n",
        "Ana.head()\n",
        "\n",
        "\n",
        "## Label each time step False(no-buy) or True(buy) based on whether the price will rise at the next closing price\n",
        "timestep = 3\n",
        "labels = []\n",
        "for i in range(0, Ana.shape[0]):\n",
        "  if(i+timestep< Ana.shape[0]):\n",
        "    aheadGain = [Ana[\"returns\"][i+j] for j in range(1,timestep+1)]\n",
        "    labels+= [np.sum(aheadGain)> 0]\n",
        "\n",
        "print(len(labels))\n",
        "\n",
        "# Price Table with Labels as Signal\n",
        "Ana = Ana.iloc[:-timestep,:].copy()\n",
        "Ana.head()\n",
        "Ana['Signal'] = labels\n",
        "Ana.head()\n",
        "\n",
        "#Feature Engineering\n",
        "Ana[\"Close30\"] =  Ana[\"Close\"].rolling(30).mean()\n",
        "Ana[\"Close100\"] =  Ana[\"Close\"].rolling(100).mean()\n",
        "Ana[\"r1\"] =  Ana[\"Close\"].diff(1)\n",
        "Ana[\"r2\"] =  Ana[\"Close\"].diff(7)\n",
        "Ana[\"marketrisk_avg30\"] = Ana[\"marketRisk\"].rolling(30).mean()\n",
        "Ana[\"marketrisk_avg90\"] = Ana[\"marketRisk\"].rolling(90).mean()\n",
        "\n",
        "#Drop NA\n",
        "Ana = Ana.dropna(0)\n",
        "Ana.head()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5446\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Close</th>\n",
              "      <th>marketRisk</th>\n",
              "      <th>returns</th>\n",
              "      <th>Target</th>\n",
              "      <th>Change</th>\n",
              "      <th>Signal</th>\n",
              "      <th>Close30</th>\n",
              "      <th>Close100</th>\n",
              "      <th>r1</th>\n",
              "      <th>r2</th>\n",
              "      <th>marketrisk_avg30</th>\n",
              "      <th>marketrisk_avg90</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1998-05-21</th>\n",
              "      <td>0.894694</td>\n",
              "      <td>0.017921</td>\n",
              "      <td>-0.005816</td>\n",
              "      <td>-0.001439</td>\n",
              "      <td>-0.001608</td>\n",
              "      <td>True</td>\n",
              "      <td>0.905687</td>\n",
              "      <td>0.916241</td>\n",
              "      <td>-0.005234</td>\n",
              "      <td>-0.007833</td>\n",
              "      <td>0.030184</td>\n",
              "      <td>0.034995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998-05-22</th>\n",
              "      <td>0.893256</td>\n",
              "      <td>0.054887</td>\n",
              "      <td>-0.001608</td>\n",
              "      <td>0.004813</td>\n",
              "      <td>0.005388</td>\n",
              "      <td>True</td>\n",
              "      <td>0.904816</td>\n",
              "      <td>0.916045</td>\n",
              "      <td>-0.001439</td>\n",
              "      <td>-0.011721</td>\n",
              "      <td>0.030500</td>\n",
              "      <td>0.035283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998-05-25</th>\n",
              "      <td>0.898069</td>\n",
              "      <td>0.072076</td>\n",
              "      <td>0.005388</td>\n",
              "      <td>0.001373</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>True</td>\n",
              "      <td>0.904089</td>\n",
              "      <td>0.915792</td>\n",
              "      <td>0.004813</td>\n",
              "      <td>-0.005518</td>\n",
              "      <td>0.031202</td>\n",
              "      <td>0.035543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998-05-26</th>\n",
              "      <td>0.899442</td>\n",
              "      <td>0.120229</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.006027</td>\n",
              "      <td>0.006700</td>\n",
              "      <td>True</td>\n",
              "      <td>0.903746</td>\n",
              "      <td>0.915530</td>\n",
              "      <td>0.001373</td>\n",
              "      <td>-0.007587</td>\n",
              "      <td>0.034618</td>\n",
              "      <td>0.036311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998-05-27</th>\n",
              "      <td>0.905469</td>\n",
              "      <td>0.057863</td>\n",
              "      <td>0.006700</td>\n",
              "      <td>-0.001718</td>\n",
              "      <td>-0.001898</td>\n",
              "      <td>False</td>\n",
              "      <td>0.903617</td>\n",
              "      <td>0.915352</td>\n",
              "      <td>0.006027</td>\n",
              "      <td>-0.001643</td>\n",
              "      <td>0.036330</td>\n",
              "      <td>0.036533</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Close  marketRisk   returns    Target    Change  Signal  \\\n",
              "Date                                                                     \n",
              "1998-05-21  0.894694    0.017921 -0.005816 -0.001439 -0.001608    True   \n",
              "1998-05-22  0.893256    0.054887 -0.001608  0.004813  0.005388    True   \n",
              "1998-05-25  0.898069    0.072076  0.005388  0.001373  0.001529    True   \n",
              "1998-05-26  0.899442    0.120229  0.001529  0.006027  0.006700    True   \n",
              "1998-05-27  0.905469    0.057863  0.006700 -0.001718 -0.001898   False   \n",
              "\n",
              "             Close30  Close100        r1        r2  marketrisk_avg30  \\\n",
              "Date                                                                   \n",
              "1998-05-21  0.905687  0.916241 -0.005234 -0.007833          0.030184   \n",
              "1998-05-22  0.904816  0.916045 -0.001439 -0.011721          0.030500   \n",
              "1998-05-25  0.904089  0.915792  0.004813 -0.005518          0.031202   \n",
              "1998-05-26  0.903746  0.915530  0.001373 -0.007587          0.034618   \n",
              "1998-05-27  0.903617  0.915352  0.006027 -0.001643          0.036330   \n",
              "\n",
              "            marketrisk_avg90  \n",
              "Date                          \n",
              "1998-05-21          0.034995  \n",
              "1998-05-22          0.035283  \n",
              "1998-05-25          0.035543  \n",
              "1998-05-26          0.036311  \n",
              "1998-05-27          0.036533  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "HInEXnin29AV",
        "outputId": "13d224b6-e3ca-4a80-8dcd-6e06fb621f2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        }
      },
      "cell_type": "code",
      "source": [
        "#Define function to convert dataframe to training, validating and testing examples\n",
        "def series_to_supervised(df, n_in=1, n_out=1):\n",
        "\tn_vars = 1 if type(df) is list else df.shape[1]\n",
        "  \n",
        "  \n",
        "\tcols, names = list(), list()\n",
        "\t# input sequence (t-n, ... t-1)\n",
        "\tfor i in range(n_in, -1, -1):\n",
        "\t\tcols.append(df.shift(i))\n",
        "\t\tnames += [(df.columns[j]+'(t-%d)' % (i)) for j in range(n_vars)]\n",
        "\n",
        "\t# put it all together\n",
        "\tagg = concat(cols, axis=1)\n",
        "\tagg.columns = names\n",
        "\treturn agg\n",
        "\n",
        "#Features Selection\n",
        "Set =  Ana[['Close', 'Close100', 'marketrisk_avg90', 'marketrisk_avg30']]\n",
        "# Set =  Ana[['r1', 'r2']]\n",
        "FEATURES_SHAPE =  Set.shape[1]\n",
        "print(FEATURES_SHAPE)\n",
        "\n",
        "#Forming examples\n",
        "SEQ_LEN = 5\n",
        "Set =  series_to_supervised(Set, SEQ_LEN-1, 0)\n",
        "Set[[\"Change\", \"Signal\"]] = Ana[[\"Change\", \"Signal\"]]\n",
        "Set = Set.dropna()\n",
        "print(Set.shape)\n",
        "Set.head()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n",
            "(5343, 22)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Close(t-4)</th>\n",
              "      <th>Close100(t-4)</th>\n",
              "      <th>marketrisk_avg90(t-4)</th>\n",
              "      <th>marketrisk_avg30(t-4)</th>\n",
              "      <th>Close(t-3)</th>\n",
              "      <th>Close100(t-3)</th>\n",
              "      <th>marketrisk_avg90(t-3)</th>\n",
              "      <th>marketrisk_avg30(t-3)</th>\n",
              "      <th>Close(t-2)</th>\n",
              "      <th>Close100(t-2)</th>\n",
              "      <th>...</th>\n",
              "      <th>Close(t-1)</th>\n",
              "      <th>Close100(t-1)</th>\n",
              "      <th>marketrisk_avg90(t-1)</th>\n",
              "      <th>marketrisk_avg30(t-1)</th>\n",
              "      <th>Close(t-0)</th>\n",
              "      <th>Close100(t-0)</th>\n",
              "      <th>marketrisk_avg90(t-0)</th>\n",
              "      <th>marketrisk_avg30(t-0)</th>\n",
              "      <th>Change</th>\n",
              "      <th>Signal</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1998-05-27</th>\n",
              "      <td>0.894694</td>\n",
              "      <td>0.916241</td>\n",
              "      <td>0.034995</td>\n",
              "      <td>0.030184</td>\n",
              "      <td>0.893256</td>\n",
              "      <td>0.916045</td>\n",
              "      <td>0.035283</td>\n",
              "      <td>0.030500</td>\n",
              "      <td>0.898069</td>\n",
              "      <td>0.915792</td>\n",
              "      <td>...</td>\n",
              "      <td>0.899442</td>\n",
              "      <td>0.915530</td>\n",
              "      <td>0.036311</td>\n",
              "      <td>0.034618</td>\n",
              "      <td>0.905469</td>\n",
              "      <td>0.915352</td>\n",
              "      <td>0.036533</td>\n",
              "      <td>0.036330</td>\n",
              "      <td>-0.001898</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998-05-28</th>\n",
              "      <td>0.893256</td>\n",
              "      <td>0.916045</td>\n",
              "      <td>0.035283</td>\n",
              "      <td>0.030500</td>\n",
              "      <td>0.898069</td>\n",
              "      <td>0.915792</td>\n",
              "      <td>0.035543</td>\n",
              "      <td>0.031202</td>\n",
              "      <td>0.899442</td>\n",
              "      <td>0.915530</td>\n",
              "      <td>...</td>\n",
              "      <td>0.905469</td>\n",
              "      <td>0.915352</td>\n",
              "      <td>0.036533</td>\n",
              "      <td>0.036330</td>\n",
              "      <td>0.903751</td>\n",
              "      <td>0.915163</td>\n",
              "      <td>0.036895</td>\n",
              "      <td>0.035828</td>\n",
              "      <td>0.003719</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998-05-29</th>\n",
              "      <td>0.898069</td>\n",
              "      <td>0.915792</td>\n",
              "      <td>0.035543</td>\n",
              "      <td>0.031202</td>\n",
              "      <td>0.899442</td>\n",
              "      <td>0.915530</td>\n",
              "      <td>0.036311</td>\n",
              "      <td>0.034618</td>\n",
              "      <td>0.905469</td>\n",
              "      <td>0.915352</td>\n",
              "      <td>...</td>\n",
              "      <td>0.903751</td>\n",
              "      <td>0.915163</td>\n",
              "      <td>0.036895</td>\n",
              "      <td>0.035828</td>\n",
              "      <td>0.907112</td>\n",
              "      <td>0.915000</td>\n",
              "      <td>0.037338</td>\n",
              "      <td>0.036237</td>\n",
              "      <td>-0.004515</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998-06-01</th>\n",
              "      <td>0.899442</td>\n",
              "      <td>0.915530</td>\n",
              "      <td>0.036311</td>\n",
              "      <td>0.034618</td>\n",
              "      <td>0.905469</td>\n",
              "      <td>0.915352</td>\n",
              "      <td>0.036533</td>\n",
              "      <td>0.036330</td>\n",
              "      <td>0.903751</td>\n",
              "      <td>0.915163</td>\n",
              "      <td>...</td>\n",
              "      <td>0.907112</td>\n",
              "      <td>0.915000</td>\n",
              "      <td>0.037338</td>\n",
              "      <td>0.036237</td>\n",
              "      <td>0.903016</td>\n",
              "      <td>0.914801</td>\n",
              "      <td>0.037892</td>\n",
              "      <td>0.038099</td>\n",
              "      <td>0.000813</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998-06-02</th>\n",
              "      <td>0.905469</td>\n",
              "      <td>0.915352</td>\n",
              "      <td>0.036533</td>\n",
              "      <td>0.036330</td>\n",
              "      <td>0.903751</td>\n",
              "      <td>0.915163</td>\n",
              "      <td>0.036895</td>\n",
              "      <td>0.035828</td>\n",
              "      <td>0.907112</td>\n",
              "      <td>0.915000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.903016</td>\n",
              "      <td>0.914801</td>\n",
              "      <td>0.037892</td>\n",
              "      <td>0.038099</td>\n",
              "      <td>0.903751</td>\n",
              "      <td>0.914626</td>\n",
              "      <td>0.038163</td>\n",
              "      <td>0.038561</td>\n",
              "      <td>-0.005840</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 22 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Close(t-4)  Close100(t-4)  marketrisk_avg90(t-4)  \\\n",
              "Date                                                           \n",
              "1998-05-27    0.894694       0.916241               0.034995   \n",
              "1998-05-28    0.893256       0.916045               0.035283   \n",
              "1998-05-29    0.898069       0.915792               0.035543   \n",
              "1998-06-01    0.899442       0.915530               0.036311   \n",
              "1998-06-02    0.905469       0.915352               0.036533   \n",
              "\n",
              "            marketrisk_avg30(t-4)  Close(t-3)  Close100(t-3)  \\\n",
              "Date                                                           \n",
              "1998-05-27               0.030184    0.893256       0.916045   \n",
              "1998-05-28               0.030500    0.898069       0.915792   \n",
              "1998-05-29               0.031202    0.899442       0.915530   \n",
              "1998-06-01               0.034618    0.905469       0.915352   \n",
              "1998-06-02               0.036330    0.903751       0.915163   \n",
              "\n",
              "            marketrisk_avg90(t-3)  marketrisk_avg30(t-3)  Close(t-2)  \\\n",
              "Date                                                                   \n",
              "1998-05-27               0.035283               0.030500    0.898069   \n",
              "1998-05-28               0.035543               0.031202    0.899442   \n",
              "1998-05-29               0.036311               0.034618    0.905469   \n",
              "1998-06-01               0.036533               0.036330    0.903751   \n",
              "1998-06-02               0.036895               0.035828    0.907112   \n",
              "\n",
              "            Close100(t-2)   ...    Close(t-1)  Close100(t-1)  \\\n",
              "Date                        ...                                \n",
              "1998-05-27       0.915792   ...      0.899442       0.915530   \n",
              "1998-05-28       0.915530   ...      0.905469       0.915352   \n",
              "1998-05-29       0.915352   ...      0.903751       0.915163   \n",
              "1998-06-01       0.915163   ...      0.907112       0.915000   \n",
              "1998-06-02       0.915000   ...      0.903016       0.914801   \n",
              "\n",
              "            marketrisk_avg90(t-1)  marketrisk_avg30(t-1)  Close(t-0)  \\\n",
              "Date                                                                   \n",
              "1998-05-27               0.036311               0.034618    0.905469   \n",
              "1998-05-28               0.036533               0.036330    0.903751   \n",
              "1998-05-29               0.036895               0.035828    0.907112   \n",
              "1998-06-01               0.037338               0.036237    0.903016   \n",
              "1998-06-02               0.037892               0.038099    0.903751   \n",
              "\n",
              "            Close100(t-0)  marketrisk_avg90(t-0)  marketrisk_avg30(t-0)  \\\n",
              "Date                                                                      \n",
              "1998-05-27       0.915352               0.036533               0.036330   \n",
              "1998-05-28       0.915163               0.036895               0.035828   \n",
              "1998-05-29       0.915000               0.037338               0.036237   \n",
              "1998-06-01       0.914801               0.037892               0.038099   \n",
              "1998-06-02       0.914626               0.038163               0.038561   \n",
              "\n",
              "              Change  Signal  \n",
              "Date                          \n",
              "1998-05-27 -0.001898   False  \n",
              "1998-05-28  0.003719    True  \n",
              "1998-05-29 -0.004515   False  \n",
              "1998-06-01  0.000813   False  \n",
              "1998-06-02 -0.005840   False  \n",
              "\n",
              "[5 rows x 22 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "metadata": {
        "id": "9ar94vy_ip_7",
        "colab_type": "code",
        "outputId": "c1cdd20d-7e7c-4ba7-9d84-4f0077e35fe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "cell_type": "code",
      "source": [
        "#Remove bad training and validating examples\n",
        "SetAdjusted = Set.copy()\n",
        "# SetAdjusted = Set.loc[(Set.Change< -0.0015)| (Set.Change> 0.003)]\n",
        "SetAdjusted.head()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Close(t-4)</th>\n",
              "      <th>Close100(t-4)</th>\n",
              "      <th>marketrisk_avg90(t-4)</th>\n",
              "      <th>marketrisk_avg30(t-4)</th>\n",
              "      <th>Close(t-3)</th>\n",
              "      <th>Close100(t-3)</th>\n",
              "      <th>marketrisk_avg90(t-3)</th>\n",
              "      <th>marketrisk_avg30(t-3)</th>\n",
              "      <th>Close(t-2)</th>\n",
              "      <th>Close100(t-2)</th>\n",
              "      <th>...</th>\n",
              "      <th>Close(t-1)</th>\n",
              "      <th>Close100(t-1)</th>\n",
              "      <th>marketrisk_avg90(t-1)</th>\n",
              "      <th>marketrisk_avg30(t-1)</th>\n",
              "      <th>Close(t-0)</th>\n",
              "      <th>Close100(t-0)</th>\n",
              "      <th>marketrisk_avg90(t-0)</th>\n",
              "      <th>marketrisk_avg30(t-0)</th>\n",
              "      <th>Change</th>\n",
              "      <th>Signal</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1998-05-27</th>\n",
              "      <td>0.894694</td>\n",
              "      <td>0.916241</td>\n",
              "      <td>0.034995</td>\n",
              "      <td>0.030184</td>\n",
              "      <td>0.893256</td>\n",
              "      <td>0.916045</td>\n",
              "      <td>0.035283</td>\n",
              "      <td>0.030500</td>\n",
              "      <td>0.898069</td>\n",
              "      <td>0.915792</td>\n",
              "      <td>...</td>\n",
              "      <td>0.899442</td>\n",
              "      <td>0.915530</td>\n",
              "      <td>0.036311</td>\n",
              "      <td>0.034618</td>\n",
              "      <td>0.905469</td>\n",
              "      <td>0.915352</td>\n",
              "      <td>0.036533</td>\n",
              "      <td>0.036330</td>\n",
              "      <td>-0.001898</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998-05-28</th>\n",
              "      <td>0.893256</td>\n",
              "      <td>0.916045</td>\n",
              "      <td>0.035283</td>\n",
              "      <td>0.030500</td>\n",
              "      <td>0.898069</td>\n",
              "      <td>0.915792</td>\n",
              "      <td>0.035543</td>\n",
              "      <td>0.031202</td>\n",
              "      <td>0.899442</td>\n",
              "      <td>0.915530</td>\n",
              "      <td>...</td>\n",
              "      <td>0.905469</td>\n",
              "      <td>0.915352</td>\n",
              "      <td>0.036533</td>\n",
              "      <td>0.036330</td>\n",
              "      <td>0.903751</td>\n",
              "      <td>0.915163</td>\n",
              "      <td>0.036895</td>\n",
              "      <td>0.035828</td>\n",
              "      <td>0.003719</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998-05-29</th>\n",
              "      <td>0.898069</td>\n",
              "      <td>0.915792</td>\n",
              "      <td>0.035543</td>\n",
              "      <td>0.031202</td>\n",
              "      <td>0.899442</td>\n",
              "      <td>0.915530</td>\n",
              "      <td>0.036311</td>\n",
              "      <td>0.034618</td>\n",
              "      <td>0.905469</td>\n",
              "      <td>0.915352</td>\n",
              "      <td>...</td>\n",
              "      <td>0.903751</td>\n",
              "      <td>0.915163</td>\n",
              "      <td>0.036895</td>\n",
              "      <td>0.035828</td>\n",
              "      <td>0.907112</td>\n",
              "      <td>0.915000</td>\n",
              "      <td>0.037338</td>\n",
              "      <td>0.036237</td>\n",
              "      <td>-0.004515</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998-06-01</th>\n",
              "      <td>0.899442</td>\n",
              "      <td>0.915530</td>\n",
              "      <td>0.036311</td>\n",
              "      <td>0.034618</td>\n",
              "      <td>0.905469</td>\n",
              "      <td>0.915352</td>\n",
              "      <td>0.036533</td>\n",
              "      <td>0.036330</td>\n",
              "      <td>0.903751</td>\n",
              "      <td>0.915163</td>\n",
              "      <td>...</td>\n",
              "      <td>0.907112</td>\n",
              "      <td>0.915000</td>\n",
              "      <td>0.037338</td>\n",
              "      <td>0.036237</td>\n",
              "      <td>0.903016</td>\n",
              "      <td>0.914801</td>\n",
              "      <td>0.037892</td>\n",
              "      <td>0.038099</td>\n",
              "      <td>0.000813</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998-06-02</th>\n",
              "      <td>0.905469</td>\n",
              "      <td>0.915352</td>\n",
              "      <td>0.036533</td>\n",
              "      <td>0.036330</td>\n",
              "      <td>0.903751</td>\n",
              "      <td>0.915163</td>\n",
              "      <td>0.036895</td>\n",
              "      <td>0.035828</td>\n",
              "      <td>0.907112</td>\n",
              "      <td>0.915000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.903016</td>\n",
              "      <td>0.914801</td>\n",
              "      <td>0.037892</td>\n",
              "      <td>0.038099</td>\n",
              "      <td>0.903751</td>\n",
              "      <td>0.914626</td>\n",
              "      <td>0.038163</td>\n",
              "      <td>0.038561</td>\n",
              "      <td>-0.005840</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 22 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Close(t-4)  Close100(t-4)  marketrisk_avg90(t-4)  \\\n",
              "Date                                                           \n",
              "1998-05-27    0.894694       0.916241               0.034995   \n",
              "1998-05-28    0.893256       0.916045               0.035283   \n",
              "1998-05-29    0.898069       0.915792               0.035543   \n",
              "1998-06-01    0.899442       0.915530               0.036311   \n",
              "1998-06-02    0.905469       0.915352               0.036533   \n",
              "\n",
              "            marketrisk_avg30(t-4)  Close(t-3)  Close100(t-3)  \\\n",
              "Date                                                           \n",
              "1998-05-27               0.030184    0.893256       0.916045   \n",
              "1998-05-28               0.030500    0.898069       0.915792   \n",
              "1998-05-29               0.031202    0.899442       0.915530   \n",
              "1998-06-01               0.034618    0.905469       0.915352   \n",
              "1998-06-02               0.036330    0.903751       0.915163   \n",
              "\n",
              "            marketrisk_avg90(t-3)  marketrisk_avg30(t-3)  Close(t-2)  \\\n",
              "Date                                                                   \n",
              "1998-05-27               0.035283               0.030500    0.898069   \n",
              "1998-05-28               0.035543               0.031202    0.899442   \n",
              "1998-05-29               0.036311               0.034618    0.905469   \n",
              "1998-06-01               0.036533               0.036330    0.903751   \n",
              "1998-06-02               0.036895               0.035828    0.907112   \n",
              "\n",
              "            Close100(t-2)   ...    Close(t-1)  Close100(t-1)  \\\n",
              "Date                        ...                                \n",
              "1998-05-27       0.915792   ...      0.899442       0.915530   \n",
              "1998-05-28       0.915530   ...      0.905469       0.915352   \n",
              "1998-05-29       0.915352   ...      0.903751       0.915163   \n",
              "1998-06-01       0.915163   ...      0.907112       0.915000   \n",
              "1998-06-02       0.915000   ...      0.903016       0.914801   \n",
              "\n",
              "            marketrisk_avg90(t-1)  marketrisk_avg30(t-1)  Close(t-0)  \\\n",
              "Date                                                                   \n",
              "1998-05-27               0.036311               0.034618    0.905469   \n",
              "1998-05-28               0.036533               0.036330    0.903751   \n",
              "1998-05-29               0.036895               0.035828    0.907112   \n",
              "1998-06-01               0.037338               0.036237    0.903016   \n",
              "1998-06-02               0.037892               0.038099    0.903751   \n",
              "\n",
              "            Close100(t-0)  marketrisk_avg90(t-0)  marketrisk_avg30(t-0)  \\\n",
              "Date                                                                      \n",
              "1998-05-27       0.915352               0.036533               0.036330   \n",
              "1998-05-28       0.915163               0.036895               0.035828   \n",
              "1998-05-29       0.915000               0.037338               0.036237   \n",
              "1998-06-01       0.914801               0.037892               0.038099   \n",
              "1998-06-02       0.914626               0.038163               0.038561   \n",
              "\n",
              "              Change  Signal  \n",
              "Date                          \n",
              "1998-05-27 -0.001898   False  \n",
              "1998-05-28  0.003719    True  \n",
              "1998-05-29 -0.004515   False  \n",
              "1998-06-01  0.000813   False  \n",
              "1998-06-02 -0.005840   False  \n",
              "\n",
              "[5 rows x 22 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Ovn_3v_G1Xuq"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.2 Train-Validate-Test Split"
      ]
    },
    {
      "metadata": {
        "id": "-1pSODY0CUY4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fcf9b1be-b520-4eaa-8b70-c81e84c4ee87"
      },
      "cell_type": "code",
      "source": [
        "daterange = pd.date_range(start='1/1/2017', periods=2, freq='3D')\n",
        "print(daterange)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DatetimeIndex(['2017-01-01', '2017-01-04'], dtype='datetime64[ns]', freq='3D')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7cwIwS1Pe1Uq",
        "colab_type": "code",
        "outputId": "7236b42b-21af-404b-fb65-65140df6ce6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "testyhat = []\n",
        "testyhat += [[1,2,3]]\n",
        "testyhat += [[1,2,3]]\n",
        "testyhat\n",
        "np.mean(testyhat, axis =0) > 1"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([False,  True,  True])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "JvOKmcgM1da_",
        "outputId": "b4108a3c-3ae1-4736-a8d0-cfa5b9a582b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241702
        }
      },
      "cell_type": "code",
      "source": [
        "daterange = pd.date_range(start='1/1/2017', periods=8, freq='1W')\n",
        "print(daterange)\n",
        "tradeTable= Ana.loc[\"2017-01-01\":\"2018-01-01\",:].copy()\n",
        "tradeTable[\"predictSignal\"] = 0\n",
        "# tradeTable.head()\n",
        "\n",
        "for single_date in daterange:\n",
        "  testyhat = []\n",
        "  for i in range(0, 4):\n",
        "    TRAIN_START = (single_date - timedelta(days=90)).strftime(\"%Y-%m-%d\")\n",
        "    TRAIN_END = (single_date - timedelta(days=1)).strftime(\"%Y-%m-%d\") \n",
        "    TEST_START = (single_date).strftime(\"%Y-%m-%d\")\n",
        "    TEST_END = (single_date + timedelta(days=6)).strftime(\"%Y-%m-%d\")\n",
        "    print(TEST_START)\n",
        "    print(TEST_END)\n",
        "\n",
        "    #Set x and y variables\n",
        "    FIRSTTRAINVAR = SetAdjusted.columns[0]\n",
        "    print(FIRSTTRAINVAR)\n",
        "    LASTTRAINVARIABLE = SetAdjusted.columns[-3]\n",
        "    print(LASTTRAINVARIABLE)\n",
        "    YVAR= \"Signal\"\n",
        "\n",
        "  #   #Split train and test set at TRAIN_DATE_END and TEST_DATE_START\n",
        "  #   TRAIN_START = \"2017-01-01\"\n",
        "  #   TRAIN_END = \"2017-03-31\"\n",
        "  #   # VALIDATE_START = \"2018-01-01\"\n",
        "  #   # VALIDATE_END = \"2018-05-31\"\n",
        "  #   TEST_START = \"2017-01-01\"\n",
        "  #   TEST_END = \"2017-04-30\"\n",
        "\n",
        "    #Specify LSTM input shape\n",
        "    X_SHAPE1 = 5\n",
        "    X_SHAPE2 = FEATURES_SHAPE\n",
        "\n",
        "    #train_x\n",
        "    train_x = SetAdjusted.loc[TRAIN_START:TRAIN_END, FIRSTTRAINVAR:LASTTRAINVARIABLE]\n",
        "    # Normalised train_x to [0,1]\n",
        "    # print(train_x.head())\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    train_x = scaler.fit_transform(train_x)\n",
        "  #   print(train_x.shape)\n",
        "    #reshape train_x for RNN input\n",
        "    train_x = train_x.reshape(train_x.shape[0], X_SHAPE1, X_SHAPE2)\n",
        "  #   print(train_x.shape)\n",
        "\n",
        "    #train_y\n",
        "    train_y = SetAdjusted.loc[TRAIN_START:TRAIN_END, YVAR]\n",
        "    train_y = train_y.values.reshape((train_y.shape[0], 1))\n",
        "    yscaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    # train_y = yscaler.fit_transform(train_y)\n",
        "  #   print(train_y.shape)\n",
        "\n",
        "    #test_x\n",
        "    test_x = Set.loc[TEST_START:TEST_END, FIRSTTRAINVAR:LASTTRAINVARIABLE]\n",
        "    # Normalised train_x to [0,1]\n",
        "    test_x = scaler.transform(test_x)\n",
        "    #reshape test_x for RNN input\n",
        "    test_x = test_x.reshape(test_x.shape[0], X_SHAPE1, X_SHAPE2)\n",
        "  #   print(test_x.shape)\n",
        "\n",
        "    #test_y\n",
        "    test_y = Set.loc[TEST_START:TEST_END, YVAR]\n",
        "    test_y = test_y.values.reshape((test_y.shape[0], 1))\n",
        "    # test_y = yscaler.transform(test_y)\n",
        "\n",
        "  #   print(test_y.shape)\n",
        "\n",
        "\n",
        "    TESTEND = test_x.shape[0]\n",
        "    # TESTSTART = train_x.shape[0] + validate_x.shape[0]\n",
        "    TESTSTART = train_x.shape[0]\n",
        "    # VALIDATESTART = train_x.shape[0]\n",
        "    print(TESTSTART)\n",
        "\n",
        "    ## Specifying Batch Size\n",
        "    n_batch = 2\n",
        "\n",
        "    ## Only to use when removing model \n",
        "    !rm best.h5\n",
        "\n",
        "    #Check train_y statistic\n",
        "    trainStats = np.unique(train_y, return_counts=True)\n",
        "    print(trainStats)\n",
        "\n",
        "    #Set Classweight\n",
        "    if trainStats[1][0] > trainStats[1][1]: \n",
        "      classweight = {0:1, 1:trainStats[1][0]/trainStats[1][1]}\n",
        "    else:\n",
        "      classweight = {0:trainStats[1][1]/trainStats[1][0], 1:1}\n",
        "\n",
        "    print(classweight)\n",
        "\n",
        "    units = 20\n",
        "    inputs = Input(shape=(train_x.shape[1], train_x.shape[2]))\n",
        "    secondary = LSTM(units, return_sequences=True)(inputs)\n",
        "    activations = LSTM(units, return_sequences=True)(secondary)\n",
        "    # out = Dense(1, activation='sigmoid')(activations)\n",
        "\n",
        "\n",
        "    attention = Dense(1, activation='tanh')(activations)\n",
        "    attention = Flatten()(attention)\n",
        "    attention = Activation('softmax')(attention)\n",
        "    attention = RepeatVector(units)(attention)\n",
        "    attention = Permute([2, 1])(attention)\n",
        "\n",
        "\n",
        "    sent_representation = multiply([activations, attention])\n",
        "    sent_representation = Lambda(lambda xin: K.sum(xin, axis=-2), output_shape=(units,))(sent_representation)\n",
        "\n",
        "    probabilities = Dense(1, activation='sigmoid')(sent_representation)\n",
        "\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=probabilities)\n",
        "    model.compile(optimizer='rmsprop',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=[\"binary_accuracy\"])\n",
        "\n",
        "    # Checkpoint\n",
        "    checkpoint = ModelCheckpoint(\"best.h5\", monitor='binary_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='max')\n",
        "    early = EarlyStopping(monitor='binary_accuracy', min_delta=0, patience=40, verbose=0, mode='auto')\n",
        "    callbacks_list = [checkpoint]\n",
        "\n",
        "    # Fit model\n",
        "    if os.path.isfile(\"best.h5\"):\n",
        "      model.load_weights(\"best.h5\")\n",
        "\n",
        "    history = model.fit(train_x, train_y, epochs=300,  batch_size=n_batch, callbacks=[checkpoint, early],\n",
        "  #                       validation_data=(validate_x, validate_y), \n",
        "                        shuffle=False, verbose =  1,\n",
        "                          class_weight= classweight\n",
        "                          )\n",
        "\n",
        "    # Make predictions, yhat, on test set\n",
        "    if os.path.isfile(\"best.h5\"):\n",
        "      model.load_weights(\"best.h5\")\n",
        "    yhat = model.predict(test_x, batch_size=n_batch)\n",
        "    # yhat = yscaler.inverse_transform(yhat)\n",
        "\n",
        "    print(yhat.shape)\n",
        "\n",
        "    ## Reading yhat as predictSignal\n",
        "    testyhat += [yhat]\n",
        "    tradeTable.loc[TEST_START:TEST_END, \"predictSignal\"] = np.mean(testyhat, axis =0) >0.5\n",
        "    # tradeTable[\"predictPrice\"] = yhat\n",
        "    # tradeTable[\"predictSignal\"]= tradeTable[\"predictPrice\"]>0\n",
        "  #   tradeTable.head()\n",
        "\n",
        "#Changing Signals to Weights\n",
        "tradeTable['predictedweight'] = np.where(tradeTable['predictSignal'] == True, 1, -1)\n",
        "tradeTable['labelledweight'] = np.where(tradeTable['Signal'] == True, 1, -1)\n",
        "tradeTable.head()\n",
        "\n",
        "# calculating r, %tage change of closing price over previous time step\n",
        "tradeTable['r'] = (tradeTable['Close']- tradeTable['Close'].shift(1))/tradeTable['Close'].shift(1)\n",
        "tradeTable.head()\n",
        "\n",
        "# calculating predicted and labelled trade returns over each time step\n",
        "tradeTable['predictedreturns'] = tradeTable['r']*tradeTable['predictedweight'].shift(1)\n",
        "tradeTable['labelledreturns'] = tradeTable['r']*tradeTable['labelledweight'].shift(1)\n",
        "\n",
        "tradeTable\n",
        "\n",
        "# # calculating cumulative predicted and labelled trade returns over each time step\n",
        "# TrainTable =  tradeTable.loc[TRAIN_START:TRAIN_END,:].copy()\n",
        "# TrainTable[\"labelledPNL\"] =  (1+TrainTable['labelledreturns']).cumprod()\n",
        "# TrainTable[\"PNL\"] = (1+TrainTable['predictedreturns']).cumprod()\n",
        "# TrainTable.head()\n",
        "\n",
        "# # # calculating cumulative predicted and labelled trade returns over each time step\n",
        "# # ValTable =  tradeTable.loc[VALIDATE_START:VALIDATE_END,:].copy()\n",
        "# # ValTable[\"labelledPNL\"] =  (1+ValTable['labelledreturns']).cumprod()\n",
        "# # ValTable[\"PNL\"] = (1+ValTable['predictedreturns']).cumprod()\n",
        "# # ValTable.head()\n",
        "\n",
        "# # calculating cumulative predicted and labelled trade returns over each time step\n",
        "# PNLTable =  tradeTable.loc[TEST_START:TEST_END,:].copy()\n",
        "# PNLTable[\"labelledPNL\"] =  (1+PNLTable['labelledreturns']).cumprod()\n",
        "# PNLTable[\"PNL\"] = (1+PNLTable['predictedreturns']).cumprod()\n",
        "# PNLTable.head()\n",
        "\n",
        "# #plotting Confusion Matrix\n",
        "# f = plt.figure(figsize=(15,7))\n",
        "# ax = f.add_subplot(131)\n",
        "# plotConfusion(TRAIN_START, TRAIN_END, \"Train\")\n",
        "\n",
        "# #   plt.subplot(132)\n",
        "# #   plotConfusion(VALIDATE_START, VALIDATE_END, \"Validate\")\n",
        "\n",
        "# plt.subplot(133)\n",
        "# plotConfusion(TEST_START, TEST_END, \"Test\")\n",
        "\n",
        "# plt.show()\n",
        "\n",
        "# f = plt.figure(figsize=(36,6))\n",
        "# ax = f.add_subplot(131)\n",
        "\n",
        "# # PNL Plot for the Test Period\n",
        "# PNLTable[\"PNL\"].plot()\n",
        "\n",
        "# # PNL Plot for the Test Period\n",
        "# TrainTable[\"PNL\"].plot()\n",
        "\n",
        "# plt.show()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DatetimeIndex(['2017-01-01', '2017-01-08', '2017-01-15', '2017-01-22',\n",
            "               '2017-01-29', '2017-02-05', '2017-02-12', '2017-02-19'],\n",
            "              dtype='datetime64[ns]', freq='W-SUN')\n",
            "2017-01-01\n",
            "2017-01-07\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([25, 40]))\n",
            "{0: 1.6, 1: 1}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 28s 429ms/step - loss: 0.8705 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.61538, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8617 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00002: binary_accuracy did not improve from 0.61538\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8574 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00003: binary_accuracy did not improve from 0.61538\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8540 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00004: binary_accuracy did not improve from 0.61538\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8511 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00005: binary_accuracy did not improve from 0.61538\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8484 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00006: binary_accuracy did not improve from 0.61538\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8459 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00007: binary_accuracy improved from 0.61538 to 0.63077, saving model to best.h5\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8433 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00008: binary_accuracy did not improve from 0.63077\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8406 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00009: binary_accuracy did not improve from 0.63077\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8378 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00010: binary_accuracy did not improve from 0.63077\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8349 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.63077\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8318 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00012: binary_accuracy did not improve from 0.63077\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8286 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00013: binary_accuracy did not improve from 0.63077\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8251 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00014: binary_accuracy did not improve from 0.63077\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8215 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.63077\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8176 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.63077\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8136 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00017: binary_accuracy improved from 0.63077 to 0.64615, saving model to best.h5\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8095 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.64615\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8052 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.64615\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8008 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.64615\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7963 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.64615\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7919 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.64615\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7874 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.64615\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7830 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.64615\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7786 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.64615\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7742 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.64615\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7697 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.64615\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7652 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.64615\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7604 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.64615\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7555 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.64615\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7505 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.64615\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7454 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.64615\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7402 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.64615\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7351 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.64615\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7300 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.64615\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7251 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.64615\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7205 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.64615\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7159 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.64615\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7115 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.64615\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7073 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.64615\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7033 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00041: binary_accuracy improved from 0.64615 to 0.66154, saving model to best.h5\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6995 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.66154\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6956 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.66154\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6921 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.66154\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6885 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00045: binary_accuracy improved from 0.66154 to 0.67692, saving model to best.h5\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6850 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.67692\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6816 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.67692\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6780 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.67692\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6743 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.67692\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6707 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.67692\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6668 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00051: binary_accuracy did not improve from 0.67692\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6629 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.67692\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6584 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00053: binary_accuracy did not improve from 0.67692\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6539 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.67692\n",
            "Epoch 55/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6490 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00055: binary_accuracy did not improve from 0.67692\n",
            "Epoch 56/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6438 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00056: binary_accuracy did not improve from 0.67692\n",
            "Epoch 57/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6384 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00057: binary_accuracy did not improve from 0.67692\n",
            "Epoch 58/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6328 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00058: binary_accuracy improved from 0.67692 to 0.69231, saving model to best.h5\n",
            "Epoch 59/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6268 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00059: binary_accuracy did not improve from 0.69231\n",
            "Epoch 60/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6208 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00060: binary_accuracy did not improve from 0.69231\n",
            "Epoch 61/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6151 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00061: binary_accuracy did not improve from 0.69231\n",
            "Epoch 62/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6094 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00062: binary_accuracy did not improve from 0.69231\n",
            "Epoch 63/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6038 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00063: binary_accuracy improved from 0.69231 to 0.70769, saving model to best.h5\n",
            "Epoch 64/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5986 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00064: binary_accuracy did not improve from 0.70769\n",
            "Epoch 65/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5940 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00065: binary_accuracy did not improve from 0.70769\n",
            "Epoch 66/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5904 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00066: binary_accuracy did not improve from 0.70769\n",
            "Epoch 67/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5872 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00067: binary_accuracy did not improve from 0.70769\n",
            "Epoch 68/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5845 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00068: binary_accuracy did not improve from 0.70769\n",
            "Epoch 69/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5821 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00069: binary_accuracy improved from 0.70769 to 0.72308, saving model to best.h5\n",
            "Epoch 70/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5801 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00070: binary_accuracy did not improve from 0.72308\n",
            "Epoch 71/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5783 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00071: binary_accuracy did not improve from 0.72308\n",
            "Epoch 72/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5769 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00072: binary_accuracy did not improve from 0.72308\n",
            "Epoch 73/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5754 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00073: binary_accuracy improved from 0.72308 to 0.73846, saving model to best.h5\n",
            "Epoch 74/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5742 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00074: binary_accuracy did not improve from 0.73846\n",
            "Epoch 75/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5731 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00075: binary_accuracy did not improve from 0.73846\n",
            "Epoch 76/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5721 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00076: binary_accuracy did not improve from 0.73846\n",
            "Epoch 77/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5712 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00077: binary_accuracy improved from 0.73846 to 0.75385, saving model to best.h5\n",
            "Epoch 78/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5705 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00078: binary_accuracy did not improve from 0.75385\n",
            "Epoch 79/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5696 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00079: binary_accuracy did not improve from 0.75385\n",
            "Epoch 80/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5690 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00080: binary_accuracy did not improve from 0.75385\n",
            "Epoch 81/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5682 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00081: binary_accuracy did not improve from 0.75385\n",
            "Epoch 82/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5676 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00082: binary_accuracy did not improve from 0.75385\n",
            "Epoch 83/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5667 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00083: binary_accuracy did not improve from 0.75385\n",
            "Epoch 84/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5659 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00084: binary_accuracy did not improve from 0.75385\n",
            "Epoch 85/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5651 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00085: binary_accuracy did not improve from 0.75385\n",
            "Epoch 86/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5638 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00086: binary_accuracy did not improve from 0.75385\n",
            "Epoch 87/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5627 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00087: binary_accuracy did not improve from 0.75385\n",
            "Epoch 88/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5614 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00088: binary_accuracy did not improve from 0.75385\n",
            "Epoch 89/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5599 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00089: binary_accuracy did not improve from 0.75385\n",
            "Epoch 90/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5583 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00090: binary_accuracy did not improve from 0.75385\n",
            "Epoch 91/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5567 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00091: binary_accuracy did not improve from 0.75385\n",
            "Epoch 92/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5549 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00092: binary_accuracy did not improve from 0.75385\n",
            "Epoch 93/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5531 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00093: binary_accuracy did not improve from 0.75385\n",
            "Epoch 94/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5517 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00094: binary_accuracy did not improve from 0.75385\n",
            "Epoch 95/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5496 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00095: binary_accuracy did not improve from 0.75385\n",
            "Epoch 96/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5473 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00096: binary_accuracy did not improve from 0.75385\n",
            "Epoch 97/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5453 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00097: binary_accuracy did not improve from 0.75385\n",
            "Epoch 98/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5431 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00098: binary_accuracy did not improve from 0.75385\n",
            "Epoch 99/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5410 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00099: binary_accuracy did not improve from 0.75385\n",
            "Epoch 100/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5390 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00100: binary_accuracy did not improve from 0.75385\n",
            "Epoch 101/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5373 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00101: binary_accuracy did not improve from 0.75385\n",
            "Epoch 102/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5356 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00102: binary_accuracy did not improve from 0.75385\n",
            "Epoch 103/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5340 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00103: binary_accuracy did not improve from 0.75385\n",
            "Epoch 104/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5323 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00104: binary_accuracy did not improve from 0.75385\n",
            "Epoch 105/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5308 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00105: binary_accuracy did not improve from 0.75385\n",
            "Epoch 106/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5285 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00106: binary_accuracy did not improve from 0.75385\n",
            "Epoch 107/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5256 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00107: binary_accuracy improved from 0.75385 to 0.76923, saving model to best.h5\n",
            "Epoch 108/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5219 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00108: binary_accuracy did not improve from 0.76923\n",
            "Epoch 109/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5178 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00109: binary_accuracy improved from 0.76923 to 0.78462, saving model to best.h5\n",
            "Epoch 110/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5130 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00110: binary_accuracy did not improve from 0.78462\n",
            "Epoch 111/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5087 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00111: binary_accuracy did not improve from 0.78462\n",
            "Epoch 112/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5052 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00112: binary_accuracy did not improve from 0.78462\n",
            "Epoch 113/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5021 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00113: binary_accuracy did not improve from 0.78462\n",
            "Epoch 114/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.4993 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00114: binary_accuracy did not improve from 0.78462\n",
            "Epoch 115/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4962 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00115: binary_accuracy did not improve from 0.78462\n",
            "Epoch 116/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.4925 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00116: binary_accuracy did not improve from 0.78462\n",
            "Epoch 117/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.4883 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00117: binary_accuracy did not improve from 0.78462\n",
            "Epoch 118/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.4843 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00118: binary_accuracy did not improve from 0.78462\n",
            "Epoch 119/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.4801 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00119: binary_accuracy did not improve from 0.78462\n",
            "Epoch 120/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.4763 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00120: binary_accuracy did not improve from 0.78462\n",
            "Epoch 121/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.4728 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00121: binary_accuracy did not improve from 0.78462\n",
            "Epoch 122/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.4696 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00122: binary_accuracy did not improve from 0.78462\n",
            "Epoch 123/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.4659 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00123: binary_accuracy did not improve from 0.78462\n",
            "Epoch 124/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.4626 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00124: binary_accuracy did not improve from 0.78462\n",
            "Epoch 125/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4594 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00125: binary_accuracy did not improve from 0.78462\n",
            "Epoch 126/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.4562 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00126: binary_accuracy did not improve from 0.78462\n",
            "Epoch 127/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.4547 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00127: binary_accuracy did not improve from 0.78462\n",
            "Epoch 128/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.4512 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00128: binary_accuracy did not improve from 0.78462\n",
            "Epoch 129/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.4481 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00129: binary_accuracy did not improve from 0.78462\n",
            "Epoch 130/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.4456 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00130: binary_accuracy did not improve from 0.78462\n",
            "Epoch 131/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.4427 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00131: binary_accuracy did not improve from 0.78462\n",
            "Epoch 132/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.4399 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00132: binary_accuracy did not improve from 0.78462\n",
            "Epoch 133/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4375 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00133: binary_accuracy did not improve from 0.78462\n",
            "Epoch 134/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.4350 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00134: binary_accuracy did not improve from 0.78462\n",
            "Epoch 135/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.4322 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00135: binary_accuracy did not improve from 0.78462\n",
            "Epoch 136/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.4297 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00136: binary_accuracy did not improve from 0.78462\n",
            "Epoch 137/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.4272 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00137: binary_accuracy did not improve from 0.78462\n",
            "Epoch 138/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.4251 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00138: binary_accuracy did not improve from 0.78462\n",
            "Epoch 139/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4228 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00139: binary_accuracy improved from 0.78462 to 0.80000, saving model to best.h5\n",
            "Epoch 140/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4201 - binary_accuracy: 0.8154\n",
            "\n",
            "Epoch 00140: binary_accuracy improved from 0.80000 to 0.81538, saving model to best.h5\n",
            "Epoch 141/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.4182 - binary_accuracy: 0.8154\n",
            "\n",
            "Epoch 00141: binary_accuracy did not improve from 0.81538\n",
            "Epoch 142/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.4160 - binary_accuracy: 0.8308\n",
            "\n",
            "Epoch 00142: binary_accuracy improved from 0.81538 to 0.83077, saving model to best.h5\n",
            "Epoch 143/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4137 - binary_accuracy: 0.8308\n",
            "\n",
            "Epoch 00143: binary_accuracy did not improve from 0.83077\n",
            "Epoch 144/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.4113 - binary_accuracy: 0.8308\n",
            "\n",
            "Epoch 00144: binary_accuracy did not improve from 0.83077\n",
            "Epoch 145/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.4088 - binary_accuracy: 0.8462\n",
            "\n",
            "Epoch 00145: binary_accuracy improved from 0.83077 to 0.84615, saving model to best.h5\n",
            "Epoch 146/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4053 - binary_accuracy: 0.8462\n",
            "\n",
            "Epoch 00146: binary_accuracy did not improve from 0.84615\n",
            "Epoch 147/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4031 - binary_accuracy: 0.8615\n",
            "\n",
            "Epoch 00147: binary_accuracy improved from 0.84615 to 0.86154, saving model to best.h5\n",
            "Epoch 148/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.4020 - binary_accuracy: 0.8615\n",
            "\n",
            "Epoch 00148: binary_accuracy did not improve from 0.86154\n",
            "Epoch 149/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.3981 - binary_accuracy: 0.8615\n",
            "\n",
            "Epoch 00149: binary_accuracy did not improve from 0.86154\n",
            "Epoch 150/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.4016 - binary_accuracy: 0.8462\n",
            "\n",
            "Epoch 00150: binary_accuracy did not improve from 0.86154\n",
            "Epoch 151/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.3862 - binary_accuracy: 0.8615\n",
            "\n",
            "Epoch 00151: binary_accuracy did not improve from 0.86154\n",
            "Epoch 152/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.3949 - binary_accuracy: 0.8462\n",
            "\n",
            "Epoch 00152: binary_accuracy did not improve from 0.86154\n",
            "Epoch 153/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.3877 - binary_accuracy: 0.8308\n",
            "\n",
            "Epoch 00153: binary_accuracy did not improve from 0.86154\n",
            "Epoch 154/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.3867 - binary_accuracy: 0.8615\n",
            "\n",
            "Epoch 00154: binary_accuracy did not improve from 0.86154\n",
            "Epoch 155/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.3869 - binary_accuracy: 0.8769\n",
            "\n",
            "Epoch 00155: binary_accuracy improved from 0.86154 to 0.87692, saving model to best.h5\n",
            "Epoch 156/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.3909 - binary_accuracy: 0.8615\n",
            "\n",
            "Epoch 00156: binary_accuracy did not improve from 0.87692\n",
            "Epoch 157/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.3849 - binary_accuracy: 0.8769\n",
            "\n",
            "Epoch 00157: binary_accuracy did not improve from 0.87692\n",
            "Epoch 158/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.3843 - binary_accuracy: 0.8615\n",
            "\n",
            "Epoch 00158: binary_accuracy did not improve from 0.87692\n",
            "Epoch 159/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.3828 - binary_accuracy: 0.8615\n",
            "\n",
            "Epoch 00159: binary_accuracy did not improve from 0.87692\n",
            "Epoch 160/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.3818 - binary_accuracy: 0.8615\n",
            "\n",
            "Epoch 00160: binary_accuracy did not improve from 0.87692\n",
            "Epoch 161/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.3809 - binary_accuracy: 0.8615\n",
            "\n",
            "Epoch 00161: binary_accuracy did not improve from 0.87692\n",
            "Epoch 162/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.3800 - binary_accuracy: 0.8615\n",
            "\n",
            "Epoch 00162: binary_accuracy did not improve from 0.87692\n",
            "Epoch 163/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.3791 - binary_accuracy: 0.8615\n",
            "\n",
            "Epoch 00163: binary_accuracy did not improve from 0.87692\n",
            "Epoch 164/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.3779 - binary_accuracy: 0.8615\n",
            "\n",
            "Epoch 00164: binary_accuracy did not improve from 0.87692\n",
            "Epoch 165/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.3769 - binary_accuracy: 0.8615\n",
            "\n",
            "Epoch 00165: binary_accuracy did not improve from 0.87692\n",
            "Epoch 166/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.3760 - binary_accuracy: 0.8615\n",
            "\n",
            "Epoch 00166: binary_accuracy did not improve from 0.87692\n",
            "Epoch 167/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.3751 - binary_accuracy: 0.8615\n",
            "\n",
            "Epoch 00167: binary_accuracy did not improve from 0.87692\n",
            "Epoch 168/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.3743 - binary_accuracy: 0.8615\n",
            "\n",
            "Epoch 00168: binary_accuracy did not improve from 0.87692\n",
            "Epoch 169/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.3738 - binary_accuracy: 0.8615\n",
            "\n",
            "Epoch 00169: binary_accuracy did not improve from 0.87692\n",
            "Epoch 170/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.3730 - binary_accuracy: 0.8615\n",
            "\n",
            "Epoch 00170: binary_accuracy did not improve from 0.87692\n",
            "Epoch 171/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.3726 - binary_accuracy: 0.8615\n",
            "\n",
            "Epoch 00171: binary_accuracy did not improve from 0.87692\n",
            "Epoch 172/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.3721 - binary_accuracy: 0.8615\n",
            "\n",
            "Epoch 00172: binary_accuracy did not improve from 0.87692\n",
            "Epoch 173/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.3710 - binary_accuracy: 0.8615\n",
            "\n",
            "Epoch 00173: binary_accuracy did not improve from 0.87692\n",
            "Epoch 174/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.3709 - binary_accuracy: 0.8615\n",
            "\n",
            "Epoch 00174: binary_accuracy did not improve from 0.87692\n",
            "Epoch 175/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.3699 - binary_accuracy: 0.8462\n",
            "\n",
            "Epoch 00175: binary_accuracy did not improve from 0.87692\n",
            "Epoch 176/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.3697 - binary_accuracy: 0.8462\n",
            "\n",
            "Epoch 00176: binary_accuracy did not improve from 0.87692\n",
            "Epoch 177/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.3690 - binary_accuracy: 0.8462\n",
            "\n",
            "Epoch 00177: binary_accuracy did not improve from 0.87692\n",
            "Epoch 178/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.3681 - binary_accuracy: 0.8462\n",
            "\n",
            "Epoch 00178: binary_accuracy did not improve from 0.87692\n",
            "Epoch 179/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.3671 - binary_accuracy: 0.8462\n",
            "\n",
            "Epoch 00179: binary_accuracy did not improve from 0.87692\n",
            "Epoch 180/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.3668 - binary_accuracy: 0.8462\n",
            "\n",
            "Epoch 00180: binary_accuracy did not improve from 0.87692\n",
            "Epoch 181/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.3661 - binary_accuracy: 0.8462\n",
            "\n",
            "Epoch 00181: binary_accuracy did not improve from 0.87692\n",
            "Epoch 182/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.3655 - binary_accuracy: 0.8462\n",
            "\n",
            "Epoch 00182: binary_accuracy did not improve from 0.87692\n",
            "Epoch 183/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.3649 - binary_accuracy: 0.8462\n",
            "\n",
            "Epoch 00183: binary_accuracy did not improve from 0.87692\n",
            "Epoch 184/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.3644 - binary_accuracy: 0.8462\n",
            "\n",
            "Epoch 00184: binary_accuracy did not improve from 0.87692\n",
            "Epoch 185/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.3630 - binary_accuracy: 0.8462\n",
            "\n",
            "Epoch 00185: binary_accuracy did not improve from 0.87692\n",
            "Epoch 186/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.3636 - binary_accuracy: 0.8462\n",
            "\n",
            "Epoch 00186: binary_accuracy did not improve from 0.87692\n",
            "Epoch 187/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.3629 - binary_accuracy: 0.8462\n",
            "\n",
            "Epoch 00187: binary_accuracy did not improve from 0.87692\n",
            "Epoch 188/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.3622 - binary_accuracy: 0.8462\n",
            "\n",
            "Epoch 00188: binary_accuracy did not improve from 0.87692\n",
            "Epoch 189/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.3619 - binary_accuracy: 0.8462\n",
            "\n",
            "Epoch 00189: binary_accuracy did not improve from 0.87692\n",
            "Epoch 190/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.3613 - binary_accuracy: 0.8462\n",
            "\n",
            "Epoch 00190: binary_accuracy did not improve from 0.87692\n",
            "Epoch 191/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.3616 - binary_accuracy: 0.8462\n",
            "\n",
            "Epoch 00191: binary_accuracy did not improve from 0.87692\n",
            "Epoch 192/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.3598 - binary_accuracy: 0.8615\n",
            "\n",
            "Epoch 00192: binary_accuracy did not improve from 0.87692\n",
            "Epoch 193/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.3593 - binary_accuracy: 0.8615\n",
            "\n",
            "Epoch 00193: binary_accuracy did not improve from 0.87692\n",
            "Epoch 194/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.3595 - binary_accuracy: 0.8615\n",
            "\n",
            "Epoch 00194: binary_accuracy did not improve from 0.87692\n",
            "Epoch 195/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.3591 - binary_accuracy: 0.8769\n",
            "\n",
            "Epoch 00195: binary_accuracy did not improve from 0.87692\n",
            "(5, 1)\n",
            "2017-01-01\n",
            "2017-01-07\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([25, 40]))\n",
            "{0: 1.6, 1: 1}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 26s 396ms/step - loss: 0.8662 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.61538, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8593 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00002: binary_accuracy did not improve from 0.61538\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8561 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00003: binary_accuracy did not improve from 0.61538\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.8534 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00004: binary_accuracy improved from 0.61538 to 0.63077, saving model to best.h5\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.8512 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00005: binary_accuracy did not improve from 0.63077\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8491 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00006: binary_accuracy did not improve from 0.63077\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8470 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00007: binary_accuracy did not improve from 0.63077\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8450 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00008: binary_accuracy improved from 0.63077 to 0.66154, saving model to best.h5\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8429 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00009: binary_accuracy did not improve from 0.66154\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8406 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00010: binary_accuracy did not improve from 0.66154\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8382 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.66154\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8355 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00012: binary_accuracy did not improve from 0.66154\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8326 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00013: binary_accuracy did not improve from 0.66154\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8294 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00014: binary_accuracy did not improve from 0.66154\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8259 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.66154\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8221 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.66154\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8179 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.66154\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8134 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.66154\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8086 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.66154\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8037 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.66154\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7986 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.66154\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7935 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.66154\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7886 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.66154\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7838 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.66154\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7793 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.66154\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7750 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.66154\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7708 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.66154\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7668 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.66154\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7627 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.66154\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7586 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.66154\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7543 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.66154\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7499 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.66154\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7451 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.66154\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7400 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.66154\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7347 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.66154\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7289 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.66154\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7228 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.66154\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7163 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.66154\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7097 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.66154\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7026 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.66154\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6953 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.66154\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6879 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.66154\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6804 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00043: binary_accuracy improved from 0.66154 to 0.67692, saving model to best.h5\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6730 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00044: binary_accuracy improved from 0.67692 to 0.70769, saving model to best.h5\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6661 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00045: binary_accuracy improved from 0.70769 to 0.72308, saving model to best.h5\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6595 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.72308\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6534 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.72308\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6479 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.72308\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6430 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00049: binary_accuracy improved from 0.72308 to 0.73846, saving model to best.h5\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6387 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00050: binary_accuracy improved from 0.73846 to 0.75385, saving model to best.h5\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6352 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00051: binary_accuracy improved from 0.75385 to 0.76923, saving model to best.h5\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6338 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.76923\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6327 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00053: binary_accuracy did not improve from 0.76923\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6326 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.76923\n",
            "Epoch 55/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6319 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00055: binary_accuracy improved from 0.76923 to 0.78462, saving model to best.h5\n",
            "Epoch 56/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6328 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00056: binary_accuracy did not improve from 0.78462\n",
            "Epoch 57/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6324 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00057: binary_accuracy improved from 0.78462 to 0.80000, saving model to best.h5\n",
            "Epoch 58/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.6325 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00058: binary_accuracy did not improve from 0.80000\n",
            "Epoch 59/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6325 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00059: binary_accuracy did not improve from 0.80000\n",
            "Epoch 60/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6325 - binary_accuracy: 0.8154\n",
            "\n",
            "Epoch 00060: binary_accuracy improved from 0.80000 to 0.81538, saving model to best.h5\n",
            "Epoch 61/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6326 - binary_accuracy: 0.8154\n",
            "\n",
            "Epoch 00061: binary_accuracy did not improve from 0.81538\n",
            "Epoch 62/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6332 - binary_accuracy: 0.8154\n",
            "\n",
            "Epoch 00062: binary_accuracy did not improve from 0.81538\n",
            "Epoch 63/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6330 - binary_accuracy: 0.8154\n",
            "\n",
            "Epoch 00063: binary_accuracy did not improve from 0.81538\n",
            "Epoch 64/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6328 - binary_accuracy: 0.8154\n",
            "\n",
            "Epoch 00064: binary_accuracy did not improve from 0.81538\n",
            "Epoch 65/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6329 - binary_accuracy: 0.8154\n",
            "\n",
            "Epoch 00065: binary_accuracy did not improve from 0.81538\n",
            "Epoch 66/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6327 - binary_accuracy: 0.8154\n",
            "\n",
            "Epoch 00066: binary_accuracy did not improve from 0.81538\n",
            "Epoch 67/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6324 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00067: binary_accuracy did not improve from 0.81538\n",
            "Epoch 68/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6320 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00068: binary_accuracy did not improve from 0.81538\n",
            "Epoch 69/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6319 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00069: binary_accuracy did not improve from 0.81538\n",
            "Epoch 70/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6315 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00070: binary_accuracy did not improve from 0.81538\n",
            "Epoch 71/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6311 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00071: binary_accuracy did not improve from 0.81538\n",
            "Epoch 72/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6306 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00072: binary_accuracy did not improve from 0.81538\n",
            "Epoch 73/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6300 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00073: binary_accuracy did not improve from 0.81538\n",
            "Epoch 74/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6293 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00074: binary_accuracy did not improve from 0.81538\n",
            "Epoch 75/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6285 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00075: binary_accuracy did not improve from 0.81538\n",
            "Epoch 76/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6279 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00076: binary_accuracy did not improve from 0.81538\n",
            "Epoch 77/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6271 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00077: binary_accuracy did not improve from 0.81538\n",
            "Epoch 78/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6262 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00078: binary_accuracy did not improve from 0.81538\n",
            "Epoch 79/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6256 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00079: binary_accuracy did not improve from 0.81538\n",
            "Epoch 80/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6249 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00080: binary_accuracy did not improve from 0.81538\n",
            "Epoch 81/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6241 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00081: binary_accuracy did not improve from 0.81538\n",
            "Epoch 82/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6234 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00082: binary_accuracy did not improve from 0.81538\n",
            "Epoch 83/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6228 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00083: binary_accuracy did not improve from 0.81538\n",
            "Epoch 84/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6221 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00084: binary_accuracy did not improve from 0.81538\n",
            "Epoch 85/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6216 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00085: binary_accuracy did not improve from 0.81538\n",
            "Epoch 86/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6210 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00086: binary_accuracy did not improve from 0.81538\n",
            "Epoch 87/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6204 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00087: binary_accuracy did not improve from 0.81538\n",
            "Epoch 88/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6198 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00088: binary_accuracy did not improve from 0.81538\n",
            "Epoch 89/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6193 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00089: binary_accuracy did not improve from 0.81538\n",
            "Epoch 90/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6190 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00090: binary_accuracy did not improve from 0.81538\n",
            "Epoch 91/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6184 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00091: binary_accuracy did not improve from 0.81538\n",
            "Epoch 92/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6180 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00092: binary_accuracy did not improve from 0.81538\n",
            "Epoch 93/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6176 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00093: binary_accuracy did not improve from 0.81538\n",
            "Epoch 94/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6172 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00094: binary_accuracy did not improve from 0.81538\n",
            "Epoch 95/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6169 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00095: binary_accuracy did not improve from 0.81538\n",
            "Epoch 96/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6165 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00096: binary_accuracy did not improve from 0.81538\n",
            "Epoch 97/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6158 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00097: binary_accuracy did not improve from 0.81538\n",
            "Epoch 98/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6154 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00098: binary_accuracy did not improve from 0.81538\n",
            "Epoch 99/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6151 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00099: binary_accuracy did not improve from 0.81538\n",
            "Epoch 100/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6149 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00100: binary_accuracy did not improve from 0.81538\n",
            "(5, 1)\n",
            "2017-01-01\n",
            "2017-01-07\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([25, 40]))\n",
            "{0: 1.6, 1: 1}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 26s 396ms/step - loss: 0.8639 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.61538, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8581 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00002: binary_accuracy did not improve from 0.61538\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8551 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00003: binary_accuracy did not improve from 0.61538\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8526 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00004: binary_accuracy did not improve from 0.61538\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8502 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00005: binary_accuracy did not improve from 0.61538\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8479 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00006: binary_accuracy did not improve from 0.61538\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8453 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00007: binary_accuracy did not improve from 0.61538\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8425 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00008: binary_accuracy did not improve from 0.61538\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8393 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00009: binary_accuracy did not improve from 0.61538\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8358 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00010: binary_accuracy did not improve from 0.61538\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8319 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.61538\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8277 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00012: binary_accuracy did not improve from 0.61538\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8232 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00013: binary_accuracy did not improve from 0.61538\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8184 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00014: binary_accuracy did not improve from 0.61538\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8132 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.61538\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8077 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00016: binary_accuracy improved from 0.61538 to 0.63077, saving model to best.h5\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.8020 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.63077\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7961 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.63077\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7899 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.63077\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7836 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.63077\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7772 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.63077\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7707 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.63077\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7642 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.63077\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7577 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.63077\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7513 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00025: binary_accuracy improved from 0.63077 to 0.66154, saving model to best.h5\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7450 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.66154\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7391 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00027: binary_accuracy improved from 0.66154 to 0.67692, saving model to best.h5\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7335 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.67692\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7283 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00029: binary_accuracy improved from 0.67692 to 0.69231, saving model to best.h5\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7237 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.69231\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7195 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.69231\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7158 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.69231\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7124 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.69231\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7094 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.69231\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7064 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.69231\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7034 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.69231\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7002 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.69231\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6967 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00038: binary_accuracy improved from 0.69231 to 0.70769, saving model to best.h5\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6928 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00039: binary_accuracy improved from 0.70769 to 0.72308, saving model to best.h5\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6887 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.72308\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6842 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.72308\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6796 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.72308\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6748 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.72308\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6700 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00044: binary_accuracy improved from 0.72308 to 0.73846, saving model to best.h5\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6653 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.73846\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6607 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.73846\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6563 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.73846\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6522 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.73846\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6482 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.73846\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6446 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00050: binary_accuracy improved from 0.73846 to 0.75385, saving model to best.h5\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6413 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00051: binary_accuracy improved from 0.75385 to 0.78462, saving model to best.h5\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6385 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.78462\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6358 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00053: binary_accuracy did not improve from 0.78462\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6335 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.78462\n",
            "Epoch 55/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6323 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00055: binary_accuracy did not improve from 0.78462\n",
            "Epoch 56/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6304 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00056: binary_accuracy did not improve from 0.78462\n",
            "Epoch 57/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6288 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00057: binary_accuracy did not improve from 0.78462\n",
            "Epoch 58/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6273 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00058: binary_accuracy did not improve from 0.78462\n",
            "Epoch 59/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6261 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00059: binary_accuracy did not improve from 0.78462\n",
            "Epoch 60/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6251 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00060: binary_accuracy did not improve from 0.78462\n",
            "Epoch 61/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6241 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00061: binary_accuracy did not improve from 0.78462\n",
            "Epoch 62/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6235 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00062: binary_accuracy did not improve from 0.78462\n",
            "Epoch 63/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6227 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00063: binary_accuracy did not improve from 0.78462\n",
            "Epoch 64/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6220 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00064: binary_accuracy did not improve from 0.78462\n",
            "Epoch 65/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6216 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00065: binary_accuracy did not improve from 0.78462\n",
            "Epoch 66/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6209 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00066: binary_accuracy did not improve from 0.78462\n",
            "Epoch 67/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6203 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00067: binary_accuracy did not improve from 0.78462\n",
            "Epoch 68/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6196 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00068: binary_accuracy did not improve from 0.78462\n",
            "Epoch 69/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6189 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00069: binary_accuracy did not improve from 0.78462\n",
            "Epoch 70/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6183 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00070: binary_accuracy did not improve from 0.78462\n",
            "Epoch 71/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6176 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00071: binary_accuracy did not improve from 0.78462\n",
            "Epoch 72/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6169 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00072: binary_accuracy did not improve from 0.78462\n",
            "Epoch 73/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6162 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00073: binary_accuracy did not improve from 0.78462\n",
            "Epoch 74/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6154 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00074: binary_accuracy did not improve from 0.78462\n",
            "Epoch 75/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6147 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00075: binary_accuracy did not improve from 0.78462\n",
            "Epoch 76/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6140 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00076: binary_accuracy did not improve from 0.78462\n",
            "Epoch 77/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6133 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00077: binary_accuracy did not improve from 0.78462\n",
            "Epoch 78/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6125 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00078: binary_accuracy did not improve from 0.78462\n",
            "Epoch 79/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6118 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00079: binary_accuracy did not improve from 0.78462\n",
            "Epoch 80/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6111 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00080: binary_accuracy did not improve from 0.78462\n",
            "Epoch 81/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6105 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00081: binary_accuracy did not improve from 0.78462\n",
            "Epoch 82/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6097 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00082: binary_accuracy did not improve from 0.78462\n",
            "Epoch 83/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6090 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00083: binary_accuracy did not improve from 0.78462\n",
            "Epoch 84/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6083 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00084: binary_accuracy did not improve from 0.78462\n",
            "Epoch 85/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6075 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00085: binary_accuracy did not improve from 0.78462\n",
            "Epoch 86/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6068 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00086: binary_accuracy did not improve from 0.78462\n",
            "Epoch 87/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6060 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00087: binary_accuracy did not improve from 0.78462\n",
            "Epoch 88/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6053 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00088: binary_accuracy did not improve from 0.78462\n",
            "Epoch 89/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6046 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00089: binary_accuracy did not improve from 0.78462\n",
            "Epoch 90/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6039 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00090: binary_accuracy did not improve from 0.78462\n",
            "Epoch 91/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6033 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00091: binary_accuracy did not improve from 0.78462\n",
            "(5, 1)\n",
            "2017-01-01\n",
            "2017-01-07\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([25, 40]))\n",
            "{0: 1.6, 1: 1}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 26s 405ms/step - loss: 0.8571 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.56923, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8511 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00002: binary_accuracy did not improve from 0.56923\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8480 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00003: binary_accuracy improved from 0.56923 to 0.60000, saving model to best.h5\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8450 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00004: binary_accuracy improved from 0.60000 to 0.63077, saving model to best.h5\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8421 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00005: binary_accuracy improved from 0.63077 to 0.64615, saving model to best.h5\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8392 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00006: binary_accuracy improved from 0.64615 to 0.67692, saving model to best.h5\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8360 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00007: binary_accuracy did not improve from 0.67692\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8327 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00008: binary_accuracy did not improve from 0.67692\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8290 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00009: binary_accuracy did not improve from 0.67692\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8250 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00010: binary_accuracy did not improve from 0.67692\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8207 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.67692\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8159 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00012: binary_accuracy did not improve from 0.67692\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8107 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00013: binary_accuracy did not improve from 0.67692\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.8052 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00014: binary_accuracy did not improve from 0.67692\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7994 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.67692\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7934 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.67692\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7874 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.67692\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7815 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.67692\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7758 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.67692\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7706 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.67692\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7658 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.67692\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7612 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.67692\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7567 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.67692\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7522 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.67692\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7476 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.67692\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7428 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.67692\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7379 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.67692\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7329 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.67692\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7277 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.67692\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7223 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.67692\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7167 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.67692\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7108 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.67692\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7046 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.67692\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6979 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.67692\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6910 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.67692\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6837 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00036: binary_accuracy improved from 0.67692 to 0.69231, saving model to best.h5\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6762 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.69231\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6688 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.69231\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6616 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00039: binary_accuracy improved from 0.69231 to 0.70769, saving model to best.h5\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6549 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00040: binary_accuracy improved from 0.70769 to 0.72308, saving model to best.h5\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6491 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.72308\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6442 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.72308\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6404 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00043: binary_accuracy improved from 0.72308 to 0.75385, saving model to best.h5\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6373 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.75385\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6349 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.75385\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6329 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.75385\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6313 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.75385\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6299 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.75385\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6288 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.75385\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6276 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.75385\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6265 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00051: binary_accuracy did not improve from 0.75385\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6256 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.75385\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6247 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00053: binary_accuracy did not improve from 0.75385\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6240 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.75385\n",
            "Epoch 55/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6232 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00055: binary_accuracy did not improve from 0.75385\n",
            "Epoch 56/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6224 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00056: binary_accuracy improved from 0.75385 to 0.76923, saving model to best.h5\n",
            "Epoch 57/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6215 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00057: binary_accuracy did not improve from 0.76923\n",
            "Epoch 58/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6206 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00058: binary_accuracy did not improve from 0.76923\n",
            "Epoch 59/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6198 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00059: binary_accuracy did not improve from 0.76923\n",
            "Epoch 60/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6187 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00060: binary_accuracy did not improve from 0.76923\n",
            "Epoch 61/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6179 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00061: binary_accuracy improved from 0.76923 to 0.78462, saving model to best.h5\n",
            "Epoch 62/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6171 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00062: binary_accuracy did not improve from 0.78462\n",
            "Epoch 63/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6163 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00063: binary_accuracy did not improve from 0.78462\n",
            "Epoch 64/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6154 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00064: binary_accuracy did not improve from 0.78462\n",
            "Epoch 65/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6146 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00065: binary_accuracy did not improve from 0.78462\n",
            "Epoch 66/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6137 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00066: binary_accuracy did not improve from 0.78462\n",
            "Epoch 67/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6129 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00067: binary_accuracy did not improve from 0.78462\n",
            "Epoch 68/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6121 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00068: binary_accuracy did not improve from 0.78462\n",
            "Epoch 69/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6113 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00069: binary_accuracy did not improve from 0.78462\n",
            "Epoch 70/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6107 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00070: binary_accuracy did not improve from 0.78462\n",
            "Epoch 71/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6099 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00071: binary_accuracy did not improve from 0.78462\n",
            "Epoch 72/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6092 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00072: binary_accuracy did not improve from 0.78462\n",
            "Epoch 73/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6084 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00073: binary_accuracy did not improve from 0.78462\n",
            "Epoch 74/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6077 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00074: binary_accuracy did not improve from 0.78462\n",
            "Epoch 75/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6070 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00075: binary_accuracy did not improve from 0.78462\n",
            "Epoch 76/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6063 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00076: binary_accuracy did not improve from 0.78462\n",
            "Epoch 77/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6056 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00077: binary_accuracy did not improve from 0.78462\n",
            "Epoch 78/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6049 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00078: binary_accuracy did not improve from 0.78462\n",
            "Epoch 79/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6042 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00079: binary_accuracy did not improve from 0.78462\n",
            "Epoch 80/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6035 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00080: binary_accuracy did not improve from 0.78462\n",
            "Epoch 81/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6029 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00081: binary_accuracy did not improve from 0.78462\n",
            "Epoch 82/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6022 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00082: binary_accuracy did not improve from 0.78462\n",
            "Epoch 83/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6016 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00083: binary_accuracy did not improve from 0.78462\n",
            "Epoch 84/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6009 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00084: binary_accuracy did not improve from 0.78462\n",
            "Epoch 85/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6003 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00085: binary_accuracy did not improve from 0.78462\n",
            "Epoch 86/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5996 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00086: binary_accuracy did not improve from 0.78462\n",
            "Epoch 87/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5990 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00087: binary_accuracy did not improve from 0.78462\n",
            "Epoch 88/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5984 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00088: binary_accuracy did not improve from 0.78462\n",
            "Epoch 89/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5978 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00089: binary_accuracy did not improve from 0.78462\n",
            "Epoch 90/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5972 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00090: binary_accuracy did not improve from 0.78462\n",
            "Epoch 91/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5967 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00091: binary_accuracy did not improve from 0.78462\n",
            "Epoch 92/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5961 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00092: binary_accuracy did not improve from 0.78462\n",
            "Epoch 93/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5955 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00093: binary_accuracy did not improve from 0.78462\n",
            "Epoch 94/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5949 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00094: binary_accuracy did not improve from 0.78462\n",
            "Epoch 95/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5944 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00095: binary_accuracy did not improve from 0.78462\n",
            "Epoch 96/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5939 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00096: binary_accuracy did not improve from 0.78462\n",
            "Epoch 97/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5933 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00097: binary_accuracy did not improve from 0.78462\n",
            "Epoch 98/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5928 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00098: binary_accuracy did not improve from 0.78462\n",
            "Epoch 99/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5923 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00099: binary_accuracy did not improve from 0.78462\n",
            "Epoch 100/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5916 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00100: binary_accuracy did not improve from 0.78462\n",
            "Epoch 101/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5912 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00101: binary_accuracy did not improve from 0.78462\n",
            "(5, 1)\n",
            "2017-01-08\n",
            "2017-01-14\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([29, 36]))\n",
            "{0: 1.2413793103448276, 1: 1}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 26s 406ms/step - loss: 0.7730 - binary_accuracy: 0.4769\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.47692, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7696 - binary_accuracy: 0.4769\n",
            "\n",
            "Epoch 00002: binary_accuracy did not improve from 0.47692\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7682 - binary_accuracy: 0.4769\n",
            "\n",
            "Epoch 00003: binary_accuracy did not improve from 0.47692\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7671 - binary_accuracy: 0.5077\n",
            "\n",
            "Epoch 00004: binary_accuracy improved from 0.47692 to 0.50769, saving model to best.h5\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7659 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00005: binary_accuracy improved from 0.50769 to 0.52308, saving model to best.h5\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7647 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00006: binary_accuracy improved from 0.52308 to 0.53846, saving model to best.h5\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7634 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00007: binary_accuracy did not improve from 0.53846\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7620 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00008: binary_accuracy did not improve from 0.53846\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7604 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00009: binary_accuracy did not improve from 0.53846\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7586 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00010: binary_accuracy did not improve from 0.53846\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7566 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.53846\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7543 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00012: binary_accuracy did not improve from 0.53846\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7518 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00013: binary_accuracy improved from 0.53846 to 0.58462, saving model to best.h5\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7490 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00014: binary_accuracy improved from 0.58462 to 0.60000, saving model to best.h5\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7459 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00015: binary_accuracy improved from 0.60000 to 0.64615, saving model to best.h5\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7426 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.64615\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7390 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.64615\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7353 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.64615\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7314 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.64615\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7276 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.64615\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7237 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.64615\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7200 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.64615\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7164 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.64615\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7130 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.64615\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7098 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.64615\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7068 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.64615\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7041 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.64615\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7016 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.64615\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6994 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.64615\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6973 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.64615\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6953 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.64615\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6934 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.64615\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6915 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.64615\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6895 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.64615\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6876 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.64615\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6856 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.64615\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6836 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.64615\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6816 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00038: binary_accuracy improved from 0.64615 to 0.67692, saving model to best.h5\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6795 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.67692\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6774 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.67692\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6753 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.67692\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6732 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.67692\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6711 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.67692\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6689 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.67692\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6668 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.67692\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6648 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.67692\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6627 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.67692\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6607 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.67692\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6588 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.67692\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6570 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.67692\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6553 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00051: binary_accuracy did not improve from 0.67692\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6536 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.67692\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6519 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00053: binary_accuracy did not improve from 0.67692\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6504 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.67692\n",
            "Epoch 55/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6489 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00055: binary_accuracy did not improve from 0.67692\n",
            "Epoch 56/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6474 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00056: binary_accuracy did not improve from 0.67692\n",
            "Epoch 57/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6460 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00057: binary_accuracy did not improve from 0.67692\n",
            "Epoch 58/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6446 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00058: binary_accuracy did not improve from 0.67692\n",
            "Epoch 59/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6433 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00059: binary_accuracy did not improve from 0.67692\n",
            "Epoch 60/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6419 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00060: binary_accuracy did not improve from 0.67692\n",
            "Epoch 61/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6407 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00061: binary_accuracy did not improve from 0.67692\n",
            "Epoch 62/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6395 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00062: binary_accuracy did not improve from 0.67692\n",
            "Epoch 63/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6382 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00063: binary_accuracy did not improve from 0.67692\n",
            "Epoch 64/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6370 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00064: binary_accuracy did not improve from 0.67692\n",
            "Epoch 65/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6357 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00065: binary_accuracy did not improve from 0.67692\n",
            "Epoch 66/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6346 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00066: binary_accuracy improved from 0.67692 to 0.69231, saving model to best.h5\n",
            "Epoch 67/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6334 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00067: binary_accuracy did not improve from 0.69231\n",
            "Epoch 68/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6322 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00068: binary_accuracy did not improve from 0.69231\n",
            "Epoch 69/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6310 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00069: binary_accuracy did not improve from 0.69231\n",
            "Epoch 70/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6298 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00070: binary_accuracy did not improve from 0.69231\n",
            "Epoch 71/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6286 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00071: binary_accuracy did not improve from 0.69231\n",
            "Epoch 72/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6274 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00072: binary_accuracy did not improve from 0.69231\n",
            "Epoch 73/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6261 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00073: binary_accuracy did not improve from 0.69231\n",
            "Epoch 74/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6247 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00074: binary_accuracy did not improve from 0.69231\n",
            "Epoch 75/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6234 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00075: binary_accuracy did not improve from 0.69231\n",
            "Epoch 76/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6221 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00076: binary_accuracy did not improve from 0.69231\n",
            "Epoch 77/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6206 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00077: binary_accuracy did not improve from 0.69231\n",
            "Epoch 78/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6191 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00078: binary_accuracy did not improve from 0.69231\n",
            "Epoch 79/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6177 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00079: binary_accuracy did not improve from 0.69231\n",
            "Epoch 80/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6164 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00080: binary_accuracy did not improve from 0.69231\n",
            "Epoch 81/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6149 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00081: binary_accuracy did not improve from 0.69231\n",
            "Epoch 82/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6135 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00082: binary_accuracy did not improve from 0.69231\n",
            "Epoch 83/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6120 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00083: binary_accuracy did not improve from 0.69231\n",
            "Epoch 84/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6106 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00084: binary_accuracy did not improve from 0.69231\n",
            "Epoch 85/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6094 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00085: binary_accuracy did not improve from 0.69231\n",
            "Epoch 86/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6082 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00086: binary_accuracy did not improve from 0.69231\n",
            "Epoch 87/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6070 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00087: binary_accuracy did not improve from 0.69231\n",
            "Epoch 88/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6060 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00088: binary_accuracy improved from 0.69231 to 0.70769, saving model to best.h5\n",
            "Epoch 89/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6050 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00089: binary_accuracy did not improve from 0.70769\n",
            "Epoch 90/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6040 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00090: binary_accuracy did not improve from 0.70769\n",
            "Epoch 91/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6031 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00091: binary_accuracy did not improve from 0.70769\n",
            "Epoch 92/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6023 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00092: binary_accuracy did not improve from 0.70769\n",
            "Epoch 93/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6015 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00093: binary_accuracy did not improve from 0.70769\n",
            "Epoch 94/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6008 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00094: binary_accuracy did not improve from 0.70769\n",
            "Epoch 95/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6000 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00095: binary_accuracy did not improve from 0.70769\n",
            "Epoch 96/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5993 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00096: binary_accuracy did not improve from 0.70769\n",
            "Epoch 97/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5987 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00097: binary_accuracy improved from 0.70769 to 0.72308, saving model to best.h5\n",
            "Epoch 98/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5980 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00098: binary_accuracy did not improve from 0.72308\n",
            "Epoch 99/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5974 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00099: binary_accuracy did not improve from 0.72308\n",
            "Epoch 100/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5968 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00100: binary_accuracy did not improve from 0.72308\n",
            "Epoch 101/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5963 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00101: binary_accuracy did not improve from 0.72308\n",
            "Epoch 102/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5957 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00102: binary_accuracy did not improve from 0.72308\n",
            "Epoch 103/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5951 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00103: binary_accuracy did not improve from 0.72308\n",
            "Epoch 104/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5945 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00104: binary_accuracy did not improve from 0.72308\n",
            "Epoch 105/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5939 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00105: binary_accuracy did not improve from 0.72308\n",
            "Epoch 106/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5933 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00106: binary_accuracy did not improve from 0.72308\n",
            "Epoch 107/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5928 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00107: binary_accuracy did not improve from 0.72308\n",
            "Epoch 108/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5922 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00108: binary_accuracy did not improve from 0.72308\n",
            "Epoch 109/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5917 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00109: binary_accuracy did not improve from 0.72308\n",
            "Epoch 110/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5911 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00110: binary_accuracy did not improve from 0.72308\n",
            "Epoch 111/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5906 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00111: binary_accuracy did not improve from 0.72308\n",
            "Epoch 112/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5901 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00112: binary_accuracy did not improve from 0.72308\n",
            "Epoch 113/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5895 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00113: binary_accuracy did not improve from 0.72308\n",
            "Epoch 114/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5890 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00114: binary_accuracy did not improve from 0.72308\n",
            "Epoch 115/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5885 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00115: binary_accuracy did not improve from 0.72308\n",
            "Epoch 116/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5881 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00116: binary_accuracy did not improve from 0.72308\n",
            "Epoch 117/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5876 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00117: binary_accuracy did not improve from 0.72308\n",
            "Epoch 118/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5871 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00118: binary_accuracy did not improve from 0.72308\n",
            "Epoch 119/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5865 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00119: binary_accuracy did not improve from 0.72308\n",
            "Epoch 120/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5859 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00120: binary_accuracy did not improve from 0.72308\n",
            "Epoch 121/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5855 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00121: binary_accuracy did not improve from 0.72308\n",
            "Epoch 122/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5850 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00122: binary_accuracy did not improve from 0.72308\n",
            "Epoch 123/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5846 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00123: binary_accuracy did not improve from 0.72308\n",
            "Epoch 124/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5841 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00124: binary_accuracy did not improve from 0.72308\n",
            "Epoch 125/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5836 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00125: binary_accuracy did not improve from 0.72308\n",
            "Epoch 126/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5831 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00126: binary_accuracy did not improve from 0.72308\n",
            "Epoch 127/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5826 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00127: binary_accuracy did not improve from 0.72308\n",
            "Epoch 128/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5822 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00128: binary_accuracy did not improve from 0.72308\n",
            "Epoch 129/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5817 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00129: binary_accuracy did not improve from 0.72308\n",
            "Epoch 130/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5812 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00130: binary_accuracy did not improve from 0.72308\n",
            "Epoch 131/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5807 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00131: binary_accuracy did not improve from 0.72308\n",
            "Epoch 132/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5803 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00132: binary_accuracy did not improve from 0.72308\n",
            "Epoch 133/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5798 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00133: binary_accuracy did not improve from 0.72308\n",
            "Epoch 134/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5793 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00134: binary_accuracy did not improve from 0.72308\n",
            "Epoch 135/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5787 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00135: binary_accuracy did not improve from 0.72308\n",
            "Epoch 136/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5783 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00136: binary_accuracy did not improve from 0.72308\n",
            "Epoch 137/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5778 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00137: binary_accuracy did not improve from 0.72308\n",
            "(5, 1)\n",
            "2017-01-08\n",
            "2017-01-14\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([29, 36]))\n",
            "{0: 1.2413793103448276, 1: 1}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 29s 442ms/step - loss: 0.7754 - binary_accuracy: 0.4615\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.46154, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7710 - binary_accuracy: 0.4769\n",
            "\n",
            "Epoch 00002: binary_accuracy improved from 0.46154 to 0.47692, saving model to best.h5\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7691 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00003: binary_accuracy improved from 0.47692 to 0.49231, saving model to best.h5\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7675 - binary_accuracy: 0.4769\n",
            "\n",
            "Epoch 00004: binary_accuracy did not improve from 0.49231\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7661 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00005: binary_accuracy did not improve from 0.49231\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7648 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00006: binary_accuracy improved from 0.49231 to 0.53846, saving model to best.h5\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7635 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00007: binary_accuracy did not improve from 0.53846\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7620 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00008: binary_accuracy did not improve from 0.53846\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7605 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00009: binary_accuracy did not improve from 0.53846\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7588 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00010: binary_accuracy did not improve from 0.53846\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7570 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.53846\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7550 - binary_accuracy: 0.5077\n",
            "\n",
            "Epoch 00012: binary_accuracy did not improve from 0.53846\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7527 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00013: binary_accuracy improved from 0.53846 to 0.56923, saving model to best.h5\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7503 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00014: binary_accuracy improved from 0.56923 to 0.58462, saving model to best.h5\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7476 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00015: binary_accuracy improved from 0.58462 to 0.61538, saving model to best.h5\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7447 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00016: binary_accuracy improved from 0.61538 to 0.64615, saving model to best.h5\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7416 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.64615\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7384 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.64615\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7351 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.64615\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7319 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.64615\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7288 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.64615\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7258 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.64615\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7230 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.64615\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7205 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.64615\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7182 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.64615\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7161 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.64615\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7141 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.64615\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7123 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.64615\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7104 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.64615\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7086 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.64615\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7068 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.64615\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7049 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.64615\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7030 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.64615\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7011 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.64615\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6990 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.64615\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6970 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.64615\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6948 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.64615\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6926 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.64615\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6903 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.64615\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6881 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.64615\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6857 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.64615\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6834 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.64615\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6810 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.64615\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6786 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.64615\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6762 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.64615\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6738 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.64615\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6713 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.64615\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6689 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.64615\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6665 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.64615\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6641 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.64615\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6617 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00051: binary_accuracy did not improve from 0.64615\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6594 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.64615\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6571 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00053: binary_accuracy did not improve from 0.64615\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6548 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.64615\n",
            "Epoch 55/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6524 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00055: binary_accuracy did not improve from 0.64615\n",
            "Epoch 56/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6501 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00056: binary_accuracy did not improve from 0.64615\n",
            "(5, 1)\n",
            "2017-01-08\n",
            "2017-01-14\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([29, 36]))\n",
            "{0: 1.2413793103448276, 1: 1}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 28s 436ms/step - loss: 0.7762 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.55385, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7704 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00002: binary_accuracy did not improve from 0.55385\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7678 - binary_accuracy: 0.5077\n",
            "\n",
            "Epoch 00003: binary_accuracy did not improve from 0.55385\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7658 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00004: binary_accuracy did not improve from 0.55385\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7640 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00005: binary_accuracy improved from 0.55385 to 0.56923, saving model to best.h5\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7625 - binary_accuracy: 0.4615\n",
            "\n",
            "Epoch 00006: binary_accuracy did not improve from 0.56923\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7610 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00007: binary_accuracy did not improve from 0.56923\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7595 - binary_accuracy: 0.5077\n",
            "\n",
            "Epoch 00008: binary_accuracy did not improve from 0.56923\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7580 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00009: binary_accuracy did not improve from 0.56923\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7563 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00010: binary_accuracy improved from 0.56923 to 0.60000, saving model to best.h5\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7544 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.60000\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7524 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00012: binary_accuracy improved from 0.60000 to 0.61538, saving model to best.h5\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7501 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00013: binary_accuracy improved from 0.61538 to 0.63077, saving model to best.h5\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7476 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00014: binary_accuracy improved from 0.63077 to 0.66154, saving model to best.h5\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7449 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.66154\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7420 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.66154\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7389 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.66154\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7356 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.66154\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7322 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.66154\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7287 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.66154\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7252 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.66154\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7217 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.66154\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7184 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.66154\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7151 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.66154\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7120 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.66154\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7090 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.66154\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7060 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.66154\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7032 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.66154\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7004 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.66154\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6976 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.66154\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6948 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.66154\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6920 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.66154\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6891 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.66154\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6861 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.66154\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6831 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.66154\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6800 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.66154\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6769 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.66154\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6737 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.66154\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6704 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.66154\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6672 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.66154\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6639 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.66154\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6607 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.66154\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6575 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.66154\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6545 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00044: binary_accuracy improved from 0.66154 to 0.67692, saving model to best.h5\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6516 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.67692\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6488 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.67692\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6463 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.67692\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6438 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.67692\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6415 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.67692\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6393 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.67692\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6372 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00051: binary_accuracy did not improve from 0.67692\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6352 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.67692\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6332 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00053: binary_accuracy did not improve from 0.67692\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6313 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.67692\n",
            "Epoch 55/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6295 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00055: binary_accuracy improved from 0.67692 to 0.69231, saving model to best.h5\n",
            "Epoch 56/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6277 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00056: binary_accuracy did not improve from 0.69231\n",
            "Epoch 57/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6258 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00057: binary_accuracy did not improve from 0.69231\n",
            "Epoch 58/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6239 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00058: binary_accuracy did not improve from 0.69231\n",
            "Epoch 59/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6220 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00059: binary_accuracy did not improve from 0.69231\n",
            "Epoch 60/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6201 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00060: binary_accuracy improved from 0.69231 to 0.70769, saving model to best.h5\n",
            "Epoch 61/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6181 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00061: binary_accuracy did not improve from 0.70769\n",
            "Epoch 62/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6161 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00062: binary_accuracy did not improve from 0.70769\n",
            "Epoch 63/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6142 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00063: binary_accuracy did not improve from 0.70769\n",
            "Epoch 64/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6123 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00064: binary_accuracy did not improve from 0.70769\n",
            "Epoch 65/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6103 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00065: binary_accuracy did not improve from 0.70769\n",
            "Epoch 66/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6083 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00066: binary_accuracy did not improve from 0.70769\n",
            "Epoch 67/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6062 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00067: binary_accuracy did not improve from 0.70769\n",
            "Epoch 68/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6042 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00068: binary_accuracy did not improve from 0.70769\n",
            "Epoch 69/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6022 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00069: binary_accuracy did not improve from 0.70769\n",
            "Epoch 70/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6002 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00070: binary_accuracy did not improve from 0.70769\n",
            "Epoch 71/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5982 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00071: binary_accuracy did not improve from 0.70769\n",
            "Epoch 72/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5960 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00072: binary_accuracy did not improve from 0.70769\n",
            "Epoch 73/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5939 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00073: binary_accuracy did not improve from 0.70769\n",
            "Epoch 74/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5917 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00074: binary_accuracy did not improve from 0.70769\n",
            "Epoch 75/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5894 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00075: binary_accuracy did not improve from 0.70769\n",
            "Epoch 76/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5874 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00076: binary_accuracy did not improve from 0.70769\n",
            "Epoch 77/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5856 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00077: binary_accuracy did not improve from 0.70769\n",
            "Epoch 78/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5838 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00078: binary_accuracy did not improve from 0.70769\n",
            "Epoch 79/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5821 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00079: binary_accuracy did not improve from 0.70769\n",
            "Epoch 80/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5809 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00080: binary_accuracy did not improve from 0.70769\n",
            "Epoch 81/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5815 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00081: binary_accuracy did not improve from 0.70769\n",
            "Epoch 82/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5811 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00082: binary_accuracy did not improve from 0.70769\n",
            "Epoch 83/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5787 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00083: binary_accuracy did not improve from 0.70769\n",
            "Epoch 84/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5780 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00084: binary_accuracy did not improve from 0.70769\n",
            "Epoch 85/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5843 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00085: binary_accuracy did not improve from 0.70769\n",
            "Epoch 86/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5787 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00086: binary_accuracy did not improve from 0.70769\n",
            "Epoch 87/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5811 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00087: binary_accuracy did not improve from 0.70769\n",
            "Epoch 88/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5912 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00088: binary_accuracy did not improve from 0.70769\n",
            "Epoch 89/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5696 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00089: binary_accuracy did not improve from 0.70769\n",
            "Epoch 90/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5744 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00090: binary_accuracy did not improve from 0.70769\n",
            "Epoch 91/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5849 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00091: binary_accuracy did not improve from 0.70769\n",
            "Epoch 92/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5624 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00092: binary_accuracy did not improve from 0.70769\n",
            "Epoch 93/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5934 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00093: binary_accuracy did not improve from 0.70769\n",
            "Epoch 94/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5812 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00094: binary_accuracy did not improve from 0.70769\n",
            "Epoch 95/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6025 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00095: binary_accuracy did not improve from 0.70769\n",
            "Epoch 96/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5737 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00096: binary_accuracy did not improve from 0.70769\n",
            "Epoch 97/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5961 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00097: binary_accuracy did not improve from 0.70769\n",
            "Epoch 98/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5897 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00098: binary_accuracy did not improve from 0.70769\n",
            "Epoch 99/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6004 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00099: binary_accuracy did not improve from 0.70769\n",
            "Epoch 100/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5832 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00100: binary_accuracy did not improve from 0.70769\n",
            "(5, 1)\n",
            "2017-01-08\n",
            "2017-01-14\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([29, 36]))\n",
            "{0: 1.2413793103448276, 1: 1}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 28s 428ms/step - loss: 0.7721 - binary_accuracy: 0.4462\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.44615, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7681 - binary_accuracy: 0.4769\n",
            "\n",
            "Epoch 00002: binary_accuracy improved from 0.44615 to 0.47692, saving model to best.h5\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7664 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00003: binary_accuracy improved from 0.47692 to 0.52308, saving model to best.h5\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7649 - binary_accuracy: 0.5077\n",
            "\n",
            "Epoch 00004: binary_accuracy did not improve from 0.52308\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7635 - binary_accuracy: 0.5077\n",
            "\n",
            "Epoch 00005: binary_accuracy did not improve from 0.52308\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7621 - binary_accuracy: 0.5077\n",
            "\n",
            "Epoch 00006: binary_accuracy did not improve from 0.52308\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7604 - binary_accuracy: 0.5077\n",
            "\n",
            "Epoch 00007: binary_accuracy did not improve from 0.52308\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7586 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00008: binary_accuracy did not improve from 0.52308\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7566 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00009: binary_accuracy improved from 0.52308 to 0.53846, saving model to best.h5\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7543 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00010: binary_accuracy improved from 0.53846 to 0.56923, saving model to best.h5\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7517 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00011: binary_accuracy improved from 0.56923 to 0.60000, saving model to best.h5\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7487 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00012: binary_accuracy improved from 0.60000 to 0.63077, saving model to best.h5\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7454 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00013: binary_accuracy did not improve from 0.63077\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7417 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00014: binary_accuracy improved from 0.63077 to 0.67692, saving model to best.h5\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7375 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00015: binary_accuracy improved from 0.67692 to 0.69231, saving model to best.h5\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7329 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.69231\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7280 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.69231\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7227 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.69231\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7171 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.69231\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7114 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.69231\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7057 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.69231\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7003 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.69231\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6951 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.69231\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6904 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.69231\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6859 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.69231\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6817 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.69231\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6774 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.69231\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6732 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.69231\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6690 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.69231\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6649 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.69231\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6608 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.69231\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6567 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.69231\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6526 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.69231\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6484 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.69231\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6444 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.69231\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6406 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.69231\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6371 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.69231\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6334 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.69231\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6300 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.69231\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6265 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.69231\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6233 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.69231\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6199 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.69231\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6167 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.69231\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6137 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.69231\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6107 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.69231\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6096 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.69231\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6080 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.69231\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6067 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.69231\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6107 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.69231\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6174 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.69231\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6047 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00051: binary_accuracy did not improve from 0.69231\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6107 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.69231\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6029 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00053: binary_accuracy did not improve from 0.69231\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6068 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.69231\n",
            "Epoch 55/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6133 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00055: binary_accuracy did not improve from 0.69231\n",
            "(5, 1)\n",
            "2017-01-15\n",
            "2017-01-21\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([32, 33]))\n",
            "{0: 1.03125, 1: 1}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 28s 426ms/step - loss: 0.7086 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.49231, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7052 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00002: binary_accuracy improved from 0.49231 to 0.52308, saving model to best.h5\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7035 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00003: binary_accuracy did not improve from 0.52308\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7018 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00004: binary_accuracy improved from 0.52308 to 0.55385, saving model to best.h5\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7000 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00005: binary_accuracy improved from 0.55385 to 0.56923, saving model to best.h5\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6980 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00006: binary_accuracy improved from 0.56923 to 0.61538, saving model to best.h5\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6957 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00007: binary_accuracy improved from 0.61538 to 0.64615, saving model to best.h5\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6932 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00008: binary_accuracy improved from 0.64615 to 0.66154, saving model to best.h5\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6903 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00009: binary_accuracy improved from 0.66154 to 0.69231, saving model to best.h5\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6869 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00010: binary_accuracy improved from 0.69231 to 0.72308, saving model to best.h5\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6832 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.72308\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6789 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00012: binary_accuracy did not improve from 0.72308\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6742 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00013: binary_accuracy did not improve from 0.72308\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6689 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00014: binary_accuracy did not improve from 0.72308\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6633 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.72308\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6574 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.72308\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6513 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.72308\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6453 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.72308\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6397 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.72308\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6344 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.72308\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6298 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.72308\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6258 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.72308\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6224 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.72308\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6194 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.72308\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6169 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.72308\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6147 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.72308\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6127 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.72308\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6108 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.72308\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6091 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.72308\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6075 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.72308\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6060 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.72308\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6045 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.72308\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6031 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.72308\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6018 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.72308\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6004 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.72308\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5991 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.72308\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5979 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.72308\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5966 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.72308\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5955 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.72308\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5943 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.72308\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5933 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.72308\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5922 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.72308\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5912 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.72308\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5901 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.72308\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5892 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.72308\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5882 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.72308\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5872 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.72308\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5863 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.72308\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5853 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.72308\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5844 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.72308\n",
            "(5, 1)\n",
            "2017-01-15\n",
            "2017-01-21\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([32, 33]))\n",
            "{0: 1.03125, 1: 1}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 32s 495ms/step - loss: 0.7075 - binary_accuracy: 0.4615\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.46154, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.7040 - binary_accuracy: 0.5077\n",
            "\n",
            "Epoch 00002: binary_accuracy improved from 0.46154 to 0.50769, saving model to best.h5\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7024 - binary_accuracy: 0.5077\n",
            "\n",
            "Epoch 00003: binary_accuracy did not improve from 0.50769\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7008 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00004: binary_accuracy did not improve from 0.50769\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6992 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00005: binary_accuracy improved from 0.50769 to 0.53846, saving model to best.h5\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6975 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00006: binary_accuracy did not improve from 0.53846\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6956 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00007: binary_accuracy improved from 0.53846 to 0.61538, saving model to best.h5\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6935 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00008: binary_accuracy improved from 0.61538 to 0.64615, saving model to best.h5\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6913 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00009: binary_accuracy improved from 0.64615 to 0.67692, saving model to best.h5\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6887 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00010: binary_accuracy did not improve from 0.67692\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6859 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.67692\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6829 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00012: binary_accuracy did not improve from 0.67692\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6797 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00013: binary_accuracy did not improve from 0.67692\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6762 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00014: binary_accuracy did not improve from 0.67692\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6726 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.67692\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6689 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.67692\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6651 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.67692\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6614 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.67692\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6578 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.67692\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6544 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.67692\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6512 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.67692\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6482 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.67692\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6454 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.67692\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6428 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.67692\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6404 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.67692\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6382 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.67692\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6361 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.67692\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6341 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.67692\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6323 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.67692\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6305 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.67692\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6289 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.67692\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6273 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.67692\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6258 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.67692\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6243 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.67692\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6228 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.67692\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6213 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.67692\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6198 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.67692\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6183 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.67692\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6167 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.67692\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6151 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.67692\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6135 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.67692\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6118 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.67692\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6101 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.67692\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6083 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.67692\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6065 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.67692\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6047 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.67692\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6028 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.67692\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6010 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.67692\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5991 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.67692\n",
            "(5, 1)\n",
            "2017-01-15\n",
            "2017-01-21\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([32, 33]))\n",
            "{0: 1.03125, 1: 1}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 30s 459ms/step - loss: 0.7044 - binary_accuracy: 0.4462\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.44615, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7019 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00002: binary_accuracy improved from 0.44615 to 0.53846, saving model to best.h5\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7007 - binary_accuracy: 0.5077\n",
            "\n",
            "Epoch 00003: binary_accuracy did not improve from 0.53846\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.6995 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00004: binary_accuracy did not improve from 0.53846\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6981 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00005: binary_accuracy did not improve from 0.53846\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6967 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00006: binary_accuracy improved from 0.53846 to 0.56923, saving model to best.h5\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6951 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00007: binary_accuracy improved from 0.56923 to 0.60000, saving model to best.h5\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6933 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00008: binary_accuracy improved from 0.60000 to 0.61538, saving model to best.h5\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6913 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00009: binary_accuracy did not improve from 0.61538\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6891 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00010: binary_accuracy improved from 0.61538 to 0.64615, saving model to best.h5\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6867 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.64615\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6840 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00012: binary_accuracy did not improve from 0.64615\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6811 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00013: binary_accuracy did not improve from 0.64615\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6779 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00014: binary_accuracy improved from 0.64615 to 0.66154, saving model to best.h5\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6745 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.66154\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6710 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.66154\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6673 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.66154\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6635 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.66154\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6597 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.66154\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6558 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.66154\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6519 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.66154\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6480 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.66154\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6441 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.66154\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6402 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.66154\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6363 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.66154\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6327 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.66154\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6292 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.66154\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6260 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.66154\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6231 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.66154\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6204 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.66154\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6180 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.66154\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6157 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.66154\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6136 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.66154\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6115 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.66154\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6096 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.66154\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6076 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.66154\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6057 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.66154\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6038 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.66154\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6019 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.66154\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6000 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.66154\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5982 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.66154\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5964 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.66154\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5945 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.66154\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5927 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.66154\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5909 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.66154\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5891 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.66154\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5872 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.66154\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5853 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.66154\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5834 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.66154\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5815 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.66154\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5796 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00051: binary_accuracy did not improve from 0.66154\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5776 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.66154\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5756 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00053: binary_accuracy did not improve from 0.66154\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5737 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.66154\n",
            "(5, 1)\n",
            "2017-01-15\n",
            "2017-01-21\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([32, 33]))\n",
            "{0: 1.03125, 1: 1}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 32s 485ms/step - loss: 0.7081 - binary_accuracy: 0.4462\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.44615, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7058 - binary_accuracy: 0.4769\n",
            "\n",
            "Epoch 00002: binary_accuracy improved from 0.44615 to 0.47692, saving model to best.h5\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7049 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00003: binary_accuracy improved from 0.47692 to 0.49231, saving model to best.h5\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7042 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00004: binary_accuracy did not improve from 0.49231\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7035 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00005: binary_accuracy improved from 0.49231 to 0.52308, saving model to best.h5\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7027 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00006: binary_accuracy improved from 0.52308 to 0.53846, saving model to best.h5\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7019 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00007: binary_accuracy did not improve from 0.53846\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7011 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00008: binary_accuracy did not improve from 0.53846\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7001 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00009: binary_accuracy did not improve from 0.53846\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6990 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00010: binary_accuracy did not improve from 0.53846\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6976 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.53846\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6960 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00012: binary_accuracy did not improve from 0.53846\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6941 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00013: binary_accuracy improved from 0.53846 to 0.55385, saving model to best.h5\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6917 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00014: binary_accuracy improved from 0.55385 to 0.56923, saving model to best.h5\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6889 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00015: binary_accuracy improved from 0.56923 to 0.60000, saving model to best.h5\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6855 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00016: binary_accuracy improved from 0.60000 to 0.61538, saving model to best.h5\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6815 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00017: binary_accuracy improved from 0.61538 to 0.64615, saving model to best.h5\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6769 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.64615\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6717 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.64615\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6660 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.64615\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6599 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00021: binary_accuracy improved from 0.64615 to 0.66154, saving model to best.h5\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6537 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.66154\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6475 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.66154\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6415 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.66154\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6360 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.66154\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6310 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.66154\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6266 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.66154\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6229 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.66154\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6196 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.66154\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6168 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00030: binary_accuracy improved from 0.66154 to 0.67692, saving model to best.h5\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6144 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00031: binary_accuracy improved from 0.67692 to 0.70769, saving model to best.h5\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6124 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.70769\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6106 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.70769\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6090 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.70769\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6076 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.70769\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6063 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.70769\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6052 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.70769\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6042 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.70769\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6032 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.70769\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6023 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.70769\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6015 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00041: binary_accuracy improved from 0.70769 to 0.72308, saving model to best.h5\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6007 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.72308\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5999 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.72308\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5992 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.72308\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5984 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.72308\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5977 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.72308\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5969 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.72308\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5962 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.72308\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5955 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.72308\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5947 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.72308\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5940 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00051: binary_accuracy did not improve from 0.72308\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5932 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.72308\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5924 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00053: binary_accuracy did not improve from 0.72308\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5915 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.72308\n",
            "Epoch 55/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5907 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00055: binary_accuracy did not improve from 0.72308\n",
            "Epoch 56/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5898 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00056: binary_accuracy did not improve from 0.72308\n",
            "Epoch 57/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5888 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00057: binary_accuracy did not improve from 0.72308\n",
            "Epoch 58/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5879 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00058: binary_accuracy did not improve from 0.72308\n",
            "Epoch 59/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5869 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00059: binary_accuracy did not improve from 0.72308\n",
            "Epoch 60/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5859 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00060: binary_accuracy did not improve from 0.72308\n",
            "Epoch 61/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5849 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00061: binary_accuracy did not improve from 0.72308\n",
            "Epoch 62/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5838 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00062: binary_accuracy did not improve from 0.72308\n",
            "Epoch 63/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5827 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00063: binary_accuracy did not improve from 0.72308\n",
            "Epoch 64/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5816 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00064: binary_accuracy did not improve from 0.72308\n",
            "Epoch 65/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5806 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00065: binary_accuracy did not improve from 0.72308\n",
            "Epoch 66/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5795 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00066: binary_accuracy did not improve from 0.72308\n",
            "Epoch 67/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5784 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00067: binary_accuracy did not improve from 0.72308\n",
            "Epoch 68/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5773 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00068: binary_accuracy did not improve from 0.72308\n",
            "Epoch 69/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5762 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00069: binary_accuracy did not improve from 0.72308\n",
            "Epoch 70/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5751 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00070: binary_accuracy did not improve from 0.72308\n",
            "Epoch 71/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5740 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00071: binary_accuracy did not improve from 0.72308\n",
            "Epoch 72/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5730 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00072: binary_accuracy did not improve from 0.72308\n",
            "Epoch 73/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5719 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00073: binary_accuracy did not improve from 0.72308\n",
            "Epoch 74/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5709 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00074: binary_accuracy did not improve from 0.72308\n",
            "Epoch 75/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5698 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00075: binary_accuracy did not improve from 0.72308\n",
            "Epoch 76/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5688 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00076: binary_accuracy did not improve from 0.72308\n",
            "Epoch 77/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5677 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00077: binary_accuracy did not improve from 0.72308\n",
            "Epoch 78/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5667 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00078: binary_accuracy did not improve from 0.72308\n",
            "Epoch 79/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5656 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00079: binary_accuracy did not improve from 0.72308\n",
            "Epoch 80/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5646 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00080: binary_accuracy did not improve from 0.72308\n",
            "Epoch 81/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5636 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00081: binary_accuracy did not improve from 0.72308\n",
            "(5, 1)\n",
            "2017-01-22\n",
            "2017-01-28\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([35, 30]))\n",
            "{0: 1, 1: 1.1666666666666667}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 32s 495ms/step - loss: 0.7491 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.49231, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.7452 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00002: binary_accuracy improved from 0.49231 to 0.60000, saving model to best.h5\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.7432 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00003: binary_accuracy did not improve from 0.60000\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.7410 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00004: binary_accuracy did not improve from 0.60000\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7384 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00005: binary_accuracy improved from 0.60000 to 0.66154, saving model to best.h5\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7355 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00006: binary_accuracy did not improve from 0.66154\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7319 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00007: binary_accuracy improved from 0.66154 to 0.67692, saving model to best.h5\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7277 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00008: binary_accuracy did not improve from 0.67692\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7227 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00009: binary_accuracy did not improve from 0.67692\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7169 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00010: binary_accuracy did not improve from 0.67692\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7102 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.67692\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7029 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00012: binary_accuracy did not improve from 0.67692\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6951 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00013: binary_accuracy did not improve from 0.67692\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6872 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00014: binary_accuracy did not improve from 0.67692\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6796 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.67692\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6725 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.67692\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6659 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.67692\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6595 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.67692\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6534 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.67692\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6475 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.67692\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6420 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.67692\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6369 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.67692\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6322 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.67692\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6280 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00024: binary_accuracy improved from 0.67692 to 0.69231, saving model to best.h5\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6241 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.69231\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6206 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00026: binary_accuracy improved from 0.69231 to 0.70769, saving model to best.h5\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6174 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.70769\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6146 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.70769\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6119 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.70769\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6096 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.70769\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6075 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.70769\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6056 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.70769\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6039 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.70769\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6024 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.70769\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6011 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.70769\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5997 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.70769\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5984 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.70769\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5970 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00038: binary_accuracy improved from 0.70769 to 0.72308, saving model to best.h5\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5956 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.72308\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5939 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.72308\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5918 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.72308\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5896 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.72308\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5870 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.72308\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5836 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.72308\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5809 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.72308\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5781 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.72308\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5757 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.72308\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5729 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.72308\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5711 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.72308\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5686 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.72308\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5670 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00051: binary_accuracy did not improve from 0.72308\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5650 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.72308\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5639 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00053: binary_accuracy did not improve from 0.72308\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5624 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.72308\n",
            "Epoch 55/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5615 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00055: binary_accuracy did not improve from 0.72308\n",
            "Epoch 56/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5598 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00056: binary_accuracy did not improve from 0.72308\n",
            "Epoch 57/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5603 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00057: binary_accuracy did not improve from 0.72308\n",
            "Epoch 58/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5584 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00058: binary_accuracy did not improve from 0.72308\n",
            "Epoch 59/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5593 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00059: binary_accuracy did not improve from 0.72308\n",
            "Epoch 60/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5566 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00060: binary_accuracy did not improve from 0.72308\n",
            "Epoch 61/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5589 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00061: binary_accuracy did not improve from 0.72308\n",
            "Epoch 62/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5548 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00062: binary_accuracy did not improve from 0.72308\n",
            "Epoch 63/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5587 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00063: binary_accuracy did not improve from 0.72308\n",
            "Epoch 64/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5524 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00064: binary_accuracy improved from 0.72308 to 0.73846, saving model to best.h5\n",
            "Epoch 65/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5591 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00065: binary_accuracy did not improve from 0.73846\n",
            "Epoch 66/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5499 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00066: binary_accuracy did not improve from 0.73846\n",
            "Epoch 67/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5600 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00067: binary_accuracy did not improve from 0.73846\n",
            "Epoch 68/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5464 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00068: binary_accuracy did not improve from 0.73846\n",
            "Epoch 69/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5620 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00069: binary_accuracy did not improve from 0.73846\n",
            "Epoch 70/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5422 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00070: binary_accuracy improved from 0.73846 to 0.75385, saving model to best.h5\n",
            "Epoch 71/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5647 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00071: binary_accuracy did not improve from 0.75385\n",
            "Epoch 72/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5371 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00072: binary_accuracy did not improve from 0.75385\n",
            "Epoch 73/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5665 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00073: binary_accuracy did not improve from 0.75385\n",
            "Epoch 74/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5325 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00074: binary_accuracy did not improve from 0.75385\n",
            "Epoch 75/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5675 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00075: binary_accuracy did not improve from 0.75385\n",
            "Epoch 76/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5292 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00076: binary_accuracy did not improve from 0.75385\n",
            "Epoch 77/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5720 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00077: binary_accuracy did not improve from 0.75385\n",
            "Epoch 78/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5275 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00078: binary_accuracy did not improve from 0.75385\n",
            "Epoch 79/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5714 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00079: binary_accuracy did not improve from 0.75385\n",
            "Epoch 80/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5272 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00080: binary_accuracy did not improve from 0.75385\n",
            "Epoch 81/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5609 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00081: binary_accuracy did not improve from 0.75385\n",
            "Epoch 82/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5495 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00082: binary_accuracy did not improve from 0.75385\n",
            "Epoch 83/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5255 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00083: binary_accuracy did not improve from 0.75385\n",
            "Epoch 84/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5708 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00084: binary_accuracy did not improve from 0.75385\n",
            "Epoch 85/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5256 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00085: binary_accuracy did not improve from 0.75385\n",
            "Epoch 86/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5550 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00086: binary_accuracy did not improve from 0.75385\n",
            "Epoch 87/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5727 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00087: binary_accuracy did not improve from 0.75385\n",
            "Epoch 88/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5240 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00088: binary_accuracy did not improve from 0.75385\n",
            "Epoch 89/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5630 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00089: binary_accuracy did not improve from 0.75385\n",
            "Epoch 90/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5407 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00090: binary_accuracy did not improve from 0.75385\n",
            "Epoch 91/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5239 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00091: binary_accuracy did not improve from 0.75385\n",
            "Epoch 92/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5633 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00092: binary_accuracy did not improve from 0.75385\n",
            "Epoch 93/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5231 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00093: binary_accuracy did not improve from 0.75385\n",
            "Epoch 94/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5555 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00094: binary_accuracy did not improve from 0.75385\n",
            "Epoch 95/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5601 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00095: binary_accuracy did not improve from 0.75385\n",
            "Epoch 96/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5222 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00096: binary_accuracy did not improve from 0.75385\n",
            "Epoch 97/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5641 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00097: binary_accuracy did not improve from 0.75385\n",
            "Epoch 98/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5262 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00098: binary_accuracy did not improve from 0.75385\n",
            "Epoch 99/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5357 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00099: binary_accuracy did not improve from 0.75385\n",
            "Epoch 100/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6258 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00100: binary_accuracy did not improve from 0.75385\n",
            "Epoch 101/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5508 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00101: binary_accuracy did not improve from 0.75385\n",
            "Epoch 102/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5367 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00102: binary_accuracy did not improve from 0.75385\n",
            "Epoch 103/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5030 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00103: binary_accuracy improved from 0.75385 to 0.76923, saving model to best.h5\n",
            "Epoch 104/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6232 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00104: binary_accuracy did not improve from 0.76923\n",
            "Epoch 105/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5271 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00105: binary_accuracy did not improve from 0.76923\n",
            "Epoch 106/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6222 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00106: binary_accuracy did not improve from 0.76923\n",
            "Epoch 107/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5295 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00107: binary_accuracy did not improve from 0.76923\n",
            "Epoch 108/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4990 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00108: binary_accuracy did not improve from 0.76923\n",
            "Epoch 109/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6091 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00109: binary_accuracy did not improve from 0.76923\n",
            "Epoch 110/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5210 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00110: binary_accuracy did not improve from 0.76923\n",
            "Epoch 111/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5415 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00111: binary_accuracy did not improve from 0.76923\n",
            "Epoch 112/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6061 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00112: binary_accuracy did not improve from 0.76923\n",
            "Epoch 113/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5231 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00113: binary_accuracy did not improve from 0.76923\n",
            "Epoch 114/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5877 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00114: binary_accuracy did not improve from 0.76923\n",
            "Epoch 115/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5163 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00115: binary_accuracy did not improve from 0.76923\n",
            "Epoch 116/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5301 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00116: binary_accuracy did not improve from 0.76923\n",
            "Epoch 117/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4971 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00117: binary_accuracy did not improve from 0.76923\n",
            "Epoch 118/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6202 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00118: binary_accuracy did not improve from 0.76923\n",
            "Epoch 119/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5245 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00119: binary_accuracy did not improve from 0.76923\n",
            "Epoch 120/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5357 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00120: binary_accuracy did not improve from 0.76923\n",
            "Epoch 121/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4927 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00121: binary_accuracy did not improve from 0.76923\n",
            "Epoch 122/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5252 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00122: binary_accuracy did not improve from 0.76923\n",
            "Epoch 123/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4925 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00123: binary_accuracy did not improve from 0.76923\n",
            "Epoch 124/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4914 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00124: binary_accuracy did not improve from 0.76923\n",
            "Epoch 125/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5869 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00125: binary_accuracy did not improve from 0.76923\n",
            "Epoch 126/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5132 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00126: binary_accuracy did not improve from 0.76923\n",
            "Epoch 127/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5299 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00127: binary_accuracy did not improve from 0.76923\n",
            "Epoch 128/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5170 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00128: binary_accuracy did not improve from 0.76923\n",
            "Epoch 129/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5605 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00129: binary_accuracy did not improve from 0.76923\n",
            "Epoch 130/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5108 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00130: binary_accuracy did not improve from 0.76923\n",
            "Epoch 131/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5277 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00131: binary_accuracy did not improve from 0.76923\n",
            "Epoch 132/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5138 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00132: binary_accuracy did not improve from 0.76923\n",
            "Epoch 133/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5567 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00133: binary_accuracy did not improve from 0.76923\n",
            "Epoch 134/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5077 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00134: binary_accuracy did not improve from 0.76923\n",
            "Epoch 135/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5232 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00135: binary_accuracy did not improve from 0.76923\n",
            "Epoch 136/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5263 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00136: binary_accuracy did not improve from 0.76923\n",
            "Epoch 137/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5046 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00137: binary_accuracy did not improve from 0.76923\n",
            "Epoch 138/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5166 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00138: binary_accuracy did not improve from 0.76923\n",
            "Epoch 139/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6105 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00139: binary_accuracy did not improve from 0.76923\n",
            "Epoch 140/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5126 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00140: binary_accuracy did not improve from 0.76923\n",
            "Epoch 141/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5281 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00141: binary_accuracy did not improve from 0.76923\n",
            "Epoch 142/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4995 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00142: binary_accuracy did not improve from 0.76923\n",
            "Epoch 143/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5159 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00143: binary_accuracy did not improve from 0.76923\n",
            "(5, 1)\n",
            "2017-01-22\n",
            "2017-01-28\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([35, 30]))\n",
            "{0: 1, 1: 1.1666666666666667}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 30s 462ms/step - loss: 0.7457 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.53846, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7411 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00002: binary_accuracy improved from 0.53846 to 0.66154, saving model to best.h5\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.7379 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00003: binary_accuracy improved from 0.66154 to 0.69231, saving model to best.h5\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7345 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00004: binary_accuracy improved from 0.69231 to 0.70769, saving model to best.h5\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7307 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00005: binary_accuracy improved from 0.70769 to 0.72308, saving model to best.h5\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7264 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00006: binary_accuracy did not improve from 0.72308\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7215 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00007: binary_accuracy did not improve from 0.72308\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7162 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00008: binary_accuracy did not improve from 0.72308\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7101 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00009: binary_accuracy did not improve from 0.72308\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7032 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00010: binary_accuracy did not improve from 0.72308\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6955 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.72308\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6871 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00012: binary_accuracy did not improve from 0.72308\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6783 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00013: binary_accuracy did not improve from 0.72308\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6694 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00014: binary_accuracy did not improve from 0.72308\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6606 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.72308\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6519 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.72308\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6435 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.72308\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6355 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.72308\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6280 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.72308\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6212 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.72308\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6151 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.72308\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6097 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.72308\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6047 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.72308\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6002 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.72308\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5961 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.72308\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5924 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.72308\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5889 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.72308\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5857 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.72308\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5829 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.72308\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5803 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.72308\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5777 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.72308\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5751 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.72308\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5723 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.72308\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5698 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.72308\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5672 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.72308\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5645 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.72308\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5617 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00037: binary_accuracy improved from 0.72308 to 0.73846, saving model to best.h5\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5587 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.73846\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5559 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.73846\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5533 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.73846\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5510 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.73846\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5494 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.73846\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5481 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.73846\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5464 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00044: binary_accuracy improved from 0.73846 to 0.75385, saving model to best.h5\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5452 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.75385\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5440 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.75385\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5429 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.75385\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5417 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.75385\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5406 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.75385\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5404 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.75385\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5388 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00051: binary_accuracy did not improve from 0.75385\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5386 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.75385\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5386 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00053: binary_accuracy did not improve from 0.75385\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5375 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.75385\n",
            "Epoch 55/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5375 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00055: binary_accuracy did not improve from 0.75385\n",
            "Epoch 56/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5375 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00056: binary_accuracy did not improve from 0.75385\n",
            "Epoch 57/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5361 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00057: binary_accuracy did not improve from 0.75385\n",
            "Epoch 58/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5370 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00058: binary_accuracy did not improve from 0.75385\n",
            "Epoch 59/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5369 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00059: binary_accuracy did not improve from 0.75385\n",
            "Epoch 60/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5345 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00060: binary_accuracy did not improve from 0.75385\n",
            "Epoch 61/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5362 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00061: binary_accuracy did not improve from 0.75385\n",
            "Epoch 62/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5410 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00062: binary_accuracy did not improve from 0.75385\n",
            "Epoch 63/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5294 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00063: binary_accuracy did not improve from 0.75385\n",
            "Epoch 64/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5358 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00064: binary_accuracy did not improve from 0.75385\n",
            "Epoch 65/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5649 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00065: binary_accuracy did not improve from 0.75385\n",
            "Epoch 66/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5278 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00066: binary_accuracy did not improve from 0.75385\n",
            "Epoch 67/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5359 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00067: binary_accuracy did not improve from 0.75385\n",
            "Epoch 68/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6039 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00068: binary_accuracy did not improve from 0.75385\n",
            "Epoch 69/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5404 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00069: binary_accuracy did not improve from 0.75385\n",
            "Epoch 70/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5262 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00070: binary_accuracy did not improve from 0.75385\n",
            "Epoch 71/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5365 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00071: binary_accuracy did not improve from 0.75385\n",
            "Epoch 72/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6091 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00072: binary_accuracy did not improve from 0.75385\n",
            "Epoch 73/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5586 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00073: binary_accuracy did not improve from 0.75385\n",
            "Epoch 74/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5270 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00074: binary_accuracy did not improve from 0.75385\n",
            "Epoch 75/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5474 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00075: binary_accuracy did not improve from 0.75385\n",
            "Epoch 76/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5266 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00076: binary_accuracy did not improve from 0.75385\n",
            "Epoch 77/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5501 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00077: binary_accuracy did not improve from 0.75385\n",
            "Epoch 78/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5255 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00078: binary_accuracy did not improve from 0.75385\n",
            "Epoch 79/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5475 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00079: binary_accuracy did not improve from 0.75385\n",
            "Epoch 80/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5265 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00080: binary_accuracy did not improve from 0.75385\n",
            "Epoch 81/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5505 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00081: binary_accuracy did not improve from 0.75385\n",
            "Epoch 82/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5249 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00082: binary_accuracy did not improve from 0.75385\n",
            "Epoch 83/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5457 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00083: binary_accuracy did not improve from 0.75385\n",
            "Epoch 84/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5283 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00084: binary_accuracy did not improve from 0.75385\n",
            "(5, 1)\n",
            "2017-01-22\n",
            "2017-01-28\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([35, 30]))\n",
            "{0: 1, 1: 1.1666666666666667}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 33s 510ms/step - loss: 0.7494 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.53846, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.7455 - binary_accuracy: 0.5077\n",
            "\n",
            "Epoch 00002: binary_accuracy did not improve from 0.53846\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.7438 - binary_accuracy: 0.5077\n",
            "\n",
            "Epoch 00003: binary_accuracy did not improve from 0.53846\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.7421 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00004: binary_accuracy improved from 0.53846 to 0.55385, saving model to best.h5\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.7400 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00005: binary_accuracy improved from 0.55385 to 0.63077, saving model to best.h5\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7377 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00006: binary_accuracy improved from 0.63077 to 0.66154, saving model to best.h5\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7351 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00007: binary_accuracy did not improve from 0.66154\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7320 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00008: binary_accuracy improved from 0.66154 to 0.67692, saving model to best.h5\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7284 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00009: binary_accuracy did not improve from 0.67692\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7241 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00010: binary_accuracy did not improve from 0.67692\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7193 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.67692\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7136 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00012: binary_accuracy did not improve from 0.67692\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7072 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00013: binary_accuracy did not improve from 0.67692\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7001 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00014: binary_accuracy did not improve from 0.67692\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6922 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.67692\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6840 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.67692\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6756 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.67692\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6672 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.67692\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6590 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.67692\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6512 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.67692\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6441 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.67692\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6378 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.67692\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6322 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.67692\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6273 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.67692\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6229 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00025: binary_accuracy improved from 0.67692 to 0.69231, saving model to best.h5\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6189 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00026: binary_accuracy improved from 0.69231 to 0.70769, saving model to best.h5\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6152 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.70769\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6117 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.70769\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6084 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.70769\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6053 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.70769\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6025 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.70769\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5996 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.70769\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5969 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.70769\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.5944 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.70769\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.5919 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.70769\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.5897 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.70769\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5877 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00037: binary_accuracy improved from 0.70769 to 0.72308, saving model to best.h5\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5857 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.72308\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5839 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.72308\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5817 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.72308\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5799 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.72308\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5779 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.72308\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5762 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.72308\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5740 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.72308\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5735 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.72308\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5686 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.72308\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5816 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.72308\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.5625 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.72308\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6203 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.72308\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5691 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.72308\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5688 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00051: binary_accuracy did not improve from 0.72308\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.5637 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.72308\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5694 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00053: binary_accuracy did not improve from 0.72308\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5607 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.72308\n",
            "Epoch 55/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5776 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00055: binary_accuracy did not improve from 0.72308\n",
            "Epoch 56/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5554 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00056: binary_accuracy did not improve from 0.72308\n",
            "Epoch 57/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6104 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00057: binary_accuracy did not improve from 0.72308\n",
            "Epoch 58/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5585 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00058: binary_accuracy did not improve from 0.72308\n",
            "Epoch 59/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5752 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00059: binary_accuracy did not improve from 0.72308\n",
            "Epoch 60/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5531 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00060: binary_accuracy did not improve from 0.72308\n",
            "Epoch 61/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5772 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00061: binary_accuracy did not improve from 0.72308\n",
            "Epoch 62/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5513 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00062: binary_accuracy improved from 0.72308 to 0.73846, saving model to best.h5\n",
            "Epoch 63/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5763 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00063: binary_accuracy did not improve from 0.73846\n",
            "Epoch 64/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5501 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00064: binary_accuracy did not improve from 0.73846\n",
            "Epoch 65/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5749 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00065: binary_accuracy did not improve from 0.73846\n",
            "Epoch 66/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5488 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00066: binary_accuracy did not improve from 0.73846\n",
            "Epoch 67/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5733 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00067: binary_accuracy did not improve from 0.73846\n",
            "Epoch 68/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5474 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00068: binary_accuracy did not improve from 0.73846\n",
            "Epoch 69/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5721 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00069: binary_accuracy did not improve from 0.73846\n",
            "Epoch 70/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5457 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00070: binary_accuracy did not improve from 0.73846\n",
            "Epoch 71/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5714 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00071: binary_accuracy did not improve from 0.73846\n",
            "Epoch 72/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5435 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00072: binary_accuracy did not improve from 0.73846\n",
            "Epoch 73/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5711 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00073: binary_accuracy did not improve from 0.73846\n",
            "Epoch 74/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5408 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00074: binary_accuracy improved from 0.73846 to 0.75385, saving model to best.h5\n",
            "Epoch 75/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5713 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00075: binary_accuracy did not improve from 0.75385\n",
            "Epoch 76/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5386 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00076: binary_accuracy did not improve from 0.75385\n",
            "Epoch 77/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5717 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00077: binary_accuracy did not improve from 0.75385\n",
            "Epoch 78/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5360 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00078: binary_accuracy did not improve from 0.75385\n",
            "Epoch 79/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5724 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00079: binary_accuracy did not improve from 0.75385\n",
            "Epoch 80/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5336 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00080: binary_accuracy did not improve from 0.75385\n",
            "Epoch 81/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5734 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00081: binary_accuracy did not improve from 0.75385\n",
            "Epoch 82/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5315 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00082: binary_accuracy did not improve from 0.75385\n",
            "Epoch 83/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5745 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00083: binary_accuracy did not improve from 0.75385\n",
            "Epoch 84/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5299 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00084: binary_accuracy did not improve from 0.75385\n",
            "Epoch 85/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5750 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00085: binary_accuracy did not improve from 0.75385\n",
            "Epoch 86/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5286 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00086: binary_accuracy did not improve from 0.75385\n",
            "Epoch 87/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5753 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00087: binary_accuracy did not improve from 0.75385\n",
            "Epoch 88/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5278 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00088: binary_accuracy did not improve from 0.75385\n",
            "Epoch 89/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5743 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00089: binary_accuracy did not improve from 0.75385\n",
            "Epoch 90/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5272 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00090: binary_accuracy did not improve from 0.75385\n",
            "Epoch 91/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5725 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00091: binary_accuracy did not improve from 0.75385\n",
            "Epoch 92/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5265 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00092: binary_accuracy did not improve from 0.75385\n",
            "Epoch 93/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5699 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00093: binary_accuracy did not improve from 0.75385\n",
            "Epoch 94/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5254 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00094: binary_accuracy did not improve from 0.75385\n",
            "Epoch 95/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5673 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00095: binary_accuracy did not improve from 0.75385\n",
            "Epoch 96/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5244 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00096: binary_accuracy did not improve from 0.75385\n",
            "Epoch 97/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5631 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00097: binary_accuracy did not improve from 0.75385\n",
            "Epoch 98/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5238 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00098: binary_accuracy did not improve from 0.75385\n",
            "Epoch 99/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5576 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00099: binary_accuracy did not improve from 0.75385\n",
            "Epoch 100/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5243 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00100: binary_accuracy did not improve from 0.75385\n",
            "Epoch 101/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5516 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00101: binary_accuracy did not improve from 0.75385\n",
            "Epoch 102/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5272 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00102: binary_accuracy did not improve from 0.75385\n",
            "Epoch 103/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5443 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00103: binary_accuracy did not improve from 0.75385\n",
            "Epoch 104/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5668 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00104: binary_accuracy did not improve from 0.75385\n",
            "Epoch 105/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5224 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00105: binary_accuracy did not improve from 0.75385\n",
            "Epoch 106/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5559 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00106: binary_accuracy did not improve from 0.75385\n",
            "Epoch 107/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5215 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00107: binary_accuracy did not improve from 0.75385\n",
            "Epoch 108/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5603 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00108: binary_accuracy did not improve from 0.75385\n",
            "Epoch 109/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5195 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00109: binary_accuracy did not improve from 0.75385\n",
            "Epoch 110/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5506 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00110: binary_accuracy did not improve from 0.75385\n",
            "Epoch 111/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5325 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00111: binary_accuracy did not improve from 0.75385\n",
            "Epoch 112/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5245 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00112: binary_accuracy did not improve from 0.75385\n",
            "Epoch 113/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5531 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00113: binary_accuracy did not improve from 0.75385\n",
            "Epoch 114/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5185 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00114: binary_accuracy did not improve from 0.75385\n",
            "(5, 1)\n",
            "2017-01-22\n",
            "2017-01-28\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([35, 30]))\n",
            "{0: 1, 1: 1.1666666666666667}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 32s 494ms/step - loss: 0.7493 - binary_accuracy: 0.4615\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.46154, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7457 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00002: binary_accuracy improved from 0.46154 to 0.49231, saving model to best.h5\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7437 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00003: binary_accuracy improved from 0.49231 to 0.53846, saving model to best.h5\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7418 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00004: binary_accuracy improved from 0.53846 to 0.60000, saving model to best.h5\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7396 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00005: binary_accuracy improved from 0.60000 to 0.66154, saving model to best.h5\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7373 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00006: binary_accuracy improved from 0.66154 to 0.67692, saving model to best.h5\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7348 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00007: binary_accuracy improved from 0.67692 to 0.69231, saving model to best.h5\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7320 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00008: binary_accuracy did not improve from 0.69231\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7288 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00009: binary_accuracy did not improve from 0.69231\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7251 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00010: binary_accuracy did not improve from 0.69231\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7209 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.69231\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7160 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00012: binary_accuracy did not improve from 0.69231\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7103 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00013: binary_accuracy did not improve from 0.69231\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7038 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00014: binary_accuracy did not improve from 0.69231\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6968 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.69231\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6898 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.69231\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6832 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.69231\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6770 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.69231\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6708 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.69231\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6645 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.69231\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6580 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.69231\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6514 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.69231\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6450 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.69231\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6386 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.69231\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6326 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.69231\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6270 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.69231\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6218 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.69231\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6169 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.69231\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6124 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.69231\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6082 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00030: binary_accuracy improved from 0.69231 to 0.70769, saving model to best.h5\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6041 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.70769\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6000 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.70769\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5959 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.70769\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5921 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.70769\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5881 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.70769\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5848 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.70769\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5811 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.70769\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5781 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.70769\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5743 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.70769\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5722 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.70769\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5679 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.70769\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5670 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.70769\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5625 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.70769\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5628 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.70769\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5581 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00045: binary_accuracy improved from 0.70769 to 0.72308, saving model to best.h5\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5590 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.72308\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5544 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.72308\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5551 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.72308\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5517 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00049: binary_accuracy improved from 0.72308 to 0.73846, saving model to best.h5\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5514 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.73846\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5492 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00051: binary_accuracy did not improve from 0.73846\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5484 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.73846\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5472 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00053: binary_accuracy improved from 0.73846 to 0.75385, saving model to best.h5\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5463 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.75385\n",
            "Epoch 55/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5454 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00055: binary_accuracy did not improve from 0.75385\n",
            "Epoch 56/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5446 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00056: binary_accuracy did not improve from 0.75385\n",
            "Epoch 57/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5437 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00057: binary_accuracy did not improve from 0.75385\n",
            "Epoch 58/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5425 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00058: binary_accuracy did not improve from 0.75385\n",
            "Epoch 59/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5412 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00059: binary_accuracy did not improve from 0.75385\n",
            "Epoch 60/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5400 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00060: binary_accuracy did not improve from 0.75385\n",
            "Epoch 61/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5406 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00061: binary_accuracy did not improve from 0.75385\n",
            "Epoch 62/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5343 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00062: binary_accuracy did not improve from 0.75385\n",
            "Epoch 63/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5391 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00063: binary_accuracy did not improve from 0.75385\n",
            "Epoch 64/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5384 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00064: binary_accuracy did not improve from 0.75385\n",
            "Epoch 65/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5330 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00065: binary_accuracy did not improve from 0.75385\n",
            "Epoch 66/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5372 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00066: binary_accuracy did not improve from 0.75385\n",
            "Epoch 67/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5396 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00067: binary_accuracy did not improve from 0.75385\n",
            "Epoch 68/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5292 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00068: binary_accuracy did not improve from 0.75385\n",
            "Epoch 69/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5372 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00069: binary_accuracy did not improve from 0.75385\n",
            "Epoch 70/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5350 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00070: binary_accuracy did not improve from 0.75385\n",
            "Epoch 71/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5610 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00071: binary_accuracy did not improve from 0.75385\n",
            "Epoch 72/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5251 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00072: binary_accuracy did not improve from 0.75385\n",
            "Epoch 73/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5362 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00073: binary_accuracy did not improve from 0.75385\n",
            "Epoch 74/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5394 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00074: binary_accuracy did not improve from 0.75385\n",
            "Epoch 75/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5268 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00075: binary_accuracy did not improve from 0.75385\n",
            "Epoch 76/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5329 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00076: binary_accuracy did not improve from 0.75385\n",
            "Epoch 77/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5932 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00077: binary_accuracy did not improve from 0.75385\n",
            "Epoch 78/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5223 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00078: binary_accuracy did not improve from 0.75385\n",
            "Epoch 79/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5333 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00079: binary_accuracy did not improve from 0.75385\n",
            "Epoch 80/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6031 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00080: binary_accuracy did not improve from 0.75385\n",
            "Epoch 81/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5241 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00081: binary_accuracy did not improve from 0.75385\n",
            "Epoch 82/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5323 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00082: binary_accuracy did not improve from 0.75385\n",
            "Epoch 83/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6221 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00083: binary_accuracy did not improve from 0.75385\n",
            "Epoch 84/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5388 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00084: binary_accuracy did not improve from 0.75385\n",
            "Epoch 85/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5323 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00085: binary_accuracy did not improve from 0.75385\n",
            "Epoch 86/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4986 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00086: binary_accuracy improved from 0.75385 to 0.76923, saving model to best.h5\n",
            "Epoch 87/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4943 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00087: binary_accuracy did not improve from 0.76923\n",
            "Epoch 88/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5704 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00088: binary_accuracy did not improve from 0.76923\n",
            "Epoch 89/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5246 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00089: binary_accuracy did not improve from 0.76923\n",
            "Epoch 90/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5325 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00090: binary_accuracy did not improve from 0.76923\n",
            "Epoch 91/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5343 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00091: binary_accuracy did not improve from 0.76923\n",
            "Epoch 92/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5913 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00092: binary_accuracy did not improve from 0.76923\n",
            "Epoch 93/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5201 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00093: binary_accuracy did not improve from 0.76923\n",
            "Epoch 94/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5308 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00094: binary_accuracy did not improve from 0.76923\n",
            "Epoch 95/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5316 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00095: binary_accuracy did not improve from 0.76923\n",
            "Epoch 96/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4910 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00096: binary_accuracy did not improve from 0.76923\n",
            "Epoch 97/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4900 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00097: binary_accuracy did not improve from 0.76923\n",
            "Epoch 98/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4896 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00098: binary_accuracy did not improve from 0.76923\n",
            "Epoch 99/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6225 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00099: binary_accuracy did not improve from 0.76923\n",
            "Epoch 100/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5692 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00100: binary_accuracy did not improve from 0.76923\n",
            "Epoch 101/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5288 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00101: binary_accuracy did not improve from 0.76923\n",
            "Epoch 102/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5369 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00102: binary_accuracy did not improve from 0.76923\n",
            "Epoch 103/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5343 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00103: binary_accuracy did not improve from 0.76923\n",
            "Epoch 104/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5312 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00104: binary_accuracy did not improve from 0.76923\n",
            "Epoch 105/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5297 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00105: binary_accuracy did not improve from 0.76923\n",
            "Epoch 106/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4952 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00106: binary_accuracy did not improve from 0.76923\n",
            "Epoch 107/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5217 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00107: binary_accuracy did not improve from 0.76923\n",
            "Epoch 108/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5321 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00108: binary_accuracy did not improve from 0.76923\n",
            "Epoch 109/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5319 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00109: binary_accuracy did not improve from 0.76923\n",
            "Epoch 110/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5278 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00110: binary_accuracy did not improve from 0.76923\n",
            "Epoch 111/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4858 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00111: binary_accuracy did not improve from 0.76923\n",
            "Epoch 112/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4859 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00112: binary_accuracy did not improve from 0.76923\n",
            "Epoch 113/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6058 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00113: binary_accuracy did not improve from 0.76923\n",
            "Epoch 114/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5181 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00114: binary_accuracy did not improve from 0.76923\n",
            "Epoch 115/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5344 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00115: binary_accuracy did not improve from 0.76923\n",
            "Epoch 116/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5294 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00116: binary_accuracy did not improve from 0.76923\n",
            "Epoch 117/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5313 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00117: binary_accuracy did not improve from 0.76923\n",
            "Epoch 118/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5300 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00118: binary_accuracy did not improve from 0.76923\n",
            "Epoch 119/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5259 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00119: binary_accuracy did not improve from 0.76923\n",
            "Epoch 120/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4865 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00120: binary_accuracy did not improve from 0.76923\n",
            "Epoch 121/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5440 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00121: binary_accuracy did not improve from 0.76923\n",
            "Epoch 122/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5167 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00122: binary_accuracy did not improve from 0.76923\n",
            "Epoch 123/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5222 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00123: binary_accuracy did not improve from 0.76923\n",
            "Epoch 124/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5725 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00124: binary_accuracy did not improve from 0.76923\n",
            "Epoch 125/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5172 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00125: binary_accuracy did not improve from 0.76923\n",
            "Epoch 126/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5239 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00126: binary_accuracy did not improve from 0.76923\n",
            "(5, 1)\n",
            "2017-01-29\n",
            "2017-02-04\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([32, 33]))\n",
            "{0: 1.03125, 1: 1}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 32s 489ms/step - loss: 0.7117 - binary_accuracy: 0.3846\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.38462, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7040 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00002: binary_accuracy improved from 0.38462 to 0.53846, saving model to best.h5\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6994 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00003: binary_accuracy did not improve from 0.53846\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6955 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00004: binary_accuracy improved from 0.53846 to 0.61538, saving model to best.h5\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6918 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00005: binary_accuracy did not improve from 0.61538\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6881 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00006: binary_accuracy improved from 0.61538 to 0.64615, saving model to best.h5\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6841 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00007: binary_accuracy did not improve from 0.64615\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6799 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00008: binary_accuracy did not improve from 0.64615\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6754 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00009: binary_accuracy did not improve from 0.64615\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6705 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00010: binary_accuracy did not improve from 0.64615\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6655 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.64615\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6604 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00012: binary_accuracy did not improve from 0.64615\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6553 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00013: binary_accuracy did not improve from 0.64615\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6505 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00014: binary_accuracy did not improve from 0.64615\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6459 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.64615\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6416 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.64615\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6376 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.64615\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6338 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.64615\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6302 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.64615\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6269 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.64615\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6237 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.64615\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6208 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.64615\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6182 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.64615\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6158 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.64615\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6136 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.64615\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6117 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.64615\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6099 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.64615\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6082 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.64615\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6067 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.64615\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6053 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.64615\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6039 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.64615\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6026 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.64615\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6013 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.64615\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.6000 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.64615\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5988 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.64615\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5975 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.64615\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5962 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.64615\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5949 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.64615\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5935 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.64615\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5921 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00040: binary_accuracy improved from 0.64615 to 0.66154, saving model to best.h5\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5907 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.66154\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5892 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00042: binary_accuracy improved from 0.66154 to 0.67692, saving model to best.h5\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5876 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.67692\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5861 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.67692\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5845 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.67692\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5828 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00046: binary_accuracy improved from 0.67692 to 0.69231, saving model to best.h5\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5811 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.69231\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5793 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.69231\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5774 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.69231\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5756 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.69231\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5737 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00051: binary_accuracy did not improve from 0.69231\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5717 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.69231\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5697 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00053: binary_accuracy did not improve from 0.69231\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5677 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.69231\n",
            "Epoch 55/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5657 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00055: binary_accuracy did not improve from 0.69231\n",
            "Epoch 56/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5636 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00056: binary_accuracy did not improve from 0.69231\n",
            "Epoch 57/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5615 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00057: binary_accuracy did not improve from 0.69231\n",
            "Epoch 58/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5594 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00058: binary_accuracy did not improve from 0.69231\n",
            "Epoch 59/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5573 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00059: binary_accuracy did not improve from 0.69231\n",
            "Epoch 60/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5551 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00060: binary_accuracy did not improve from 0.69231\n",
            "Epoch 61/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5530 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00061: binary_accuracy did not improve from 0.69231\n",
            "Epoch 62/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5509 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00062: binary_accuracy did not improve from 0.69231\n",
            "Epoch 63/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5488 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00063: binary_accuracy did not improve from 0.69231\n",
            "Epoch 64/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5466 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00064: binary_accuracy did not improve from 0.69231\n",
            "Epoch 65/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5445 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00065: binary_accuracy did not improve from 0.69231\n",
            "Epoch 66/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5425 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00066: binary_accuracy did not improve from 0.69231\n",
            "Epoch 67/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5404 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00067: binary_accuracy did not improve from 0.69231\n",
            "Epoch 68/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5383 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00068: binary_accuracy did not improve from 0.69231\n",
            "Epoch 69/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5363 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00069: binary_accuracy did not improve from 0.69231\n",
            "Epoch 70/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5342 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00070: binary_accuracy did not improve from 0.69231\n",
            "Epoch 71/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5321 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00071: binary_accuracy improved from 0.69231 to 0.70769, saving model to best.h5\n",
            "Epoch 72/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5301 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00072: binary_accuracy did not improve from 0.70769\n",
            "Epoch 73/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5281 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00073: binary_accuracy did not improve from 0.70769\n",
            "Epoch 74/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5262 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00074: binary_accuracy did not improve from 0.70769\n",
            "Epoch 75/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5242 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00075: binary_accuracy did not improve from 0.70769\n",
            "Epoch 76/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5222 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00076: binary_accuracy did not improve from 0.70769\n",
            "Epoch 77/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5202 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00077: binary_accuracy improved from 0.70769 to 0.72308, saving model to best.h5\n",
            "Epoch 78/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5182 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00078: binary_accuracy did not improve from 0.72308\n",
            "Epoch 79/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5163 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00079: binary_accuracy did not improve from 0.72308\n",
            "Epoch 80/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5146 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00080: binary_accuracy did not improve from 0.72308\n",
            "Epoch 81/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5128 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00081: binary_accuracy did not improve from 0.72308\n",
            "Epoch 82/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5111 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00082: binary_accuracy did not improve from 0.72308\n",
            "Epoch 83/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5096 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00083: binary_accuracy did not improve from 0.72308\n",
            "Epoch 84/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5085 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00084: binary_accuracy did not improve from 0.72308\n",
            "Epoch 85/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5073 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00085: binary_accuracy did not improve from 0.72308\n",
            "Epoch 86/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5063 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00086: binary_accuracy did not improve from 0.72308\n",
            "Epoch 87/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5057 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00087: binary_accuracy did not improve from 0.72308\n",
            "Epoch 88/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5051 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00088: binary_accuracy did not improve from 0.72308\n",
            "Epoch 89/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5046 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00089: binary_accuracy did not improve from 0.72308\n",
            "Epoch 90/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5043 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00090: binary_accuracy did not improve from 0.72308\n",
            "Epoch 91/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5038 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00091: binary_accuracy did not improve from 0.72308\n",
            "Epoch 92/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5035 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00092: binary_accuracy did not improve from 0.72308\n",
            "Epoch 93/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5030 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00093: binary_accuracy did not improve from 0.72308\n",
            "Epoch 94/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5026 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00094: binary_accuracy did not improve from 0.72308\n",
            "Epoch 95/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5021 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00095: binary_accuracy did not improve from 0.72308\n",
            "Epoch 96/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5018 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00096: binary_accuracy did not improve from 0.72308\n",
            "Epoch 97/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5013 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00097: binary_accuracy did not improve from 0.72308\n",
            "Epoch 98/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5009 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00098: binary_accuracy did not improve from 0.72308\n",
            "Epoch 99/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5005 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00099: binary_accuracy did not improve from 0.72308\n",
            "Epoch 100/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5000 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00100: binary_accuracy did not improve from 0.72308\n",
            "Epoch 101/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4996 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00101: binary_accuracy did not improve from 0.72308\n",
            "Epoch 102/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4992 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00102: binary_accuracy did not improve from 0.72308\n",
            "Epoch 103/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4988 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00103: binary_accuracy did not improve from 0.72308\n",
            "Epoch 104/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4984 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00104: binary_accuracy did not improve from 0.72308\n",
            "Epoch 105/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4980 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00105: binary_accuracy did not improve from 0.72308\n",
            "Epoch 106/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4977 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00106: binary_accuracy did not improve from 0.72308\n",
            "Epoch 107/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4973 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00107: binary_accuracy did not improve from 0.72308\n",
            "Epoch 108/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4970 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00108: binary_accuracy did not improve from 0.72308\n",
            "Epoch 109/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4966 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00109: binary_accuracy did not improve from 0.72308\n",
            "Epoch 110/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4963 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00110: binary_accuracy did not improve from 0.72308\n",
            "Epoch 111/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4959 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00111: binary_accuracy did not improve from 0.72308\n",
            "Epoch 112/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4956 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00112: binary_accuracy did not improve from 0.72308\n",
            "Epoch 113/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4952 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00113: binary_accuracy did not improve from 0.72308\n",
            "Epoch 114/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4949 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00114: binary_accuracy did not improve from 0.72308\n",
            "Epoch 115/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4946 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00115: binary_accuracy improved from 0.72308 to 0.73846, saving model to best.h5\n",
            "Epoch 116/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4943 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00116: binary_accuracy did not improve from 0.73846\n",
            "Epoch 117/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4940 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00117: binary_accuracy did not improve from 0.73846\n",
            "Epoch 118/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4937 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00118: binary_accuracy did not improve from 0.73846\n",
            "Epoch 119/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4934 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00119: binary_accuracy did not improve from 0.73846\n",
            "Epoch 120/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4930 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00120: binary_accuracy improved from 0.73846 to 0.75385, saving model to best.h5\n",
            "Epoch 121/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4927 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00121: binary_accuracy did not improve from 0.75385\n",
            "Epoch 122/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4924 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00122: binary_accuracy did not improve from 0.75385\n",
            "Epoch 123/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4921 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00123: binary_accuracy did not improve from 0.75385\n",
            "Epoch 124/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4917 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00124: binary_accuracy did not improve from 0.75385\n",
            "Epoch 125/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4914 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00125: binary_accuracy did not improve from 0.75385\n",
            "Epoch 126/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4911 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00126: binary_accuracy did not improve from 0.75385\n",
            "Epoch 127/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4910 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00127: binary_accuracy did not improve from 0.75385\n",
            "Epoch 128/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4906 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00128: binary_accuracy did not improve from 0.75385\n",
            "Epoch 129/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4901 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00129: binary_accuracy did not improve from 0.75385\n",
            "Epoch 130/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4900 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00130: binary_accuracy did not improve from 0.75385\n",
            "Epoch 131/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4896 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00131: binary_accuracy did not improve from 0.75385\n",
            "Epoch 132/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4891 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00132: binary_accuracy did not improve from 0.75385\n",
            "Epoch 133/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4888 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00133: binary_accuracy did not improve from 0.75385\n",
            "Epoch 134/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4885 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00134: binary_accuracy did not improve from 0.75385\n",
            "Epoch 135/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4882 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00135: binary_accuracy did not improve from 0.75385\n",
            "Epoch 136/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4878 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00136: binary_accuracy did not improve from 0.75385\n",
            "Epoch 137/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4874 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00137: binary_accuracy did not improve from 0.75385\n",
            "Epoch 138/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4870 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00138: binary_accuracy did not improve from 0.75385\n",
            "Epoch 139/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4867 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00139: binary_accuracy did not improve from 0.75385\n",
            "Epoch 140/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4862 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00140: binary_accuracy did not improve from 0.75385\n",
            "Epoch 141/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.4858 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00141: binary_accuracy did not improve from 0.75385\n",
            "Epoch 142/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4854 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00142: binary_accuracy did not improve from 0.75385\n",
            "Epoch 143/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4850 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00143: binary_accuracy did not improve from 0.75385\n",
            "Epoch 144/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4846 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00144: binary_accuracy did not improve from 0.75385\n",
            "Epoch 145/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4841 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00145: binary_accuracy did not improve from 0.75385\n",
            "Epoch 146/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4836 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00146: binary_accuracy did not improve from 0.75385\n",
            "Epoch 147/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4832 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00147: binary_accuracy did not improve from 0.75385\n",
            "Epoch 148/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4826 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00148: binary_accuracy did not improve from 0.75385\n",
            "Epoch 149/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4821 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00149: binary_accuracy did not improve from 0.75385\n",
            "Epoch 150/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4815 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00150: binary_accuracy did not improve from 0.75385\n",
            "Epoch 151/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4810 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00151: binary_accuracy did not improve from 0.75385\n",
            "Epoch 152/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4804 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00152: binary_accuracy did not improve from 0.75385\n",
            "Epoch 153/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4798 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00153: binary_accuracy did not improve from 0.75385\n",
            "Epoch 154/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4792 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00154: binary_accuracy did not improve from 0.75385\n",
            "Epoch 155/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4786 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00155: binary_accuracy did not improve from 0.75385\n",
            "Epoch 156/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4780 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00156: binary_accuracy did not improve from 0.75385\n",
            "Epoch 157/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.4774 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00157: binary_accuracy did not improve from 0.75385\n",
            "Epoch 158/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4768 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00158: binary_accuracy did not improve from 0.75385\n",
            "Epoch 159/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4763 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00159: binary_accuracy did not improve from 0.75385\n",
            "Epoch 160/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4757 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00160: binary_accuracy did not improve from 0.75385\n",
            "(5, 1)\n",
            "2017-01-29\n",
            "2017-02-04\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([32, 33]))\n",
            "{0: 1.03125, 1: 1}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 34s 518ms/step - loss: 0.7082 - binary_accuracy: 0.4615\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.46154, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.7034 - binary_accuracy: 0.4615\n",
            "\n",
            "Epoch 00002: binary_accuracy did not improve from 0.46154\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.7008 - binary_accuracy: 0.4615\n",
            "\n",
            "Epoch 00003: binary_accuracy did not improve from 0.46154\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.6983 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00004: binary_accuracy improved from 0.46154 to 0.49231, saving model to best.h5\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6957 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00005: binary_accuracy did not improve from 0.49231\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6930 - binary_accuracy: 0.5077\n",
            "\n",
            "Epoch 00006: binary_accuracy improved from 0.49231 to 0.50769, saving model to best.h5\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6899 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00007: binary_accuracy improved from 0.50769 to 0.58462, saving model to best.h5\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6865 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00008: binary_accuracy improved from 0.58462 to 0.67692, saving model to best.h5\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6828 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00009: binary_accuracy did not improve from 0.67692\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6788 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00010: binary_accuracy did not improve from 0.67692\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6744 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.67692\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6698 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00012: binary_accuracy did not improve from 0.67692\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6649 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00013: binary_accuracy did not improve from 0.67692\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6600 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00014: binary_accuracy did not improve from 0.67692\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6551 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.67692\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6503 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.67692\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6458 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.67692\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6414 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.67692\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6374 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.67692\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6336 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.67692\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6300 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.67692\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6266 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.67692\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6235 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.67692\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6206 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.67692\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6179 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.67692\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6155 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.67692\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6133 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.67692\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6114 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.67692\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6097 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.67692\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6081 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.67692\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6067 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.67692\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6054 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.67692\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6042 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.67692\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6030 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.67692\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6019 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.67692\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6008 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.67692\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5997 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.67692\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5986 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.67692\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5975 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.67692\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5964 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.67692\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5953 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.67692\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5941 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.67692\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5929 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.67692\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5916 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.67692\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5903 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.67692\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5889 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.67692\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5875 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.67692\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5860 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.67692\n",
            "(5, 1)\n",
            "2017-01-29\n",
            "2017-02-04\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([32, 33]))\n",
            "{0: 1.03125, 1: 1}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 32s 491ms/step - loss: 0.7070 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.49231, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7021 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00002: binary_accuracy improved from 0.49231 to 0.61538, saving model to best.h5\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.6990 - binary_accuracy: 0.5077\n",
            "\n",
            "Epoch 00003: binary_accuracy did not improve from 0.61538\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.6961 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00004: binary_accuracy did not improve from 0.61538\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6933 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00005: binary_accuracy improved from 0.61538 to 0.64615, saving model to best.h5\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6903 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00006: binary_accuracy did not improve from 0.64615\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6872 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00007: binary_accuracy did not improve from 0.64615\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6839 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00008: binary_accuracy did not improve from 0.64615\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.6803 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00009: binary_accuracy did not improve from 0.64615\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6763 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00010: binary_accuracy did not improve from 0.64615\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6721 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.64615\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6676 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00012: binary_accuracy did not improve from 0.64615\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6628 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00013: binary_accuracy did not improve from 0.64615\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.6580 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00014: binary_accuracy did not improve from 0.64615\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6533 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.64615\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6486 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.64615\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6440 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.64615\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6396 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.64615\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6351 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.64615\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6308 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.64615\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6267 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.64615\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6228 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.64615\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6192 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.64615\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6160 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.64615\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6132 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.64615\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6108 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.64615\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6087 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.64615\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6070 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.64615\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6055 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.64615\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6042 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.64615\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6031 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.64615\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6020 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.64615\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6011 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.64615\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6002 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.64615\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5993 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.64615\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5984 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.64615\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5975 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.64615\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5966 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.64615\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5957 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.64615\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5948 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.64615\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5938 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00041: binary_accuracy improved from 0.64615 to 0.66154, saving model to best.h5\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5929 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.66154\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5919 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.66154\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5909 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.66154\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5899 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.66154\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5889 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.66154\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5878 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00047: binary_accuracy improved from 0.66154 to 0.67692, saving model to best.h5\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5867 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.67692\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5855 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.67692\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5844 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.67692\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5832 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00051: binary_accuracy did not improve from 0.67692\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5819 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.67692\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5807 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00053: binary_accuracy did not improve from 0.67692\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5794 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.67692\n",
            "Epoch 55/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5780 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00055: binary_accuracy did not improve from 0.67692\n",
            "Epoch 56/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5767 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00056: binary_accuracy did not improve from 0.67692\n",
            "Epoch 57/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5753 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00057: binary_accuracy did not improve from 0.67692\n",
            "Epoch 58/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5738 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00058: binary_accuracy did not improve from 0.67692\n",
            "Epoch 59/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5723 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00059: binary_accuracy did not improve from 0.67692\n",
            "Epoch 60/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5707 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00060: binary_accuracy did not improve from 0.67692\n",
            "Epoch 61/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5691 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00061: binary_accuracy did not improve from 0.67692\n",
            "Epoch 62/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5674 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00062: binary_accuracy did not improve from 0.67692\n",
            "Epoch 63/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5658 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00063: binary_accuracy did not improve from 0.67692\n",
            "Epoch 64/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5640 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00064: binary_accuracy did not improve from 0.67692\n",
            "Epoch 65/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5623 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00065: binary_accuracy did not improve from 0.67692\n",
            "Epoch 66/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5605 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00066: binary_accuracy did not improve from 0.67692\n",
            "Epoch 67/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5589 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00067: binary_accuracy did not improve from 0.67692\n",
            "Epoch 68/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5571 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00068: binary_accuracy did not improve from 0.67692\n",
            "Epoch 69/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5553 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00069: binary_accuracy did not improve from 0.67692\n",
            "Epoch 70/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5537 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00070: binary_accuracy did not improve from 0.67692\n",
            "Epoch 71/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5517 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00071: binary_accuracy did not improve from 0.67692\n",
            "Epoch 72/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5502 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00072: binary_accuracy did not improve from 0.67692\n",
            "Epoch 73/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5483 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00073: binary_accuracy did not improve from 0.67692\n",
            "Epoch 74/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5466 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00074: binary_accuracy did not improve from 0.67692\n",
            "Epoch 75/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5447 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00075: binary_accuracy did not improve from 0.67692\n",
            "Epoch 76/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5430 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00076: binary_accuracy did not improve from 0.67692\n",
            "Epoch 77/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5413 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00077: binary_accuracy did not improve from 0.67692\n",
            "Epoch 78/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5399 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00078: binary_accuracy did not improve from 0.67692\n",
            "Epoch 79/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5381 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00079: binary_accuracy did not improve from 0.67692\n",
            "Epoch 80/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5370 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00080: binary_accuracy did not improve from 0.67692\n",
            "Epoch 81/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5358 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00081: binary_accuracy did not improve from 0.67692\n",
            "Epoch 82/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5345 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00082: binary_accuracy did not improve from 0.67692\n",
            "Epoch 83/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5337 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00083: binary_accuracy did not improve from 0.67692\n",
            "Epoch 84/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5331 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00084: binary_accuracy did not improve from 0.67692\n",
            "Epoch 85/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5331 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00085: binary_accuracy did not improve from 0.67692\n",
            "Epoch 86/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5336 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00086: binary_accuracy did not improve from 0.67692\n",
            "Epoch 87/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5347 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00087: binary_accuracy did not improve from 0.67692\n",
            "(5, 1)\n",
            "2017-01-29\n",
            "2017-02-04\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([32, 33]))\n",
            "{0: 1.03125, 1: 1}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 34s 524ms/step - loss: 0.7055 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.49231, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7017 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00002: binary_accuracy did not improve from 0.49231\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6994 - binary_accuracy: 0.4769\n",
            "\n",
            "Epoch 00003: binary_accuracy did not improve from 0.49231\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.6971 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00004: binary_accuracy did not improve from 0.49231\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.6947 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00005: binary_accuracy improved from 0.49231 to 0.52308, saving model to best.h5\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6921 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00006: binary_accuracy improved from 0.52308 to 0.56923, saving model to best.h5\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6893 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00007: binary_accuracy improved from 0.56923 to 0.63077, saving model to best.h5\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6864 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00008: binary_accuracy improved from 0.63077 to 0.64615, saving model to best.h5\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6834 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00009: binary_accuracy improved from 0.64615 to 0.69231, saving model to best.h5\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6802 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00010: binary_accuracy did not improve from 0.69231\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6770 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.69231\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6736 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00012: binary_accuracy did not improve from 0.69231\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6702 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00013: binary_accuracy did not improve from 0.69231\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6667 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00014: binary_accuracy did not improve from 0.69231\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6632 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.69231\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6596 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.69231\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6561 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.69231\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6525 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.69231\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6489 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.69231\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6453 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.69231\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6416 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.69231\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6379 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.69231\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6342 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.69231\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6306 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.69231\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6270 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.69231\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6237 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.69231\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6206 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.69231\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6177 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.69231\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6151 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.69231\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6129 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.69231\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6109 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.69231\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6091 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.69231\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6076 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.69231\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6062 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.69231\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6050 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.69231\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6039 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.69231\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6029 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.69231\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6019 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.69231\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6010 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.69231\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6001 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.69231\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5992 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.69231\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5983 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.69231\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5974 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.69231\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5965 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.69231\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5957 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.69231\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5948 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.69231\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5939 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.69231\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5929 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.69231\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5919 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.69231\n",
            "(5, 1)\n",
            "2017-02-05\n",
            "2017-02-11\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([31, 34]))\n",
            "{0: 1.096774193548387, 1: 1}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 37s 564ms/step - loss: 0.7390 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.52308, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7310 - binary_accuracy: 0.4615\n",
            "\n",
            "Epoch 00002: binary_accuracy did not improve from 0.52308\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7275 - binary_accuracy: 0.5077\n",
            "\n",
            "Epoch 00003: binary_accuracy did not improve from 0.52308\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7246 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00004: binary_accuracy improved from 0.52308 to 0.55385, saving model to best.h5\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7219 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00005: binary_accuracy improved from 0.55385 to 0.60000, saving model to best.h5\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7193 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00006: binary_accuracy did not improve from 0.60000\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7167 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00007: binary_accuracy did not improve from 0.60000\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7139 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00008: binary_accuracy did not improve from 0.60000\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7110 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00009: binary_accuracy did not improve from 0.60000\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7077 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00010: binary_accuracy did not improve from 0.60000\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7041 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.60000\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7000 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00012: binary_accuracy did not improve from 0.60000\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6955 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00013: binary_accuracy did not improve from 0.60000\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6905 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00014: binary_accuracy did not improve from 0.60000\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6852 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.60000\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6796 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.60000\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6739 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.60000\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6682 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.60000\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6626 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.60000\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6569 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.60000\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6510 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.60000\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6450 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.60000\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6390 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.60000\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6334 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.60000\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6281 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.60000\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6233 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.60000\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6189 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.60000\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6148 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00028: binary_accuracy improved from 0.60000 to 0.61538, saving model to best.h5\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6111 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.61538\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6076 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.61538\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6045 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00031: binary_accuracy improved from 0.61538 to 0.63077, saving model to best.h5\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6017 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.63077\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5990 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.63077\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5965 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00034: binary_accuracy improved from 0.63077 to 0.64615, saving model to best.h5\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5941 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.64615\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5919 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.64615\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5899 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.64615\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5880 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00038: binary_accuracy improved from 0.64615 to 0.67692, saving model to best.h5\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5863 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.67692\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5846 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.67692\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5830 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.67692\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5816 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.67692\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5802 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.67692\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5789 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.67692\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5775 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.67692\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5764 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.67692\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5753 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.67692\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5742 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.67692\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5731 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00049: binary_accuracy improved from 0.67692 to 0.69231, saving model to best.h5\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5721 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.69231\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5710 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00051: binary_accuracy did not improve from 0.69231\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5701 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.69231\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5692 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00053: binary_accuracy did not improve from 0.69231\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5682 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.69231\n",
            "Epoch 55/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5672 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00055: binary_accuracy did not improve from 0.69231\n",
            "Epoch 56/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5663 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00056: binary_accuracy did not improve from 0.69231\n",
            "Epoch 57/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5654 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00057: binary_accuracy did not improve from 0.69231\n",
            "Epoch 58/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5644 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00058: binary_accuracy did not improve from 0.69231\n",
            "Epoch 59/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5635 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00059: binary_accuracy did not improve from 0.69231\n",
            "Epoch 60/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5625 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00060: binary_accuracy did not improve from 0.69231\n",
            "Epoch 61/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5616 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00061: binary_accuracy did not improve from 0.69231\n",
            "Epoch 62/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5606 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00062: binary_accuracy did not improve from 0.69231\n",
            "Epoch 63/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5597 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00063: binary_accuracy did not improve from 0.69231\n",
            "Epoch 64/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5587 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00064: binary_accuracy did not improve from 0.69231\n",
            "Epoch 65/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5578 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00065: binary_accuracy did not improve from 0.69231\n",
            "Epoch 66/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5570 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00066: binary_accuracy did not improve from 0.69231\n",
            "Epoch 67/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5561 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00067: binary_accuracy did not improve from 0.69231\n",
            "Epoch 68/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5553 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00068: binary_accuracy did not improve from 0.69231\n",
            "Epoch 69/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5544 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00069: binary_accuracy did not improve from 0.69231\n",
            "Epoch 70/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5535 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00070: binary_accuracy did not improve from 0.69231\n",
            "Epoch 71/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5526 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00071: binary_accuracy did not improve from 0.69231\n",
            "Epoch 72/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5518 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00072: binary_accuracy did not improve from 0.69231\n",
            "Epoch 73/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5510 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00073: binary_accuracy did not improve from 0.69231\n",
            "Epoch 74/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5502 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00074: binary_accuracy did not improve from 0.69231\n",
            "Epoch 75/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5493 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00075: binary_accuracy did not improve from 0.69231\n",
            "Epoch 76/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5484 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00076: binary_accuracy did not improve from 0.69231\n",
            "Epoch 77/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5476 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00077: binary_accuracy did not improve from 0.69231\n",
            "Epoch 78/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5467 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00078: binary_accuracy did not improve from 0.69231\n",
            "Epoch 79/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5458 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00079: binary_accuracy did not improve from 0.69231\n",
            "Epoch 80/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5449 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00080: binary_accuracy did not improve from 0.69231\n",
            "Epoch 81/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5441 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00081: binary_accuracy improved from 0.69231 to 0.72308, saving model to best.h5\n",
            "Epoch 82/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5432 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00082: binary_accuracy improved from 0.72308 to 0.73846, saving model to best.h5\n",
            "Epoch 83/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5424 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00083: binary_accuracy did not improve from 0.73846\n",
            "Epoch 84/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5415 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00084: binary_accuracy did not improve from 0.73846\n",
            "Epoch 85/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5406 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00085: binary_accuracy did not improve from 0.73846\n",
            "Epoch 86/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5398 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00086: binary_accuracy did not improve from 0.73846\n",
            "Epoch 87/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5389 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00087: binary_accuracy did not improve from 0.73846\n",
            "Epoch 88/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5381 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00088: binary_accuracy did not improve from 0.73846\n",
            "Epoch 89/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5373 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00089: binary_accuracy did not improve from 0.73846\n",
            "Epoch 90/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5365 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00090: binary_accuracy did not improve from 0.73846\n",
            "Epoch 91/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5358 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00091: binary_accuracy did not improve from 0.73846\n",
            "Epoch 92/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5351 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00092: binary_accuracy did not improve from 0.73846\n",
            "Epoch 93/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5343 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00093: binary_accuracy improved from 0.73846 to 0.75385, saving model to best.h5\n",
            "Epoch 94/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5336 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00094: binary_accuracy did not improve from 0.75385\n",
            "Epoch 95/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5329 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00095: binary_accuracy did not improve from 0.75385\n",
            "Epoch 96/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5322 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00096: binary_accuracy improved from 0.75385 to 0.76923, saving model to best.h5\n",
            "Epoch 97/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5315 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00097: binary_accuracy did not improve from 0.76923\n",
            "Epoch 98/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5309 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00098: binary_accuracy did not improve from 0.76923\n",
            "Epoch 99/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5302 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00099: binary_accuracy did not improve from 0.76923\n",
            "Epoch 100/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5296 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00100: binary_accuracy did not improve from 0.76923\n",
            "Epoch 101/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5290 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00101: binary_accuracy did not improve from 0.76923\n",
            "Epoch 102/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5283 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00102: binary_accuracy did not improve from 0.76923\n",
            "Epoch 103/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5277 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00103: binary_accuracy did not improve from 0.76923\n",
            "Epoch 104/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5271 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00104: binary_accuracy did not improve from 0.76923\n",
            "Epoch 105/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5265 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00105: binary_accuracy did not improve from 0.76923\n",
            "Epoch 106/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5259 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00106: binary_accuracy did not improve from 0.76923\n",
            "Epoch 107/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5254 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00107: binary_accuracy did not improve from 0.76923\n",
            "Epoch 108/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5247 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00108: binary_accuracy improved from 0.76923 to 0.78462, saving model to best.h5\n",
            "Epoch 109/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5241 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00109: binary_accuracy did not improve from 0.78462\n",
            "Epoch 110/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5235 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00110: binary_accuracy did not improve from 0.78462\n",
            "Epoch 111/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5229 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00111: binary_accuracy did not improve from 0.78462\n",
            "Epoch 112/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5223 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00112: binary_accuracy did not improve from 0.78462\n",
            "Epoch 113/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5217 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00113: binary_accuracy did not improve from 0.78462\n",
            "Epoch 114/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5211 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00114: binary_accuracy did not improve from 0.78462\n",
            "Epoch 115/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5205 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00115: binary_accuracy did not improve from 0.78462\n",
            "Epoch 116/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5198 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00116: binary_accuracy did not improve from 0.78462\n",
            "Epoch 117/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5192 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00117: binary_accuracy did not improve from 0.78462\n",
            "Epoch 118/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5185 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00118: binary_accuracy did not improve from 0.78462\n",
            "Epoch 119/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5178 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00119: binary_accuracy did not improve from 0.78462\n",
            "Epoch 120/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5171 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00120: binary_accuracy did not improve from 0.78462\n",
            "Epoch 121/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5165 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00121: binary_accuracy did not improve from 0.78462\n",
            "Epoch 122/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5159 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00122: binary_accuracy did not improve from 0.78462\n",
            "Epoch 123/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5152 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00123: binary_accuracy did not improve from 0.78462\n",
            "Epoch 124/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5146 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00124: binary_accuracy did not improve from 0.78462\n",
            "Epoch 125/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5139 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00125: binary_accuracy did not improve from 0.78462\n",
            "Epoch 126/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5132 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00126: binary_accuracy did not improve from 0.78462\n",
            "Epoch 127/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5125 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00127: binary_accuracy did not improve from 0.78462\n",
            "Epoch 128/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5119 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00128: binary_accuracy did not improve from 0.78462\n",
            "Epoch 129/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5112 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00129: binary_accuracy did not improve from 0.78462\n",
            "Epoch 130/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5105 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00130: binary_accuracy did not improve from 0.78462\n",
            "Epoch 131/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5098 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00131: binary_accuracy did not improve from 0.78462\n",
            "Epoch 132/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5090 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00132: binary_accuracy did not improve from 0.78462\n",
            "Epoch 133/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5083 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00133: binary_accuracy did not improve from 0.78462\n",
            "Epoch 134/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5075 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00134: binary_accuracy did not improve from 0.78462\n",
            "Epoch 135/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5068 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00135: binary_accuracy did not improve from 0.78462\n",
            "Epoch 136/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5060 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00136: binary_accuracy did not improve from 0.78462\n",
            "Epoch 137/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5052 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00137: binary_accuracy did not improve from 0.78462\n",
            "Epoch 138/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5043 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00138: binary_accuracy did not improve from 0.78462\n",
            "Epoch 139/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5035 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00139: binary_accuracy did not improve from 0.78462\n",
            "Epoch 140/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5027 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00140: binary_accuracy did not improve from 0.78462\n",
            "Epoch 141/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5017 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00141: binary_accuracy did not improve from 0.78462\n",
            "Epoch 142/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5008 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00142: binary_accuracy did not improve from 0.78462\n",
            "Epoch 143/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4999 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00143: binary_accuracy did not improve from 0.78462\n",
            "Epoch 144/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4990 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00144: binary_accuracy did not improve from 0.78462\n",
            "Epoch 145/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4980 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00145: binary_accuracy did not improve from 0.78462\n",
            "Epoch 146/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4971 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00146: binary_accuracy did not improve from 0.78462\n",
            "Epoch 147/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4961 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00147: binary_accuracy did not improve from 0.78462\n",
            "Epoch 148/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4953 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00148: binary_accuracy did not improve from 0.78462\n",
            "(5, 1)\n",
            "2017-02-05\n",
            "2017-02-11\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([31, 34]))\n",
            "{0: 1.096774193548387, 1: 1}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 37s 563ms/step - loss: 0.7337 - binary_accuracy: 0.4462\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.44615, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.7267 - binary_accuracy: 0.5077\n",
            "\n",
            "Epoch 00002: binary_accuracy improved from 0.44615 to 0.50769, saving model to best.h5\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7235 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00003: binary_accuracy improved from 0.50769 to 0.53846, saving model to best.h5\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7207 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00004: binary_accuracy improved from 0.53846 to 0.60000, saving model to best.h5\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7181 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00005: binary_accuracy did not improve from 0.60000\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7155 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00006: binary_accuracy did not improve from 0.60000\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7127 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00007: binary_accuracy did not improve from 0.60000\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7095 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00008: binary_accuracy did not improve from 0.60000\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7058 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00009: binary_accuracy did not improve from 0.60000\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7014 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00010: binary_accuracy did not improve from 0.60000\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6962 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.60000\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6906 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00012: binary_accuracy did not improve from 0.60000\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6851 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00013: binary_accuracy did not improve from 0.60000\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6798 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00014: binary_accuracy did not improve from 0.60000\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6747 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.60000\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6693 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.60000\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6636 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.60000\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6578 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.60000\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6520 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.60000\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6463 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.60000\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6410 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.60000\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6361 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.60000\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6315 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.60000\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6276 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.60000\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6239 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.60000\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6206 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.60000\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6177 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.60000\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6151 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00028: binary_accuracy improved from 0.60000 to 0.61538, saving model to best.h5\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6127 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.61538\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6107 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.61538\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6088 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.61538\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6070 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.61538\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6055 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.61538\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6040 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.61538\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6027 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00035: binary_accuracy improved from 0.61538 to 0.63077, saving model to best.h5\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6014 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.63077\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6002 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.63077\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5990 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.63077\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5979 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.63077\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5968 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.63077\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5957 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.63077\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5945 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.63077\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5934 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.63077\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5922 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.63077\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5910 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.63077\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5897 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.63077\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5885 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.63077\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5871 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.63077\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5857 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.63077\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5842 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.63077\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5826 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00051: binary_accuracy did not improve from 0.63077\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5810 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.63077\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5793 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00053: binary_accuracy did not improve from 0.63077\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5778 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.63077\n",
            "Epoch 55/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5761 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00055: binary_accuracy did not improve from 0.63077\n",
            "Epoch 56/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5745 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00056: binary_accuracy did not improve from 0.63077\n",
            "Epoch 57/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5729 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00057: binary_accuracy did not improve from 0.63077\n",
            "Epoch 58/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5714 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00058: binary_accuracy improved from 0.63077 to 0.64615, saving model to best.h5\n",
            "Epoch 59/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5697 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00059: binary_accuracy did not improve from 0.64615\n",
            "Epoch 60/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5681 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00060: binary_accuracy did not improve from 0.64615\n",
            "Epoch 61/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5665 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00061: binary_accuracy improved from 0.64615 to 0.66154, saving model to best.h5\n",
            "Epoch 62/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5646 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00062: binary_accuracy did not improve from 0.66154\n",
            "Epoch 63/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5631 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00063: binary_accuracy improved from 0.66154 to 0.67692, saving model to best.h5\n",
            "Epoch 64/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5616 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00064: binary_accuracy did not improve from 0.67692\n",
            "Epoch 65/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5601 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00065: binary_accuracy did not improve from 0.67692\n",
            "Epoch 66/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5586 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00066: binary_accuracy did not improve from 0.67692\n",
            "Epoch 67/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5572 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00067: binary_accuracy did not improve from 0.67692\n",
            "Epoch 68/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5556 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00068: binary_accuracy did not improve from 0.67692\n",
            "Epoch 69/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5544 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00069: binary_accuracy did not improve from 0.67692\n",
            "Epoch 70/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5527 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00070: binary_accuracy did not improve from 0.67692\n",
            "Epoch 71/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5516 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00071: binary_accuracy did not improve from 0.67692\n",
            "Epoch 72/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5501 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00072: binary_accuracy did not improve from 0.67692\n",
            "Epoch 73/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5488 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00073: binary_accuracy did not improve from 0.67692\n",
            "Epoch 74/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5477 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00074: binary_accuracy did not improve from 0.67692\n",
            "Epoch 75/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5462 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00075: binary_accuracy did not improve from 0.67692\n",
            "Epoch 76/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5451 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00076: binary_accuracy improved from 0.67692 to 0.69231, saving model to best.h5\n",
            "Epoch 77/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5437 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00077: binary_accuracy did not improve from 0.69231\n",
            "Epoch 78/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5425 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00078: binary_accuracy did not improve from 0.69231\n",
            "Epoch 79/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5413 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00079: binary_accuracy did not improve from 0.69231\n",
            "Epoch 80/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5399 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00080: binary_accuracy did not improve from 0.69231\n",
            "Epoch 81/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5389 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00081: binary_accuracy improved from 0.69231 to 0.72308, saving model to best.h5\n",
            "Epoch 82/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5375 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00082: binary_accuracy did not improve from 0.72308\n",
            "Epoch 83/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5363 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00083: binary_accuracy did not improve from 0.72308\n",
            "Epoch 84/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5351 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00084: binary_accuracy did not improve from 0.72308\n",
            "Epoch 85/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5338 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00085: binary_accuracy did not improve from 0.72308\n",
            "Epoch 86/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5326 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00086: binary_accuracy did not improve from 0.72308\n",
            "Epoch 87/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5313 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00087: binary_accuracy did not improve from 0.72308\n",
            "Epoch 88/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5301 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00088: binary_accuracy did not improve from 0.72308\n",
            "Epoch 89/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5288 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00089: binary_accuracy did not improve from 0.72308\n",
            "Epoch 90/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5278 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00090: binary_accuracy did not improve from 0.72308\n",
            "Epoch 91/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5264 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00091: binary_accuracy improved from 0.72308 to 0.73846, saving model to best.h5\n",
            "Epoch 92/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5253 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00092: binary_accuracy did not improve from 0.73846\n",
            "Epoch 93/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5242 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00093: binary_accuracy improved from 0.73846 to 0.75385, saving model to best.h5\n",
            "Epoch 94/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5232 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00094: binary_accuracy improved from 0.75385 to 0.76923, saving model to best.h5\n",
            "Epoch 95/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5221 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00095: binary_accuracy improved from 0.76923 to 0.78462, saving model to best.h5\n",
            "Epoch 96/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5212 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00096: binary_accuracy did not improve from 0.78462\n",
            "Epoch 97/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5203 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00097: binary_accuracy did not improve from 0.78462\n",
            "Epoch 98/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5194 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00098: binary_accuracy did not improve from 0.78462\n",
            "Epoch 99/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5185 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00099: binary_accuracy did not improve from 0.78462\n",
            "Epoch 100/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5176 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00100: binary_accuracy did not improve from 0.78462\n",
            "Epoch 101/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5168 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00101: binary_accuracy did not improve from 0.78462\n",
            "Epoch 102/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5158 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00102: binary_accuracy did not improve from 0.78462\n",
            "Epoch 103/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5148 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00103: binary_accuracy did not improve from 0.78462\n",
            "Epoch 104/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5137 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00104: binary_accuracy did not improve from 0.78462\n",
            "Epoch 105/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5126 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00105: binary_accuracy did not improve from 0.78462\n",
            "Epoch 106/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5118 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00106: binary_accuracy did not improve from 0.78462\n",
            "Epoch 107/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5108 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00107: binary_accuracy did not improve from 0.78462\n",
            "Epoch 108/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5099 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00108: binary_accuracy did not improve from 0.78462\n",
            "Epoch 109/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5090 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00109: binary_accuracy did not improve from 0.78462\n",
            "Epoch 110/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5081 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00110: binary_accuracy did not improve from 0.78462\n",
            "Epoch 111/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5073 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00111: binary_accuracy did not improve from 0.78462\n",
            "Epoch 112/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5065 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00112: binary_accuracy did not improve from 0.78462\n",
            "Epoch 113/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5057 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00113: binary_accuracy did not improve from 0.78462\n",
            "Epoch 114/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5050 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00114: binary_accuracy did not improve from 0.78462\n",
            "Epoch 115/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5043 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00115: binary_accuracy did not improve from 0.78462\n",
            "Epoch 116/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5036 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00116: binary_accuracy did not improve from 0.78462\n",
            "Epoch 117/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5029 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00117: binary_accuracy did not improve from 0.78462\n",
            "Epoch 118/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5022 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00118: binary_accuracy did not improve from 0.78462\n",
            "Epoch 119/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5016 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00119: binary_accuracy did not improve from 0.78462\n",
            "Epoch 120/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5010 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00120: binary_accuracy did not improve from 0.78462\n",
            "Epoch 121/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5003 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00121: binary_accuracy did not improve from 0.78462\n",
            "Epoch 122/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4998 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00122: binary_accuracy did not improve from 0.78462\n",
            "Epoch 123/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4992 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00123: binary_accuracy did not improve from 0.78462\n",
            "Epoch 124/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4986 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00124: binary_accuracy did not improve from 0.78462\n",
            "Epoch 125/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4981 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00125: binary_accuracy did not improve from 0.78462\n",
            "Epoch 126/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4976 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00126: binary_accuracy did not improve from 0.78462\n",
            "Epoch 127/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4970 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00127: binary_accuracy did not improve from 0.78462\n",
            "Epoch 128/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4965 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00128: binary_accuracy did not improve from 0.78462\n",
            "Epoch 129/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4961 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00129: binary_accuracy did not improve from 0.78462\n",
            "Epoch 130/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4956 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00130: binary_accuracy did not improve from 0.78462\n",
            "Epoch 131/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4951 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00131: binary_accuracy did not improve from 0.78462\n",
            "Epoch 132/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4947 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00132: binary_accuracy did not improve from 0.78462\n",
            "Epoch 133/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4942 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00133: binary_accuracy did not improve from 0.78462\n",
            "Epoch 134/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4938 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00134: binary_accuracy did not improve from 0.78462\n",
            "Epoch 135/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4934 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00135: binary_accuracy improved from 0.78462 to 0.80000, saving model to best.h5\n",
            "Epoch 136/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4929 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00136: binary_accuracy did not improve from 0.80000\n",
            "Epoch 137/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4925 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00137: binary_accuracy did not improve from 0.80000\n",
            "Epoch 138/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4921 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00138: binary_accuracy did not improve from 0.80000\n",
            "Epoch 139/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4917 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00139: binary_accuracy did not improve from 0.80000\n",
            "Epoch 140/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4912 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00140: binary_accuracy did not improve from 0.80000\n",
            "Epoch 141/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4909 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00141: binary_accuracy did not improve from 0.80000\n",
            "Epoch 142/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4905 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00142: binary_accuracy did not improve from 0.80000\n",
            "Epoch 143/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4901 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00143: binary_accuracy did not improve from 0.80000\n",
            "Epoch 144/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4898 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00144: binary_accuracy did not improve from 0.80000\n",
            "Epoch 145/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4895 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00145: binary_accuracy did not improve from 0.80000\n",
            "Epoch 146/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4892 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00146: binary_accuracy did not improve from 0.80000\n",
            "Epoch 147/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4888 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00147: binary_accuracy did not improve from 0.80000\n",
            "Epoch 148/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4885 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00148: binary_accuracy did not improve from 0.80000\n",
            "Epoch 149/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4883 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00149: binary_accuracy did not improve from 0.80000\n",
            "Epoch 150/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4880 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00150: binary_accuracy did not improve from 0.80000\n",
            "Epoch 151/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4877 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00151: binary_accuracy did not improve from 0.80000\n",
            "Epoch 152/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4875 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00152: binary_accuracy did not improve from 0.80000\n",
            "Epoch 153/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4872 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00153: binary_accuracy did not improve from 0.80000\n",
            "Epoch 154/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4870 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00154: binary_accuracy did not improve from 0.80000\n",
            "Epoch 155/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4868 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00155: binary_accuracy did not improve from 0.80000\n",
            "Epoch 156/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4867 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00156: binary_accuracy did not improve from 0.80000\n",
            "Epoch 157/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4864 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00157: binary_accuracy did not improve from 0.80000\n",
            "Epoch 158/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4862 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00158: binary_accuracy did not improve from 0.80000\n",
            "Epoch 159/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4861 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00159: binary_accuracy did not improve from 0.80000\n",
            "Epoch 160/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4859 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00160: binary_accuracy did not improve from 0.80000\n",
            "Epoch 161/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4857 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00161: binary_accuracy did not improve from 0.80000\n",
            "Epoch 162/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4855 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00162: binary_accuracy did not improve from 0.80000\n",
            "Epoch 163/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4853 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00163: binary_accuracy did not improve from 0.80000\n",
            "Epoch 164/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4851 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00164: binary_accuracy did not improve from 0.80000\n",
            "Epoch 165/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4849 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00165: binary_accuracy did not improve from 0.80000\n",
            "Epoch 166/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4848 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00166: binary_accuracy did not improve from 0.80000\n",
            "Epoch 167/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4847 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00167: binary_accuracy did not improve from 0.80000\n",
            "Epoch 168/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4845 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00168: binary_accuracy did not improve from 0.80000\n",
            "Epoch 169/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4844 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00169: binary_accuracy did not improve from 0.80000\n",
            "Epoch 170/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4842 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00170: binary_accuracy did not improve from 0.80000\n",
            "Epoch 171/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4840 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00171: binary_accuracy did not improve from 0.80000\n",
            "Epoch 172/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4840 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00172: binary_accuracy did not improve from 0.80000\n",
            "Epoch 173/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4837 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00173: binary_accuracy did not improve from 0.80000\n",
            "Epoch 174/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4836 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00174: binary_accuracy did not improve from 0.80000\n",
            "Epoch 175/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4835 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00175: binary_accuracy did not improve from 0.80000\n",
            "(5, 1)\n",
            "2017-02-05\n",
            "2017-02-11\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([31, 34]))\n",
            "{0: 1.096774193548387, 1: 1}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 35s 536ms/step - loss: 0.7289 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.53846, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.7237 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00002: binary_accuracy improved from 0.53846 to 0.56923, saving model to best.h5\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.7216 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00003: binary_accuracy did not improve from 0.56923\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.7195 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00004: binary_accuracy did not improve from 0.56923\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.7175 - binary_accuracy: 0.5077\n",
            "\n",
            "Epoch 00005: binary_accuracy did not improve from 0.56923\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7153 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00006: binary_accuracy did not improve from 0.56923\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7130 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00007: binary_accuracy did not improve from 0.56923\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7104 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00008: binary_accuracy did not improve from 0.56923\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7074 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00009: binary_accuracy did not improve from 0.56923\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7039 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00010: binary_accuracy did not improve from 0.56923\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6997 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.56923\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6949 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00012: binary_accuracy did not improve from 0.56923\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6892 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00013: binary_accuracy improved from 0.56923 to 0.58462, saving model to best.h5\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6830 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00014: binary_accuracy did not improve from 0.58462\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6766 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.58462\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6707 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.58462\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6650 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.58462\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6595 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.58462\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6540 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.58462\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6484 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.58462\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6429 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.58462\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6374 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00022: binary_accuracy improved from 0.58462 to 0.60000, saving model to best.h5\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6321 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.60000\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6271 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.60000\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6224 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.60000\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6180 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.60000\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6140 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.60000\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6102 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00028: binary_accuracy improved from 0.60000 to 0.61538, saving model to best.h5\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6068 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.61538\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6035 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.61538\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6005 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00031: binary_accuracy improved from 0.61538 to 0.64615, saving model to best.h5\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5976 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.64615\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5950 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.64615\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5924 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.64615\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5901 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.64615\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5879 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.64615\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5859 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.64615\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5840 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.64615\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5821 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.64615\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5805 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.64615\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5789 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00041: binary_accuracy improved from 0.64615 to 0.66154, saving model to best.h5\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5775 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.66154\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5761 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.66154\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5749 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00044: binary_accuracy improved from 0.66154 to 0.67692, saving model to best.h5\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5737 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00045: binary_accuracy improved from 0.67692 to 0.69231, saving model to best.h5\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5725 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.69231\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5714 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.69231\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5704 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.69231\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5694 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.69231\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5685 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.69231\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5676 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00051: binary_accuracy did not improve from 0.69231\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5667 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.69231\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5658 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00053: binary_accuracy did not improve from 0.69231\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5650 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.69231\n",
            "Epoch 55/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5642 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00055: binary_accuracy did not improve from 0.69231\n",
            "Epoch 56/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5634 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00056: binary_accuracy did not improve from 0.69231\n",
            "Epoch 57/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5626 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00057: binary_accuracy did not improve from 0.69231\n",
            "Epoch 58/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5618 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00058: binary_accuracy did not improve from 0.69231\n",
            "Epoch 59/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5610 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00059: binary_accuracy did not improve from 0.69231\n",
            "Epoch 60/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5602 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00060: binary_accuracy did not improve from 0.69231\n",
            "Epoch 61/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5594 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00061: binary_accuracy did not improve from 0.69231\n",
            "Epoch 62/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5587 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00062: binary_accuracy did not improve from 0.69231\n",
            "Epoch 63/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5578 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00063: binary_accuracy did not improve from 0.69231\n",
            "Epoch 64/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5571 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00064: binary_accuracy did not improve from 0.69231\n",
            "Epoch 65/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5563 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00065: binary_accuracy did not improve from 0.69231\n",
            "Epoch 66/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5555 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00066: binary_accuracy did not improve from 0.69231\n",
            "Epoch 67/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5548 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00067: binary_accuracy did not improve from 0.69231\n",
            "Epoch 68/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5540 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00068: binary_accuracy did not improve from 0.69231\n",
            "Epoch 69/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5532 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00069: binary_accuracy did not improve from 0.69231\n",
            "Epoch 70/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5525 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00070: binary_accuracy did not improve from 0.69231\n",
            "Epoch 71/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5517 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00071: binary_accuracy did not improve from 0.69231\n",
            "Epoch 72/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5510 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00072: binary_accuracy improved from 0.69231 to 0.70769, saving model to best.h5\n",
            "Epoch 73/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5503 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00073: binary_accuracy did not improve from 0.70769\n",
            "Epoch 74/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5497 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00074: binary_accuracy did not improve from 0.70769\n",
            "Epoch 75/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5490 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00075: binary_accuracy did not improve from 0.70769\n",
            "Epoch 76/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5483 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00076: binary_accuracy did not improve from 0.70769\n",
            "Epoch 77/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5476 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00077: binary_accuracy did not improve from 0.70769\n",
            "Epoch 78/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5468 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00078: binary_accuracy did not improve from 0.70769\n",
            "Epoch 79/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5461 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00079: binary_accuracy did not improve from 0.70769\n",
            "Epoch 80/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5453 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00080: binary_accuracy did not improve from 0.70769\n",
            "Epoch 81/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5446 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00081: binary_accuracy did not improve from 0.70769\n",
            "Epoch 82/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5439 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00082: binary_accuracy did not improve from 0.70769\n",
            "Epoch 83/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5431 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00083: binary_accuracy did not improve from 0.70769\n",
            "Epoch 84/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5424 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00084: binary_accuracy did not improve from 0.70769\n",
            "Epoch 85/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5417 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00085: binary_accuracy did not improve from 0.70769\n",
            "Epoch 86/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5410 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00086: binary_accuracy did not improve from 0.70769\n",
            "Epoch 87/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5403 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00087: binary_accuracy did not improve from 0.70769\n",
            "Epoch 88/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5395 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00088: binary_accuracy did not improve from 0.70769\n",
            "Epoch 89/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5388 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00089: binary_accuracy did not improve from 0.70769\n",
            "Epoch 90/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5382 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00090: binary_accuracy did not improve from 0.70769\n",
            "Epoch 91/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5375 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00091: binary_accuracy did not improve from 0.70769\n",
            "Epoch 92/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5368 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00092: binary_accuracy did not improve from 0.70769\n",
            "Epoch 93/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5362 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00093: binary_accuracy did not improve from 0.70769\n",
            "Epoch 94/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5355 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00094: binary_accuracy did not improve from 0.70769\n",
            "Epoch 95/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5349 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00095: binary_accuracy did not improve from 0.70769\n",
            "Epoch 96/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5342 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00096: binary_accuracy did not improve from 0.70769\n",
            "Epoch 97/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5336 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00097: binary_accuracy did not improve from 0.70769\n",
            "Epoch 98/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5330 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00098: binary_accuracy did not improve from 0.70769\n",
            "Epoch 99/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5325 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00099: binary_accuracy improved from 0.70769 to 0.72308, saving model to best.h5\n",
            "Epoch 100/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5319 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00100: binary_accuracy did not improve from 0.72308\n",
            "Epoch 101/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5313 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00101: binary_accuracy did not improve from 0.72308\n",
            "Epoch 102/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5308 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00102: binary_accuracy did not improve from 0.72308\n",
            "Epoch 103/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5302 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00103: binary_accuracy did not improve from 0.72308\n",
            "Epoch 104/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5296 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00104: binary_accuracy improved from 0.72308 to 0.73846, saving model to best.h5\n",
            "Epoch 105/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5291 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00105: binary_accuracy did not improve from 0.73846\n",
            "Epoch 106/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5285 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00106: binary_accuracy did not improve from 0.73846\n",
            "Epoch 107/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5280 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00107: binary_accuracy did not improve from 0.73846\n",
            "Epoch 108/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5275 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00108: binary_accuracy did not improve from 0.73846\n",
            "Epoch 109/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5270 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00109: binary_accuracy improved from 0.73846 to 0.75385, saving model to best.h5\n",
            "Epoch 110/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5265 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00110: binary_accuracy did not improve from 0.75385\n",
            "Epoch 111/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5261 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00111: binary_accuracy improved from 0.75385 to 0.76923, saving model to best.h5\n",
            "Epoch 112/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5256 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00112: binary_accuracy did not improve from 0.76923\n",
            "Epoch 113/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5251 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00113: binary_accuracy did not improve from 0.76923\n",
            "Epoch 114/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5246 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00114: binary_accuracy did not improve from 0.76923\n",
            "Epoch 115/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5241 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00115: binary_accuracy improved from 0.76923 to 0.78462, saving model to best.h5\n",
            "Epoch 116/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5237 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00116: binary_accuracy did not improve from 0.78462\n",
            "Epoch 117/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5232 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00117: binary_accuracy did not improve from 0.78462\n",
            "Epoch 118/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5227 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00118: binary_accuracy did not improve from 0.78462\n",
            "Epoch 119/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5222 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00119: binary_accuracy did not improve from 0.78462\n",
            "Epoch 120/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5218 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00120: binary_accuracy did not improve from 0.78462\n",
            "Epoch 121/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5213 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00121: binary_accuracy did not improve from 0.78462\n",
            "Epoch 122/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5208 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00122: binary_accuracy did not improve from 0.78462\n",
            "Epoch 123/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5203 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00123: binary_accuracy did not improve from 0.78462\n",
            "Epoch 124/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5198 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00124: binary_accuracy did not improve from 0.78462\n",
            "Epoch 125/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5193 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00125: binary_accuracy did not improve from 0.78462\n",
            "Epoch 126/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5188 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00126: binary_accuracy did not improve from 0.78462\n",
            "Epoch 127/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5183 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00127: binary_accuracy did not improve from 0.78462\n",
            "Epoch 128/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5178 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00128: binary_accuracy did not improve from 0.78462\n",
            "Epoch 129/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5174 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00129: binary_accuracy did not improve from 0.78462\n",
            "Epoch 130/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5169 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00130: binary_accuracy did not improve from 0.78462\n",
            "Epoch 131/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5165 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00131: binary_accuracy did not improve from 0.78462\n",
            "Epoch 132/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5161 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00132: binary_accuracy did not improve from 0.78462\n",
            "Epoch 133/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5157 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00133: binary_accuracy did not improve from 0.78462\n",
            "Epoch 134/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5152 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00134: binary_accuracy did not improve from 0.78462\n",
            "Epoch 135/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5149 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00135: binary_accuracy did not improve from 0.78462\n",
            "Epoch 136/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5144 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00136: binary_accuracy did not improve from 0.78462\n",
            "Epoch 137/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5141 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00137: binary_accuracy did not improve from 0.78462\n",
            "Epoch 138/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5137 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00138: binary_accuracy did not improve from 0.78462\n",
            "Epoch 139/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5133 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00139: binary_accuracy did not improve from 0.78462\n",
            "Epoch 140/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5129 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00140: binary_accuracy did not improve from 0.78462\n",
            "Epoch 141/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5126 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00141: binary_accuracy did not improve from 0.78462\n",
            "Epoch 142/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5121 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00142: binary_accuracy did not improve from 0.78462\n",
            "Epoch 143/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5118 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00143: binary_accuracy did not improve from 0.78462\n",
            "Epoch 144/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5114 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00144: binary_accuracy did not improve from 0.78462\n",
            "Epoch 145/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5111 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00145: binary_accuracy did not improve from 0.78462\n",
            "Epoch 146/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5107 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00146: binary_accuracy did not improve from 0.78462\n",
            "Epoch 147/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5103 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00147: binary_accuracy did not improve from 0.78462\n",
            "Epoch 148/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5100 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00148: binary_accuracy did not improve from 0.78462\n",
            "Epoch 149/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5097 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00149: binary_accuracy did not improve from 0.78462\n",
            "Epoch 150/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5093 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00150: binary_accuracy did not improve from 0.78462\n",
            "Epoch 151/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5089 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00151: binary_accuracy did not improve from 0.78462\n",
            "Epoch 152/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5086 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00152: binary_accuracy did not improve from 0.78462\n",
            "Epoch 153/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5083 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00153: binary_accuracy did not improve from 0.78462\n",
            "Epoch 154/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5079 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00154: binary_accuracy did not improve from 0.78462\n",
            "Epoch 155/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5076 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00155: binary_accuracy did not improve from 0.78462\n",
            "(5, 1)\n",
            "2017-02-05\n",
            "2017-02-11\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([31, 34]))\n",
            "{0: 1.096774193548387, 1: 1}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 34s 527ms/step - loss: 0.7294 - binary_accuracy: 0.4154\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.41538, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7247 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00002: binary_accuracy improved from 0.41538 to 0.52308, saving model to best.h5\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7224 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00003: binary_accuracy improved from 0.52308 to 0.60000, saving model to best.h5\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7202 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00004: binary_accuracy did not improve from 0.60000\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7180 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00005: binary_accuracy did not improve from 0.60000\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7156 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00006: binary_accuracy did not improve from 0.60000\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7130 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00007: binary_accuracy did not improve from 0.60000\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7101 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00008: binary_accuracy did not improve from 0.60000\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7069 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00009: binary_accuracy did not improve from 0.60000\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7033 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00010: binary_accuracy did not improve from 0.60000\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6990 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.60000\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6941 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00012: binary_accuracy did not improve from 0.60000\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6885 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00013: binary_accuracy did not improve from 0.60000\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6821 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00014: binary_accuracy did not improve from 0.60000\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6752 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.60000\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6684 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.60000\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6621 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.60000\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6564 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.60000\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6511 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.60000\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6460 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.60000\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6410 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.60000\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6360 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.60000\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6314 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.60000\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6270 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.60000\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6228 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.60000\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6187 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.60000\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6149 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.60000\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6112 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00028: binary_accuracy improved from 0.60000 to 0.61538, saving model to best.h5\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6077 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.61538\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6044 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.61538\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6012 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.61538\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5980 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00032: binary_accuracy improved from 0.61538 to 0.63077, saving model to best.h5\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5951 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.63077\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5923 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.63077\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5896 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00035: binary_accuracy improved from 0.63077 to 0.64615, saving model to best.h5\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5872 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.64615\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5850 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.64615\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5829 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.64615\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5811 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00039: binary_accuracy improved from 0.64615 to 0.67692, saving model to best.h5\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5794 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.67692\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5779 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.67692\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5765 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00042: binary_accuracy improved from 0.67692 to 0.69231, saving model to best.h5\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5753 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.69231\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5741 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00044: binary_accuracy improved from 0.69231 to 0.70769, saving model to best.h5\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5731 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.70769\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5720 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.70769\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5711 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00047: binary_accuracy improved from 0.70769 to 0.72308, saving model to best.h5\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5701 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.72308\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5692 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.72308\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5684 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.72308\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5675 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00051: binary_accuracy did not improve from 0.72308\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5667 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.72308\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5659 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00053: binary_accuracy did not improve from 0.72308\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5651 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.72308\n",
            "Epoch 55/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5643 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00055: binary_accuracy did not improve from 0.72308\n",
            "Epoch 56/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5635 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00056: binary_accuracy did not improve from 0.72308\n",
            "Epoch 57/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5628 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00057: binary_accuracy did not improve from 0.72308\n",
            "Epoch 58/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5621 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00058: binary_accuracy did not improve from 0.72308\n",
            "Epoch 59/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5614 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00059: binary_accuracy did not improve from 0.72308\n",
            "Epoch 60/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5607 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00060: binary_accuracy did not improve from 0.72308\n",
            "Epoch 61/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5601 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00061: binary_accuracy did not improve from 0.72308\n",
            "Epoch 62/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5596 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00062: binary_accuracy did not improve from 0.72308\n",
            "Epoch 63/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5591 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00063: binary_accuracy did not improve from 0.72308\n",
            "Epoch 64/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5586 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00064: binary_accuracy did not improve from 0.72308\n",
            "Epoch 65/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5582 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00065: binary_accuracy did not improve from 0.72308\n",
            "Epoch 66/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5577 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00066: binary_accuracy did not improve from 0.72308\n",
            "Epoch 67/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5573 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00067: binary_accuracy did not improve from 0.72308\n",
            "Epoch 68/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5568 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00068: binary_accuracy did not improve from 0.72308\n",
            "Epoch 69/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5563 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00069: binary_accuracy did not improve from 0.72308\n",
            "Epoch 70/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5557 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00070: binary_accuracy did not improve from 0.72308\n",
            "Epoch 71/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5552 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00071: binary_accuracy did not improve from 0.72308\n",
            "Epoch 72/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5547 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00072: binary_accuracy did not improve from 0.72308\n",
            "Epoch 73/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5541 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00073: binary_accuracy did not improve from 0.72308\n",
            "Epoch 74/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5536 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00074: binary_accuracy did not improve from 0.72308\n",
            "Epoch 75/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5531 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00075: binary_accuracy did not improve from 0.72308\n",
            "Epoch 76/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5526 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00076: binary_accuracy did not improve from 0.72308\n",
            "Epoch 77/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5521 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00077: binary_accuracy did not improve from 0.72308\n",
            "Epoch 78/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5516 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00078: binary_accuracy improved from 0.72308 to 0.73846, saving model to best.h5\n",
            "Epoch 79/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5511 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00079: binary_accuracy did not improve from 0.73846\n",
            "Epoch 80/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5507 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00080: binary_accuracy did not improve from 0.73846\n",
            "Epoch 81/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.5502 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00081: binary_accuracy did not improve from 0.73846\n",
            "Epoch 82/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5498 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00082: binary_accuracy did not improve from 0.73846\n",
            "Epoch 83/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5493 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00083: binary_accuracy did not improve from 0.73846\n",
            "Epoch 84/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5489 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00084: binary_accuracy did not improve from 0.73846\n",
            "Epoch 85/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5485 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00085: binary_accuracy did not improve from 0.73846\n",
            "Epoch 86/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5481 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00086: binary_accuracy improved from 0.73846 to 0.75385, saving model to best.h5\n",
            "Epoch 87/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5476 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00087: binary_accuracy did not improve from 0.75385\n",
            "Epoch 88/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5473 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00088: binary_accuracy did not improve from 0.75385\n",
            "Epoch 89/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5469 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00089: binary_accuracy did not improve from 0.75385\n",
            "Epoch 90/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5465 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00090: binary_accuracy did not improve from 0.75385\n",
            "Epoch 91/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5461 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00091: binary_accuracy did not improve from 0.75385\n",
            "Epoch 92/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5457 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00092: binary_accuracy did not improve from 0.75385\n",
            "Epoch 93/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5453 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00093: binary_accuracy did not improve from 0.75385\n",
            "Epoch 94/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5449 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00094: binary_accuracy did not improve from 0.75385\n",
            "Epoch 95/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5446 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00095: binary_accuracy did not improve from 0.75385\n",
            "Epoch 96/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5442 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00096: binary_accuracy did not improve from 0.75385\n",
            "Epoch 97/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5438 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00097: binary_accuracy did not improve from 0.75385\n",
            "Epoch 98/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5434 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00098: binary_accuracy did not improve from 0.75385\n",
            "Epoch 99/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5430 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00099: binary_accuracy did not improve from 0.75385\n",
            "Epoch 100/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5426 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00100: binary_accuracy did not improve from 0.75385\n",
            "Epoch 101/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5422 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00101: binary_accuracy did not improve from 0.75385\n",
            "Epoch 102/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5418 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00102: binary_accuracy did not improve from 0.75385\n",
            "Epoch 103/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5415 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00103: binary_accuracy did not improve from 0.75385\n",
            "Epoch 104/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5411 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00104: binary_accuracy did not improve from 0.75385\n",
            "Epoch 105/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5407 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00105: binary_accuracy improved from 0.75385 to 0.76923, saving model to best.h5\n",
            "Epoch 106/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5404 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00106: binary_accuracy did not improve from 0.76923\n",
            "Epoch 107/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5400 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00107: binary_accuracy did not improve from 0.76923\n",
            "Epoch 108/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5396 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00108: binary_accuracy did not improve from 0.76923\n",
            "Epoch 109/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5393 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00109: binary_accuracy did not improve from 0.76923\n",
            "Epoch 110/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5389 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00110: binary_accuracy did not improve from 0.76923\n",
            "Epoch 111/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5385 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00111: binary_accuracy did not improve from 0.76923\n",
            "Epoch 112/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5381 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00112: binary_accuracy did not improve from 0.76923\n",
            "Epoch 113/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5377 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00113: binary_accuracy did not improve from 0.76923\n",
            "Epoch 114/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5374 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00114: binary_accuracy did not improve from 0.76923\n",
            "Epoch 115/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5369 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00115: binary_accuracy did not improve from 0.76923\n",
            "Epoch 116/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5365 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00116: binary_accuracy did not improve from 0.76923\n",
            "Epoch 117/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5361 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00117: binary_accuracy did not improve from 0.76923\n",
            "Epoch 118/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5357 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00118: binary_accuracy did not improve from 0.76923\n",
            "Epoch 119/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5353 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00119: binary_accuracy did not improve from 0.76923\n",
            "Epoch 120/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5349 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00120: binary_accuracy did not improve from 0.76923\n",
            "Epoch 121/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5345 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00121: binary_accuracy did not improve from 0.76923\n",
            "Epoch 122/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5341 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00122: binary_accuracy did not improve from 0.76923\n",
            "Epoch 123/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5337 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00123: binary_accuracy did not improve from 0.76923\n",
            "Epoch 124/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5332 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00124: binary_accuracy did not improve from 0.76923\n",
            "Epoch 125/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5328 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00125: binary_accuracy did not improve from 0.76923\n",
            "Epoch 126/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5324 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00126: binary_accuracy did not improve from 0.76923\n",
            "Epoch 127/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5319 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00127: binary_accuracy did not improve from 0.76923\n",
            "Epoch 128/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5315 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00128: binary_accuracy did not improve from 0.76923\n",
            "Epoch 129/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5311 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00129: binary_accuracy did not improve from 0.76923\n",
            "Epoch 130/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5307 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00130: binary_accuracy did not improve from 0.76923\n",
            "Epoch 131/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5302 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00131: binary_accuracy did not improve from 0.76923\n",
            "Epoch 132/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5298 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00132: binary_accuracy did not improve from 0.76923\n",
            "Epoch 133/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5294 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00133: binary_accuracy did not improve from 0.76923\n",
            "Epoch 134/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5291 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00134: binary_accuracy did not improve from 0.76923\n",
            "Epoch 135/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5286 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00135: binary_accuracy did not improve from 0.76923\n",
            "Epoch 136/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5282 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00136: binary_accuracy did not improve from 0.76923\n",
            "Epoch 137/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5278 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00137: binary_accuracy did not improve from 0.76923\n",
            "Epoch 138/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5273 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00138: binary_accuracy did not improve from 0.76923\n",
            "Epoch 139/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5270 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00139: binary_accuracy did not improve from 0.76923\n",
            "Epoch 140/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5266 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00140: binary_accuracy did not improve from 0.76923\n",
            "Epoch 141/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5262 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00141: binary_accuracy did not improve from 0.76923\n",
            "Epoch 142/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5259 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00142: binary_accuracy did not improve from 0.76923\n",
            "Epoch 143/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5255 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00143: binary_accuracy did not improve from 0.76923\n",
            "Epoch 144/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5251 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00144: binary_accuracy did not improve from 0.76923\n",
            "Epoch 145/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5248 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00145: binary_accuracy did not improve from 0.76923\n",
            "(5, 1)\n",
            "2017-02-12\n",
            "2017-02-18\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([31, 34]))\n",
            "{0: 1.096774193548387, 1: 1}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 38s 584ms/step - loss: 0.7334 - binary_accuracy: 0.4154\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.41538, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7287 - binary_accuracy: 0.4769\n",
            "\n",
            "Epoch 00002: binary_accuracy improved from 0.41538 to 0.47692, saving model to best.h5\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7258 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00003: binary_accuracy improved from 0.47692 to 0.53846, saving model to best.h5\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7231 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00004: binary_accuracy did not improve from 0.53846\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7205 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00005: binary_accuracy did not improve from 0.53846\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7178 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00006: binary_accuracy improved from 0.53846 to 0.55385, saving model to best.h5\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7148 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00007: binary_accuracy did not improve from 0.55385\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7113 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00008: binary_accuracy improved from 0.55385 to 0.56923, saving model to best.h5\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.7072 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00009: binary_accuracy improved from 0.56923 to 0.60000, saving model to best.h5\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7024 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00010: binary_accuracy did not improve from 0.60000\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6970 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00011: binary_accuracy improved from 0.60000 to 0.61538, saving model to best.h5\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6909 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00012: binary_accuracy improved from 0.61538 to 0.63077, saving model to best.h5\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6843 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00013: binary_accuracy did not improve from 0.63077\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6775 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00014: binary_accuracy did not improve from 0.63077\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6706 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00015: binary_accuracy improved from 0.63077 to 0.66154, saving model to best.h5\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6638 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.66154\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6574 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00017: binary_accuracy improved from 0.66154 to 0.69231, saving model to best.h5\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6515 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.69231\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6462 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.69231\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6415 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.69231\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6374 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.69231\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6337 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.69231\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6304 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.69231\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6274 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.69231\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6246 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.69231\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6220 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.69231\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6196 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00027: binary_accuracy improved from 0.69231 to 0.70769, saving model to best.h5\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6174 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.70769\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6152 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.70769\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6131 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.70769\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6111 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.70769\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6092 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.70769\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6073 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.70769\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6055 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.70769\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6037 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.70769\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6020 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.70769\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6004 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.70769\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5988 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.70769\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5972 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.70769\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5957 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.70769\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5942 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.70769\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5927 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.70769\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5912 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.70769\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5898 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.70769\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5884 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.70769\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5870 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.70769\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5856 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.70769\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5842 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.70769\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5828 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.70769\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5814 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.70769\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5800 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00051: binary_accuracy did not improve from 0.70769\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5785 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.70769\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5771 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00053: binary_accuracy improved from 0.70769 to 0.72308, saving model to best.h5\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5757 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.72308\n",
            "Epoch 55/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5742 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00055: binary_accuracy did not improve from 0.72308\n",
            "Epoch 56/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5727 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00056: binary_accuracy did not improve from 0.72308\n",
            "Epoch 57/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5712 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00057: binary_accuracy did not improve from 0.72308\n",
            "Epoch 58/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5697 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00058: binary_accuracy did not improve from 0.72308\n",
            "Epoch 59/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5681 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00059: binary_accuracy did not improve from 0.72308\n",
            "Epoch 60/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5665 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00060: binary_accuracy did not improve from 0.72308\n",
            "Epoch 61/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5648 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00061: binary_accuracy did not improve from 0.72308\n",
            "Epoch 62/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5632 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00062: binary_accuracy did not improve from 0.72308\n",
            "Epoch 63/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5614 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00063: binary_accuracy did not improve from 0.72308\n",
            "Epoch 64/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5597 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00064: binary_accuracy did not improve from 0.72308\n",
            "Epoch 65/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5579 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00065: binary_accuracy did not improve from 0.72308\n",
            "Epoch 66/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5562 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00066: binary_accuracy improved from 0.72308 to 0.73846, saving model to best.h5\n",
            "Epoch 67/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5545 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00067: binary_accuracy did not improve from 0.73846\n",
            "Epoch 68/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5526 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00068: binary_accuracy did not improve from 0.73846\n",
            "Epoch 69/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5509 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00069: binary_accuracy did not improve from 0.73846\n",
            "Epoch 70/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5492 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00070: binary_accuracy did not improve from 0.73846\n",
            "Epoch 71/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5475 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00071: binary_accuracy did not improve from 0.73846\n",
            "Epoch 72/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5459 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00072: binary_accuracy did not improve from 0.73846\n",
            "Epoch 73/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5443 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00073: binary_accuracy did not improve from 0.73846\n",
            "Epoch 74/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5427 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00074: binary_accuracy did not improve from 0.73846\n",
            "Epoch 75/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5412 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00075: binary_accuracy improved from 0.73846 to 0.75385, saving model to best.h5\n",
            "Epoch 76/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5395 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00076: binary_accuracy did not improve from 0.75385\n",
            "Epoch 77/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5380 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00077: binary_accuracy did not improve from 0.75385\n",
            "Epoch 78/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5364 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00078: binary_accuracy did not improve from 0.75385\n",
            "Epoch 79/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5348 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00079: binary_accuracy did not improve from 0.75385\n",
            "Epoch 80/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5333 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00080: binary_accuracy did not improve from 0.75385\n",
            "Epoch 81/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5318 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00081: binary_accuracy did not improve from 0.75385\n",
            "Epoch 82/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5302 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00082: binary_accuracy did not improve from 0.75385\n",
            "Epoch 83/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5287 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00083: binary_accuracy did not improve from 0.75385\n",
            "Epoch 84/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5272 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00084: binary_accuracy did not improve from 0.75385\n",
            "Epoch 85/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5257 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00085: binary_accuracy did not improve from 0.75385\n",
            "Epoch 86/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5244 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00086: binary_accuracy did not improve from 0.75385\n",
            "Epoch 87/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5231 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00087: binary_accuracy did not improve from 0.75385\n",
            "Epoch 88/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5217 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00088: binary_accuracy did not improve from 0.75385\n",
            "Epoch 89/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5205 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00089: binary_accuracy did not improve from 0.75385\n",
            "Epoch 90/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5191 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00090: binary_accuracy did not improve from 0.75385\n",
            "Epoch 91/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5179 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00091: binary_accuracy did not improve from 0.75385\n",
            "Epoch 92/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5167 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00092: binary_accuracy did not improve from 0.75385\n",
            "Epoch 93/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5155 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00093: binary_accuracy did not improve from 0.75385\n",
            "Epoch 94/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5141 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00094: binary_accuracy did not improve from 0.75385\n",
            "Epoch 95/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5130 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00095: binary_accuracy did not improve from 0.75385\n",
            "Epoch 96/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5118 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00096: binary_accuracy did not improve from 0.75385\n",
            "Epoch 97/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5106 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00097: binary_accuracy did not improve from 0.75385\n",
            "Epoch 98/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5095 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00098: binary_accuracy did not improve from 0.75385\n",
            "Epoch 99/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5083 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00099: binary_accuracy did not improve from 0.75385\n",
            "Epoch 100/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5072 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00100: binary_accuracy did not improve from 0.75385\n",
            "Epoch 101/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5060 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00101: binary_accuracy did not improve from 0.75385\n",
            "Epoch 102/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5050 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00102: binary_accuracy did not improve from 0.75385\n",
            "Epoch 103/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5040 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00103: binary_accuracy improved from 0.75385 to 0.76923, saving model to best.h5\n",
            "Epoch 104/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5027 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00104: binary_accuracy did not improve from 0.76923\n",
            "Epoch 105/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5018 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00105: binary_accuracy did not improve from 0.76923\n",
            "Epoch 106/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5006 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00106: binary_accuracy did not improve from 0.76923\n",
            "Epoch 107/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4995 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00107: binary_accuracy did not improve from 0.76923\n",
            "Epoch 108/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4984 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00108: binary_accuracy did not improve from 0.76923\n",
            "Epoch 109/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4975 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00109: binary_accuracy did not improve from 0.76923\n",
            "Epoch 110/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4963 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00110: binary_accuracy did not improve from 0.76923\n",
            "Epoch 111/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4953 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00111: binary_accuracy did not improve from 0.76923\n",
            "Epoch 112/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4943 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00112: binary_accuracy did not improve from 0.76923\n",
            "Epoch 113/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4932 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00113: binary_accuracy did not improve from 0.76923\n",
            "Epoch 114/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4921 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00114: binary_accuracy did not improve from 0.76923\n",
            "Epoch 115/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4911 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00115: binary_accuracy did not improve from 0.76923\n",
            "Epoch 116/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4900 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00116: binary_accuracy did not improve from 0.76923\n",
            "Epoch 117/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4890 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00117: binary_accuracy improved from 0.76923 to 0.78462, saving model to best.h5\n",
            "Epoch 118/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4879 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00118: binary_accuracy did not improve from 0.78462\n",
            "Epoch 119/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4869 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00119: binary_accuracy did not improve from 0.78462\n",
            "Epoch 120/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4859 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00120: binary_accuracy did not improve from 0.78462\n",
            "Epoch 121/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4849 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00121: binary_accuracy did not improve from 0.78462\n",
            "Epoch 122/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4839 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00122: binary_accuracy did not improve from 0.78462\n",
            "Epoch 123/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4828 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00123: binary_accuracy did not improve from 0.78462\n",
            "Epoch 124/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4819 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00124: binary_accuracy did not improve from 0.78462\n",
            "Epoch 125/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4810 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00125: binary_accuracy did not improve from 0.78462\n",
            "Epoch 126/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4800 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00126: binary_accuracy did not improve from 0.78462\n",
            "Epoch 127/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4791 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00127: binary_accuracy did not improve from 0.78462\n",
            "Epoch 128/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4781 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00128: binary_accuracy did not improve from 0.78462\n",
            "Epoch 129/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4772 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00129: binary_accuracy did not improve from 0.78462\n",
            "Epoch 130/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4762 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00130: binary_accuracy did not improve from 0.78462\n",
            "Epoch 131/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4753 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00131: binary_accuracy did not improve from 0.78462\n",
            "Epoch 132/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4744 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00132: binary_accuracy did not improve from 0.78462\n",
            "Epoch 133/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4735 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00133: binary_accuracy did not improve from 0.78462\n",
            "Epoch 134/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4726 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00134: binary_accuracy did not improve from 0.78462\n",
            "Epoch 135/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4717 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00135: binary_accuracy did not improve from 0.78462\n",
            "Epoch 136/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4709 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00136: binary_accuracy did not improve from 0.78462\n",
            "Epoch 137/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4700 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00137: binary_accuracy did not improve from 0.78462\n",
            "Epoch 138/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4691 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00138: binary_accuracy did not improve from 0.78462\n",
            "Epoch 139/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4683 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00139: binary_accuracy did not improve from 0.78462\n",
            "Epoch 140/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4676 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00140: binary_accuracy did not improve from 0.78462\n",
            "Epoch 141/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4668 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00141: binary_accuracy did not improve from 0.78462\n",
            "Epoch 142/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4661 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00142: binary_accuracy did not improve from 0.78462\n",
            "Epoch 143/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4653 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00143: binary_accuracy did not improve from 0.78462\n",
            "Epoch 144/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4646 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00144: binary_accuracy did not improve from 0.78462\n",
            "Epoch 145/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4639 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00145: binary_accuracy did not improve from 0.78462\n",
            "Epoch 146/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4633 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00146: binary_accuracy did not improve from 0.78462\n",
            "Epoch 147/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4625 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00147: binary_accuracy did not improve from 0.78462\n",
            "Epoch 148/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4620 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00148: binary_accuracy did not improve from 0.78462\n",
            "Epoch 149/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4613 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00149: binary_accuracy did not improve from 0.78462\n",
            "Epoch 150/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4605 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00150: binary_accuracy did not improve from 0.78462\n",
            "Epoch 151/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4600 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00151: binary_accuracy did not improve from 0.78462\n",
            "Epoch 152/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4592 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00152: binary_accuracy did not improve from 0.78462\n",
            "Epoch 153/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4590 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00153: binary_accuracy did not improve from 0.78462\n",
            "Epoch 154/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4579 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00154: binary_accuracy did not improve from 0.78462\n",
            "Epoch 155/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4581 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00155: binary_accuracy did not improve from 0.78462\n",
            "Epoch 156/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4564 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00156: binary_accuracy did not improve from 0.78462\n",
            "Epoch 157/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4576 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00157: binary_accuracy did not improve from 0.78462\n",
            "(5, 1)\n",
            "2017-02-12\n",
            "2017-02-18\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([31, 34]))\n",
            "{0: 1.096774193548387, 1: 1}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 38s 590ms/step - loss: 0.7310 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.52308, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.7252 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00002: binary_accuracy improved from 0.52308 to 0.55385, saving model to best.h5\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.7215 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00003: binary_accuracy did not improve from 0.55385\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7181 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00004: binary_accuracy did not improve from 0.55385\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7146 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00005: binary_accuracy did not improve from 0.55385\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7110 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00006: binary_accuracy improved from 0.55385 to 0.56923, saving model to best.h5\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7071 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00007: binary_accuracy did not improve from 0.56923\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7028 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00008: binary_accuracy improved from 0.56923 to 0.58462, saving model to best.h5\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6979 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00009: binary_accuracy did not improve from 0.58462\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6924 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00010: binary_accuracy did not improve from 0.58462\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6864 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.58462\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6800 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00012: binary_accuracy improved from 0.58462 to 0.60000, saving model to best.h5\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6734 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00013: binary_accuracy improved from 0.60000 to 0.61538, saving model to best.h5\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6669 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00014: binary_accuracy improved from 0.61538 to 0.63077, saving model to best.h5\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6608 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.63077\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6553 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00016: binary_accuracy improved from 0.63077 to 0.64615, saving model to best.h5\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6505 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.64615\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6464 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00018: binary_accuracy improved from 0.64615 to 0.66154, saving model to best.h5\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6428 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.66154\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6398 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.66154\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6370 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.66154\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6345 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.66154\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6322 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.66154\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6300 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.66154\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6280 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00025: binary_accuracy improved from 0.66154 to 0.67692, saving model to best.h5\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6260 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.67692\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6240 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00027: binary_accuracy improved from 0.67692 to 0.69231, saving model to best.h5\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6222 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.69231\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6203 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.69231\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6185 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.69231\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6167 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.69231\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6150 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.69231\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6132 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.69231\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6114 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.69231\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6096 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.69231\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6078 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.69231\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6060 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.69231\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6042 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.69231\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6024 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00039: binary_accuracy improved from 0.69231 to 0.70769, saving model to best.h5\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6006 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.70769\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5987 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.70769\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5969 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.70769\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5951 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.70769\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5932 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.70769\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5913 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.70769\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5894 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.70769\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5875 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.70769\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5857 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.70769\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5837 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.70769\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5818 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.70769\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5798 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00051: binary_accuracy did not improve from 0.70769\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5778 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.70769\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5758 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00053: binary_accuracy did not improve from 0.70769\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5738 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.70769\n",
            "Epoch 55/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5718 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00055: binary_accuracy did not improve from 0.70769\n",
            "Epoch 56/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5698 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00056: binary_accuracy did not improve from 0.70769\n",
            "Epoch 57/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5677 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00057: binary_accuracy improved from 0.70769 to 0.72308, saving model to best.h5\n",
            "Epoch 58/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5656 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00058: binary_accuracy did not improve from 0.72308\n",
            "Epoch 59/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5635 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00059: binary_accuracy did not improve from 0.72308\n",
            "Epoch 60/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5614 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00060: binary_accuracy did not improve from 0.72308\n",
            "Epoch 61/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5592 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00061: binary_accuracy did not improve from 0.72308\n",
            "Epoch 62/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5570 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00062: binary_accuracy did not improve from 0.72308\n",
            "Epoch 63/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5547 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00063: binary_accuracy did not improve from 0.72308\n",
            "Epoch 64/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5523 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00064: binary_accuracy did not improve from 0.72308\n",
            "Epoch 65/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5498 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00065: binary_accuracy did not improve from 0.72308\n",
            "Epoch 66/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5474 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00066: binary_accuracy did not improve from 0.72308\n",
            "Epoch 67/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5450 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00067: binary_accuracy did not improve from 0.72308\n",
            "Epoch 68/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5424 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00068: binary_accuracy did not improve from 0.72308\n",
            "Epoch 69/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5398 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00069: binary_accuracy did not improve from 0.72308\n",
            "Epoch 70/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5371 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00070: binary_accuracy improved from 0.72308 to 0.73846, saving model to best.h5\n",
            "Epoch 71/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5346 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00071: binary_accuracy improved from 0.73846 to 0.75385, saving model to best.h5\n",
            "Epoch 72/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5320 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00072: binary_accuracy did not improve from 0.75385\n",
            "Epoch 73/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5295 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00073: binary_accuracy did not improve from 0.75385\n",
            "Epoch 74/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5271 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00074: binary_accuracy did not improve from 0.75385\n",
            "Epoch 75/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5249 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00075: binary_accuracy did not improve from 0.75385\n",
            "Epoch 76/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5228 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00076: binary_accuracy did not improve from 0.75385\n",
            "Epoch 77/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5216 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00077: binary_accuracy did not improve from 0.75385\n",
            "Epoch 78/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5208 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00078: binary_accuracy did not improve from 0.75385\n",
            "Epoch 79/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5205 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00079: binary_accuracy improved from 0.75385 to 0.76923, saving model to best.h5\n",
            "Epoch 80/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5210 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00080: binary_accuracy did not improve from 0.76923\n",
            "Epoch 81/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5208 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00081: binary_accuracy did not improve from 0.76923\n",
            "Epoch 82/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5202 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00082: binary_accuracy did not improve from 0.76923\n",
            "Epoch 83/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5195 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00083: binary_accuracy did not improve from 0.76923\n",
            "Epoch 84/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5186 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00084: binary_accuracy did not improve from 0.76923\n",
            "Epoch 85/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5175 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00085: binary_accuracy improved from 0.76923 to 0.78462, saving model to best.h5\n",
            "Epoch 86/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5167 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00086: binary_accuracy did not improve from 0.78462\n",
            "Epoch 87/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5157 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00087: binary_accuracy did not improve from 0.78462\n",
            "Epoch 88/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5149 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00088: binary_accuracy did not improve from 0.78462\n",
            "Epoch 89/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5138 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00089: binary_accuracy did not improve from 0.78462\n",
            "Epoch 90/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5130 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00090: binary_accuracy did not improve from 0.78462\n",
            "Epoch 91/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5120 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00091: binary_accuracy did not improve from 0.78462\n",
            "Epoch 92/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5113 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00092: binary_accuracy did not improve from 0.78462\n",
            "Epoch 93/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5107 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00093: binary_accuracy did not improve from 0.78462\n",
            "Epoch 94/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5097 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00094: binary_accuracy did not improve from 0.78462\n",
            "Epoch 95/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5091 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00095: binary_accuracy did not improve from 0.78462\n",
            "Epoch 96/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5079 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00096: binary_accuracy did not improve from 0.78462\n",
            "Epoch 97/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5076 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00097: binary_accuracy did not improve from 0.78462\n",
            "Epoch 98/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5065 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00098: binary_accuracy did not improve from 0.78462\n",
            "Epoch 99/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5060 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00099: binary_accuracy did not improve from 0.78462\n",
            "Epoch 100/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5051 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00100: binary_accuracy did not improve from 0.78462\n",
            "Epoch 101/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5042 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00101: binary_accuracy did not improve from 0.78462\n",
            "Epoch 102/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5036 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00102: binary_accuracy did not improve from 0.78462\n",
            "Epoch 103/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5025 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00103: binary_accuracy did not improve from 0.78462\n",
            "Epoch 104/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5020 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00104: binary_accuracy did not improve from 0.78462\n",
            "Epoch 105/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5008 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00105: binary_accuracy did not improve from 0.78462\n",
            "Epoch 106/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5006 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00106: binary_accuracy did not improve from 0.78462\n",
            "Epoch 107/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4992 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00107: binary_accuracy did not improve from 0.78462\n",
            "Epoch 108/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4991 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00108: binary_accuracy did not improve from 0.78462\n",
            "Epoch 109/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4981 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00109: binary_accuracy did not improve from 0.78462\n",
            "Epoch 110/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4980 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00110: binary_accuracy did not improve from 0.78462\n",
            "Epoch 111/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4969 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00111: binary_accuracy did not improve from 0.78462\n",
            "Epoch 112/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4968 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00112: binary_accuracy did not improve from 0.78462\n",
            "Epoch 113/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4958 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00113: binary_accuracy did not improve from 0.78462\n",
            "Epoch 114/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4956 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00114: binary_accuracy did not improve from 0.78462\n",
            "Epoch 115/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4948 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00115: binary_accuracy did not improve from 0.78462\n",
            "Epoch 116/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4943 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00116: binary_accuracy did not improve from 0.78462\n",
            "Epoch 117/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4937 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00117: binary_accuracy did not improve from 0.78462\n",
            "Epoch 118/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4932 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00118: binary_accuracy did not improve from 0.78462\n",
            "Epoch 119/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4925 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00119: binary_accuracy did not improve from 0.78462\n",
            "Epoch 120/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4922 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00120: binary_accuracy did not improve from 0.78462\n",
            "Epoch 121/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4915 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00121: binary_accuracy did not improve from 0.78462\n",
            "Epoch 122/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4911 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00122: binary_accuracy did not improve from 0.78462\n",
            "Epoch 123/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4905 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00123: binary_accuracy did not improve from 0.78462\n",
            "Epoch 124/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4901 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00124: binary_accuracy did not improve from 0.78462\n",
            "Epoch 125/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4894 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00125: binary_accuracy did not improve from 0.78462\n",
            "(5, 1)\n",
            "2017-02-12\n",
            "2017-02-18\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([31, 34]))\n",
            "{0: 1.096774193548387, 1: 1}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 35s 536ms/step - loss: 0.7290 - binary_accuracy: 0.4462\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.44615, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.7246 - binary_accuracy: 0.4769\n",
            "\n",
            "Epoch 00002: binary_accuracy improved from 0.44615 to 0.47692, saving model to best.h5\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7220 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00003: binary_accuracy improved from 0.47692 to 0.49231, saving model to best.h5\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7194 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00004: binary_accuracy did not improve from 0.49231\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7165 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00005: binary_accuracy improved from 0.49231 to 0.52308, saving model to best.h5\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7133 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00006: binary_accuracy improved from 0.52308 to 0.53846, saving model to best.h5\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7097 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00007: binary_accuracy improved from 0.53846 to 0.55385, saving model to best.h5\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7056 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00008: binary_accuracy improved from 0.55385 to 0.56923, saving model to best.h5\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7010 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00009: binary_accuracy did not improve from 0.56923\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6959 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00010: binary_accuracy improved from 0.56923 to 0.58462, saving model to best.h5\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6905 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.58462\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6849 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00012: binary_accuracy did not improve from 0.58462\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6793 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00013: binary_accuracy did not improve from 0.58462\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6738 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00014: binary_accuracy improved from 0.58462 to 0.60000, saving model to best.h5\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6684 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.60000\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6634 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00016: binary_accuracy improved from 0.60000 to 0.61538, saving model to best.h5\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6587 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.61538\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6543 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00018: binary_accuracy improved from 0.61538 to 0.63077, saving model to best.h5\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6503 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.63077\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6466 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.63077\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6432 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00021: binary_accuracy improved from 0.63077 to 0.64615, saving model to best.h5\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6401 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00022: binary_accuracy improved from 0.64615 to 0.67692, saving model to best.h5\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6373 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00023: binary_accuracy improved from 0.67692 to 0.69231, saving model to best.h5\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6346 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.69231\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6322 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.69231\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6300 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.69231\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6279 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.69231\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6259 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00028: binary_accuracy improved from 0.69231 to 0.70769, saving model to best.h5\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6240 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.70769\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6223 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.70769\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6206 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.70769\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6190 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.70769\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6174 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.70769\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6159 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.70769\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6144 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.70769\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6129 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.70769\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6115 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.70769\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6100 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.70769\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6086 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.70769\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6071 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.70769\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6057 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.70769\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6043 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.70769\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6028 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.70769\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6014 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.70769\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5999 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.70769\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5984 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.70769\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5969 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.70769\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5954 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.70769\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5939 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.70769\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5923 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.70769\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.5908 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00051: binary_accuracy did not improve from 0.70769\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.5892 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.70769\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5876 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00053: binary_accuracy did not improve from 0.70769\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5861 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.70769\n",
            "Epoch 55/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5845 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00055: binary_accuracy did not improve from 0.70769\n",
            "Epoch 56/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5829 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00056: binary_accuracy did not improve from 0.70769\n",
            "Epoch 57/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5813 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00057: binary_accuracy did not improve from 0.70769\n",
            "Epoch 58/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5797 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00058: binary_accuracy did not improve from 0.70769\n",
            "Epoch 59/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5780 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00059: binary_accuracy did not improve from 0.70769\n",
            "Epoch 60/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5764 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00060: binary_accuracy did not improve from 0.70769\n",
            "Epoch 61/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5748 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00061: binary_accuracy did not improve from 0.70769\n",
            "Epoch 62/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5731 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00062: binary_accuracy did not improve from 0.70769\n",
            "Epoch 63/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5715 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00063: binary_accuracy did not improve from 0.70769\n",
            "Epoch 64/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5698 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00064: binary_accuracy did not improve from 0.70769\n",
            "Epoch 65/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5681 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00065: binary_accuracy did not improve from 0.70769\n",
            "Epoch 66/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5664 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00066: binary_accuracy did not improve from 0.70769\n",
            "Epoch 67/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5647 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00067: binary_accuracy did not improve from 0.70769\n",
            "Epoch 68/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5630 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00068: binary_accuracy did not improve from 0.70769\n",
            "(5, 1)\n",
            "2017-02-12\n",
            "2017-02-18\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([31, 34]))\n",
            "{0: 1.096774193548387, 1: 1}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 37s 567ms/step - loss: 0.7271 - binary_accuracy: 0.4615\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.46154, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7235 - binary_accuracy: 0.5077\n",
            "\n",
            "Epoch 00002: binary_accuracy improved from 0.46154 to 0.50769, saving model to best.h5\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7216 - binary_accuracy: 0.5077\n",
            "\n",
            "Epoch 00003: binary_accuracy did not improve from 0.50769\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7195 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00004: binary_accuracy improved from 0.50769 to 0.52308, saving model to best.h5\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7173 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00005: binary_accuracy did not improve from 0.52308\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7148 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00006: binary_accuracy did not improve from 0.52308\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7120 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00007: binary_accuracy improved from 0.52308 to 0.53846, saving model to best.h5\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7088 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00008: binary_accuracy improved from 0.53846 to 0.55385, saving model to best.h5\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7051 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00009: binary_accuracy improved from 0.55385 to 0.60000, saving model to best.h5\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7009 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00010: binary_accuracy did not improve from 0.60000\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6961 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00011: binary_accuracy improved from 0.60000 to 0.63077, saving model to best.h5\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6909 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00012: binary_accuracy improved from 0.63077 to 0.64615, saving model to best.h5\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.6852 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00013: binary_accuracy improved from 0.64615 to 0.67692, saving model to best.h5\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.6791 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00014: binary_accuracy did not improve from 0.67692\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.6728 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.67692\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.6665 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.67692\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.6604 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.67692\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6547 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.67692\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6495 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.67692\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.6451 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.67692\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.6412 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.67692\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.6379 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.67692\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.6350 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.67692\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.6326 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.67692\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.6304 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.67692\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.6284 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.67692\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6265 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.67692\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6248 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.67692\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6233 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.67692\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6217 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.67692\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6203 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.67692\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6189 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.67692\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6176 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.67692\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6163 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.67692\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6151 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.67692\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6138 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.67692\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6127 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.67692\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6115 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.67692\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6104 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.67692\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6093 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.67692\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6082 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.67692\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6071 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.67692\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6061 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.67692\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6051 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.67692\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6041 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.67692\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6030 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.67692\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6021 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.67692\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6011 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.67692\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6001 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.67692\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5991 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.67692\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5981 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00051: binary_accuracy did not improve from 0.67692\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5972 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00052: binary_accuracy improved from 0.67692 to 0.69231, saving model to best.h5\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5962 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00053: binary_accuracy did not improve from 0.69231\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5952 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.69231\n",
            "Epoch 55/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5943 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00055: binary_accuracy did not improve from 0.69231\n",
            "Epoch 56/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5933 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00056: binary_accuracy did not improve from 0.69231\n",
            "Epoch 57/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5923 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00057: binary_accuracy did not improve from 0.69231\n",
            "Epoch 58/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5913 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00058: binary_accuracy did not improve from 0.69231\n",
            "Epoch 59/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5903 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00059: binary_accuracy did not improve from 0.69231\n",
            "Epoch 60/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5893 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00060: binary_accuracy did not improve from 0.69231\n",
            "Epoch 61/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5882 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00061: binary_accuracy did not improve from 0.69231\n",
            "Epoch 62/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5872 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00062: binary_accuracy did not improve from 0.69231\n",
            "Epoch 63/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5861 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00063: binary_accuracy did not improve from 0.69231\n",
            "Epoch 64/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5850 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00064: binary_accuracy did not improve from 0.69231\n",
            "Epoch 65/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5839 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00065: binary_accuracy did not improve from 0.69231\n",
            "Epoch 66/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5828 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00066: binary_accuracy did not improve from 0.69231\n",
            "Epoch 67/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5816 - binary_accuracy: 0.6769\n",
            "\n",
            "Epoch 00067: binary_accuracy did not improve from 0.69231\n",
            "Epoch 68/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5804 - binary_accuracy: 0.6923\n",
            "\n",
            "Epoch 00068: binary_accuracy did not improve from 0.69231\n",
            "Epoch 69/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5792 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00069: binary_accuracy improved from 0.69231 to 0.70769, saving model to best.h5\n",
            "Epoch 70/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5779 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00070: binary_accuracy did not improve from 0.70769\n",
            "Epoch 71/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5766 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00071: binary_accuracy did not improve from 0.70769\n",
            "Epoch 72/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5753 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00072: binary_accuracy did not improve from 0.70769\n",
            "Epoch 73/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5739 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00073: binary_accuracy did not improve from 0.70769\n",
            "Epoch 74/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5725 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00074: binary_accuracy did not improve from 0.70769\n",
            "Epoch 75/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5710 - binary_accuracy: 0.7077\n",
            "\n",
            "Epoch 00075: binary_accuracy did not improve from 0.70769\n",
            "Epoch 76/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5695 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00076: binary_accuracy improved from 0.70769 to 0.72308, saving model to best.h5\n",
            "Epoch 77/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5680 - binary_accuracy: 0.7231\n",
            "\n",
            "Epoch 00077: binary_accuracy did not improve from 0.72308\n",
            "Epoch 78/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5664 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00078: binary_accuracy improved from 0.72308 to 0.73846, saving model to best.h5\n",
            "Epoch 79/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5647 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00079: binary_accuracy did not improve from 0.73846\n",
            "Epoch 80/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5630 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00080: binary_accuracy did not improve from 0.73846\n",
            "Epoch 81/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5612 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00081: binary_accuracy did not improve from 0.73846\n",
            "Epoch 82/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5593 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00082: binary_accuracy did not improve from 0.73846\n",
            "Epoch 83/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5573 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00083: binary_accuracy did not improve from 0.73846\n",
            "Epoch 84/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5552 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00084: binary_accuracy did not improve from 0.73846\n",
            "Epoch 85/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5530 - binary_accuracy: 0.7385\n",
            "\n",
            "Epoch 00085: binary_accuracy did not improve from 0.73846\n",
            "Epoch 86/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5508 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00086: binary_accuracy improved from 0.73846 to 0.75385, saving model to best.h5\n",
            "Epoch 87/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5485 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00087: binary_accuracy did not improve from 0.75385\n",
            "Epoch 88/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5461 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00088: binary_accuracy did not improve from 0.75385\n",
            "Epoch 89/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5436 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00089: binary_accuracy did not improve from 0.75385\n",
            "Epoch 90/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5411 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00090: binary_accuracy did not improve from 0.75385\n",
            "Epoch 91/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5386 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00091: binary_accuracy did not improve from 0.75385\n",
            "Epoch 92/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5361 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00092: binary_accuracy did not improve from 0.75385\n",
            "Epoch 93/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5336 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00093: binary_accuracy did not improve from 0.75385\n",
            "Epoch 94/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5313 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00094: binary_accuracy improved from 0.75385 to 0.76923, saving model to best.h5\n",
            "Epoch 95/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5290 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00095: binary_accuracy did not improve from 0.76923\n",
            "Epoch 96/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5268 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00096: binary_accuracy did not improve from 0.76923\n",
            "Epoch 97/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5251 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00097: binary_accuracy did not improve from 0.76923\n",
            "Epoch 98/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5238 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00098: binary_accuracy did not improve from 0.76923\n",
            "Epoch 99/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5237 - binary_accuracy: 0.7538\n",
            "\n",
            "Epoch 00099: binary_accuracy did not improve from 0.76923\n",
            "Epoch 100/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5239 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00100: binary_accuracy did not improve from 0.76923\n",
            "Epoch 101/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5228 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00101: binary_accuracy did not improve from 0.76923\n",
            "Epoch 102/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5220 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00102: binary_accuracy improved from 0.76923 to 0.78462, saving model to best.h5\n",
            "Epoch 103/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5211 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00103: binary_accuracy did not improve from 0.78462\n",
            "Epoch 104/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5201 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00104: binary_accuracy did not improve from 0.78462\n",
            "Epoch 105/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5194 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00105: binary_accuracy did not improve from 0.78462\n",
            "Epoch 106/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5183 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00106: binary_accuracy improved from 0.78462 to 0.80000, saving model to best.h5\n",
            "Epoch 107/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5174 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00107: binary_accuracy did not improve from 0.80000\n",
            "Epoch 108/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5162 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00108: binary_accuracy did not improve from 0.80000\n",
            "Epoch 109/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5154 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00109: binary_accuracy did not improve from 0.80000\n",
            "Epoch 110/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5144 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00110: binary_accuracy did not improve from 0.80000\n",
            "Epoch 111/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5135 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00111: binary_accuracy did not improve from 0.80000\n",
            "Epoch 112/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5126 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00112: binary_accuracy did not improve from 0.80000\n",
            "Epoch 113/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5117 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00113: binary_accuracy did not improve from 0.80000\n",
            "Epoch 114/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5109 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00114: binary_accuracy did not improve from 0.80000\n",
            "Epoch 115/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5101 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00115: binary_accuracy did not improve from 0.80000\n",
            "Epoch 116/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5089 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00116: binary_accuracy did not improve from 0.80000\n",
            "Epoch 117/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5085 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00117: binary_accuracy did not improve from 0.80000\n",
            "Epoch 118/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5073 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00118: binary_accuracy did not improve from 0.80000\n",
            "Epoch 119/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5069 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00119: binary_accuracy did not improve from 0.80000\n",
            "Epoch 120/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5060 - binary_accuracy: 0.8000\n",
            "\n",
            "Epoch 00120: binary_accuracy did not improve from 0.80000\n",
            "Epoch 121/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5053 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00121: binary_accuracy did not improve from 0.80000\n",
            "Epoch 122/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5047 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00122: binary_accuracy did not improve from 0.80000\n",
            "Epoch 123/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5036 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00123: binary_accuracy did not improve from 0.80000\n",
            "Epoch 124/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5032 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00124: binary_accuracy did not improve from 0.80000\n",
            "Epoch 125/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5023 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00125: binary_accuracy did not improve from 0.80000\n",
            "Epoch 126/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5017 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00126: binary_accuracy did not improve from 0.80000\n",
            "Epoch 127/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5009 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00127: binary_accuracy did not improve from 0.80000\n",
            "Epoch 128/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.5003 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00128: binary_accuracy did not improve from 0.80000\n",
            "Epoch 129/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4996 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00129: binary_accuracy did not improve from 0.80000\n",
            "Epoch 130/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4990 - binary_accuracy: 0.7846\n",
            "\n",
            "Epoch 00130: binary_accuracy did not improve from 0.80000\n",
            "Epoch 131/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4982 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00131: binary_accuracy did not improve from 0.80000\n",
            "Epoch 132/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4977 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00132: binary_accuracy did not improve from 0.80000\n",
            "Epoch 133/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4969 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00133: binary_accuracy did not improve from 0.80000\n",
            "Epoch 134/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4964 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00134: binary_accuracy did not improve from 0.80000\n",
            "Epoch 135/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4958 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00135: binary_accuracy did not improve from 0.80000\n",
            "Epoch 136/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4952 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00136: binary_accuracy did not improve from 0.80000\n",
            "Epoch 137/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4947 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00137: binary_accuracy did not improve from 0.80000\n",
            "Epoch 138/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4940 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00138: binary_accuracy did not improve from 0.80000\n",
            "Epoch 139/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4936 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00139: binary_accuracy did not improve from 0.80000\n",
            "Epoch 140/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4930 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00140: binary_accuracy did not improve from 0.80000\n",
            "Epoch 141/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4926 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00141: binary_accuracy did not improve from 0.80000\n",
            "Epoch 142/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4920 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00142: binary_accuracy did not improve from 0.80000\n",
            "Epoch 143/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4916 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00143: binary_accuracy did not improve from 0.80000\n",
            "Epoch 144/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4910 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00144: binary_accuracy did not improve from 0.80000\n",
            "Epoch 145/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4907 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00145: binary_accuracy did not improve from 0.80000\n",
            "Epoch 146/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.4901 - binary_accuracy: 0.7692\n",
            "\n",
            "Epoch 00146: binary_accuracy did not improve from 0.80000\n",
            "(5, 1)\n",
            "2017-02-19\n",
            "2017-02-25\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([34, 31]))\n",
            "{0: 1, 1: 1.096774193548387}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 37s 565ms/step - loss: 0.7275 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.52308, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.7253 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00002: binary_accuracy did not improve from 0.52308\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.7242 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00003: binary_accuracy did not improve from 0.52308\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.7231 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00004: binary_accuracy did not improve from 0.52308\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.7219 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00005: binary_accuracy improved from 0.52308 to 0.53846, saving model to best.h5\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.7206 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00006: binary_accuracy improved from 0.53846 to 0.56923, saving model to best.h5\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7190 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00007: binary_accuracy improved from 0.56923 to 0.58462, saving model to best.h5\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7171 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00008: binary_accuracy improved from 0.58462 to 0.60000, saving model to best.h5\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7149 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00009: binary_accuracy did not improve from 0.60000\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7123 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00010: binary_accuracy did not improve from 0.60000\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7093 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.60000\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7059 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00012: binary_accuracy did not improve from 0.60000\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7022 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00013: binary_accuracy did not improve from 0.60000\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6983 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00014: binary_accuracy improved from 0.60000 to 0.61538, saving model to best.h5\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6943 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.61538\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6905 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00016: binary_accuracy improved from 0.61538 to 0.64615, saving model to best.h5\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6868 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.64615\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6835 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.64615\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6807 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.64615\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6784 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.64615\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6766 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.64615\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6752 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.64615\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6741 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.64615\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6733 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.64615\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6727 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.64615\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6722 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.64615\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6718 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.64615\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6715 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.64615\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6711 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.64615\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6707 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.64615\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6704 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.64615\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6700 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.64615\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6696 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.64615\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6692 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.64615\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6688 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00035: binary_accuracy improved from 0.64615 to 0.66154, saving model to best.h5\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6684 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.66154\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6680 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.66154\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6676 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.66154\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6672 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.66154\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6668 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.66154\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6664 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.66154\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6660 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.66154\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6656 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.66154\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6652 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.66154\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6648 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.66154\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6644 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.66154\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6640 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.66154\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6635 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.66154\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6631 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.66154\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6627 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.66154\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6623 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00051: binary_accuracy did not improve from 0.66154\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6619 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.66154\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6615 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00053: binary_accuracy did not improve from 0.66154\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6611 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.66154\n",
            "Epoch 55/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6607 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00055: binary_accuracy did not improve from 0.66154\n",
            "Epoch 56/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.6603 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00056: binary_accuracy did not improve from 0.66154\n",
            "Epoch 57/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.6599 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00057: binary_accuracy did not improve from 0.66154\n",
            "Epoch 58/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6595 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00058: binary_accuracy did not improve from 0.66154\n",
            "Epoch 59/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6591 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00059: binary_accuracy did not improve from 0.66154\n",
            "Epoch 60/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.6587 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00060: binary_accuracy did not improve from 0.66154\n",
            "Epoch 61/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.6583 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00061: binary_accuracy did not improve from 0.66154\n",
            "Epoch 62/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.6579 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00062: binary_accuracy did not improve from 0.66154\n",
            "Epoch 63/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.6575 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00063: binary_accuracy did not improve from 0.66154\n",
            "Epoch 64/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.6572 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00064: binary_accuracy did not improve from 0.66154\n",
            "Epoch 65/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6568 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00065: binary_accuracy did not improve from 0.66154\n",
            "Epoch 66/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.6564 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00066: binary_accuracy did not improve from 0.66154\n",
            "Epoch 67/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6560 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00067: binary_accuracy did not improve from 0.66154\n",
            "Epoch 68/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6555 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00068: binary_accuracy did not improve from 0.66154\n",
            "Epoch 69/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.6552 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00069: binary_accuracy did not improve from 0.66154\n",
            "Epoch 70/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.6548 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00070: binary_accuracy did not improve from 0.66154\n",
            "Epoch 71/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6544 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00071: binary_accuracy did not improve from 0.66154\n",
            "Epoch 72/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6540 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00072: binary_accuracy did not improve from 0.66154\n",
            "Epoch 73/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6536 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00073: binary_accuracy did not improve from 0.66154\n",
            "Epoch 74/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6532 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00074: binary_accuracy did not improve from 0.66154\n",
            "Epoch 75/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6528 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00075: binary_accuracy did not improve from 0.66154\n",
            "(5, 1)\n",
            "2017-02-19\n",
            "2017-02-25\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([34, 31]))\n",
            "{0: 1, 1: 1.096774193548387}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 37s 574ms/step - loss: 0.7296 - binary_accuracy: 0.4308\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.43077, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7268 - binary_accuracy: 0.4615\n",
            "\n",
            "Epoch 00002: binary_accuracy improved from 0.43077 to 0.46154, saving model to best.h5\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7252 - binary_accuracy: 0.4923\n",
            "\n",
            "Epoch 00003: binary_accuracy improved from 0.46154 to 0.49231, saving model to best.h5\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7236 - binary_accuracy: 0.5077\n",
            "\n",
            "Epoch 00004: binary_accuracy improved from 0.49231 to 0.50769, saving model to best.h5\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7219 - binary_accuracy: 0.5538\n",
            "\n",
            "Epoch 00005: binary_accuracy improved from 0.50769 to 0.55385, saving model to best.h5\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7201 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00006: binary_accuracy improved from 0.55385 to 0.58462, saving model to best.h5\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7181 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00007: binary_accuracy improved from 0.58462 to 0.61538, saving model to best.h5\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7158 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00008: binary_accuracy did not improve from 0.61538\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7132 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00009: binary_accuracy did not improve from 0.61538\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7103 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00010: binary_accuracy did not improve from 0.61538\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7071 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.61538\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7037 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00012: binary_accuracy did not improve from 0.61538\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7002 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00013: binary_accuracy did not improve from 0.61538\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6967 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00014: binary_accuracy did not improve from 0.61538\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6933 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.61538\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6903 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.61538\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6875 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.61538\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6852 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.61538\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6832 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.61538\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6816 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.61538\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6803 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.61538\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6791 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.61538\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6782 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.61538\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6773 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.61538\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6765 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.61538\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6758 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.61538\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6751 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00027: binary_accuracy improved from 0.61538 to 0.63077, saving model to best.h5\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6744 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.63077\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6738 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.63077\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6731 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.63077\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6725 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.63077\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6718 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.63077\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6712 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00033: binary_accuracy improved from 0.63077 to 0.64615, saving model to best.h5\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6706 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.64615\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6700 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.64615\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6693 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.64615\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6687 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.64615\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6682 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.64615\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6676 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.64615\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6670 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.64615\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6665 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.64615\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6659 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.64615\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6654 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.64615\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6649 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.64615\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6644 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.64615\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6639 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00046: binary_accuracy improved from 0.64615 to 0.66154, saving model to best.h5\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6634 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.66154\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6629 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.66154\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6624 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.66154\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6619 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.66154\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6615 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00051: binary_accuracy did not improve from 0.66154\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6610 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.66154\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6606 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00053: binary_accuracy did not improve from 0.66154\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6601 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.66154\n",
            "Epoch 55/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6597 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00055: binary_accuracy did not improve from 0.66154\n",
            "Epoch 56/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6592 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00056: binary_accuracy did not improve from 0.66154\n",
            "Epoch 57/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6588 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00057: binary_accuracy did not improve from 0.66154\n",
            "Epoch 58/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6583 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00058: binary_accuracy did not improve from 0.66154\n",
            "Epoch 59/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6578 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00059: binary_accuracy did not improve from 0.66154\n",
            "Epoch 60/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6574 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00060: binary_accuracy did not improve from 0.66154\n",
            "Epoch 61/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6569 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00061: binary_accuracy did not improve from 0.66154\n",
            "Epoch 62/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6564 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00062: binary_accuracy did not improve from 0.66154\n",
            "Epoch 63/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6560 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00063: binary_accuracy did not improve from 0.66154\n",
            "Epoch 64/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6555 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00064: binary_accuracy did not improve from 0.66154\n",
            "Epoch 65/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6550 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00065: binary_accuracy did not improve from 0.66154\n",
            "Epoch 66/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6546 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00066: binary_accuracy did not improve from 0.66154\n",
            "Epoch 67/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6541 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00067: binary_accuracy did not improve from 0.66154\n",
            "Epoch 68/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6536 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00068: binary_accuracy did not improve from 0.66154\n",
            "Epoch 69/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6532 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00069: binary_accuracy did not improve from 0.66154\n",
            "Epoch 70/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6527 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00070: binary_accuracy did not improve from 0.66154\n",
            "Epoch 71/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6522 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00071: binary_accuracy did not improve from 0.66154\n",
            "Epoch 72/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6517 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00072: binary_accuracy did not improve from 0.66154\n",
            "Epoch 73/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6513 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00073: binary_accuracy did not improve from 0.66154\n",
            "Epoch 74/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6508 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00074: binary_accuracy did not improve from 0.66154\n",
            "Epoch 75/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6503 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00075: binary_accuracy did not improve from 0.66154\n",
            "Epoch 76/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6498 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00076: binary_accuracy did not improve from 0.66154\n",
            "Epoch 77/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6493 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00077: binary_accuracy did not improve from 0.66154\n",
            "Epoch 78/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6488 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00078: binary_accuracy did not improve from 0.66154\n",
            "Epoch 79/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6483 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00079: binary_accuracy did not improve from 0.66154\n",
            "Epoch 80/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6478 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00080: binary_accuracy did not improve from 0.66154\n",
            "Epoch 81/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6473 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00081: binary_accuracy did not improve from 0.66154\n",
            "Epoch 82/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6468 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00082: binary_accuracy did not improve from 0.66154\n",
            "Epoch 83/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6462 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00083: binary_accuracy did not improve from 0.66154\n",
            "Epoch 84/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6457 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00084: binary_accuracy did not improve from 0.66154\n",
            "Epoch 85/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6452 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00085: binary_accuracy did not improve from 0.66154\n",
            "Epoch 86/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6447 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00086: binary_accuracy did not improve from 0.66154\n",
            "(5, 1)\n",
            "2017-02-19\n",
            "2017-02-25\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([34, 31]))\n",
            "{0: 1, 1: 1.096774193548387}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 37s 564ms/step - loss: 0.7338 - binary_accuracy: 0.4000\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.40000, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.7305 - binary_accuracy: 0.4308\n",
            "\n",
            "Epoch 00002: binary_accuracy improved from 0.40000 to 0.43077, saving model to best.h5\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.7288 - binary_accuracy: 0.4308\n",
            "\n",
            "Epoch 00003: binary_accuracy did not improve from 0.43077\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.7272 - binary_accuracy: 0.4462\n",
            "\n",
            "Epoch 00004: binary_accuracy improved from 0.43077 to 0.44615, saving model to best.h5\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7258 - binary_accuracy: 0.5231\n",
            "\n",
            "Epoch 00005: binary_accuracy improved from 0.44615 to 0.52308, saving model to best.h5\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7243 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00006: binary_accuracy improved from 0.52308 to 0.58462, saving model to best.h5\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7226 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00007: binary_accuracy improved from 0.58462 to 0.60000, saving model to best.h5\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7207 - binary_accuracy: 0.5692\n",
            "\n",
            "Epoch 00008: binary_accuracy did not improve from 0.60000\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7186 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00009: binary_accuracy did not improve from 0.60000\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7161 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00010: binary_accuracy improved from 0.60000 to 0.61538, saving model to best.h5\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7132 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00011: binary_accuracy improved from 0.61538 to 0.63077, saving model to best.h5\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7099 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00012: binary_accuracy did not improve from 0.63077\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7061 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00013: binary_accuracy improved from 0.63077 to 0.64615, saving model to best.h5\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7020 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00014: binary_accuracy improved from 0.64615 to 0.66154, saving model to best.h5\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6976 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.66154\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6932 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.66154\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6888 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.66154\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6849 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.66154\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6814 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.66154\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6785 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.66154\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6762 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.66154\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6744 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.66154\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6731 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.66154\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6720 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.66154\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6712 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.66154\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6705 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.66154\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6700 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.66154\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6694 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.66154\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6690 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.66154\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6685 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.66154\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6681 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.66154\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6676 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.66154\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6672 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.66154\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6668 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.66154\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6664 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.66154\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6659 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.66154\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6655 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.66154\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6651 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.66154\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6647 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00039: binary_accuracy did not improve from 0.66154\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6643 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.66154\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6640 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.66154\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6636 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.66154\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6632 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.66154\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6628 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.66154\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6624 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.66154\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6620 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.66154\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6617 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.66154\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6613 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.66154\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6609 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.66154\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6606 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.66154\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6602 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00051: binary_accuracy did not improve from 0.66154\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6598 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.66154\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6594 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00053: binary_accuracy did not improve from 0.66154\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6591 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.66154\n",
            "(5, 1)\n",
            "2017-02-19\n",
            "2017-02-25\n",
            "Close(t-4)\n",
            "marketrisk_avg30(t-0)\n",
            "65\n",
            "(array([False,  True]), array([34, 31]))\n",
            "{0: 1, 1: 1.096774193548387}\n",
            "Epoch 1/300\n",
            "65/65 [==============================] - 37s 576ms/step - loss: 0.7261 - binary_accuracy: 0.4615\n",
            "\n",
            "Epoch 00001: binary_accuracy improved from -inf to 0.46154, saving model to best.h5\n",
            "Epoch 2/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7235 - binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00002: binary_accuracy improved from 0.46154 to 0.53846, saving model to best.h5\n",
            "Epoch 3/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7217 - binary_accuracy: 0.5846\n",
            "\n",
            "Epoch 00003: binary_accuracy improved from 0.53846 to 0.58462, saving model to best.h5\n",
            "Epoch 4/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7196 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00004: binary_accuracy improved from 0.58462 to 0.60000, saving model to best.h5\n",
            "Epoch 5/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7173 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00005: binary_accuracy did not improve from 0.60000\n",
            "Epoch 6/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7145 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00006: binary_accuracy did not improve from 0.60000\n",
            "Epoch 7/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7113 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00007: binary_accuracy improved from 0.60000 to 0.61538, saving model to best.h5\n",
            "Epoch 8/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7077 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00008: binary_accuracy did not improve from 0.61538\n",
            "Epoch 9/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.7036 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00009: binary_accuracy improved from 0.61538 to 0.64615, saving model to best.h5\n",
            "Epoch 10/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6993 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00010: binary_accuracy did not improve from 0.64615\n",
            "Epoch 11/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6949 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00011: binary_accuracy did not improve from 0.64615\n",
            "Epoch 12/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6908 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00012: binary_accuracy did not improve from 0.64615\n",
            "Epoch 13/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6870 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00013: binary_accuracy did not improve from 0.64615\n",
            "Epoch 14/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6838 - binary_accuracy: 0.6000\n",
            "\n",
            "Epoch 00014: binary_accuracy did not improve from 0.64615\n",
            "Epoch 15/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6813 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00015: binary_accuracy did not improve from 0.64615\n",
            "Epoch 16/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6793 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00016: binary_accuracy did not improve from 0.64615\n",
            "Epoch 17/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6778 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00017: binary_accuracy did not improve from 0.64615\n",
            "Epoch 18/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6767 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00018: binary_accuracy did not improve from 0.64615\n",
            "Epoch 19/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6759 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00019: binary_accuracy did not improve from 0.64615\n",
            "Epoch 20/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6752 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00020: binary_accuracy did not improve from 0.64615\n",
            "Epoch 21/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6746 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00021: binary_accuracy did not improve from 0.64615\n",
            "Epoch 22/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6741 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00022: binary_accuracy did not improve from 0.64615\n",
            "Epoch 23/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6736 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00023: binary_accuracy did not improve from 0.64615\n",
            "Epoch 24/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6731 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00024: binary_accuracy did not improve from 0.64615\n",
            "Epoch 25/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6726 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00025: binary_accuracy did not improve from 0.64615\n",
            "Epoch 26/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6721 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00026: binary_accuracy did not improve from 0.64615\n",
            "Epoch 27/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6716 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00027: binary_accuracy did not improve from 0.64615\n",
            "Epoch 28/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6711 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00028: binary_accuracy did not improve from 0.64615\n",
            "Epoch 29/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6706 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00029: binary_accuracy did not improve from 0.64615\n",
            "Epoch 30/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6701 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00030: binary_accuracy did not improve from 0.64615\n",
            "Epoch 31/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6696 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00031: binary_accuracy did not improve from 0.64615\n",
            "Epoch 32/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6692 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00032: binary_accuracy did not improve from 0.64615\n",
            "Epoch 33/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6687 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00033: binary_accuracy did not improve from 0.64615\n",
            "Epoch 34/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6683 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00034: binary_accuracy did not improve from 0.64615\n",
            "Epoch 35/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6678 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00035: binary_accuracy did not improve from 0.64615\n",
            "Epoch 36/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6674 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00036: binary_accuracy did not improve from 0.64615\n",
            "Epoch 37/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6669 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00037: binary_accuracy did not improve from 0.64615\n",
            "Epoch 38/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6665 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00038: binary_accuracy did not improve from 0.64615\n",
            "Epoch 39/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6661 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00039: binary_accuracy improved from 0.64615 to 0.66154, saving model to best.h5\n",
            "Epoch 40/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6657 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00040: binary_accuracy did not improve from 0.66154\n",
            "Epoch 41/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6653 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00041: binary_accuracy did not improve from 0.66154\n",
            "Epoch 42/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6649 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00042: binary_accuracy did not improve from 0.66154\n",
            "Epoch 43/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6645 - binary_accuracy: 0.6615\n",
            "\n",
            "Epoch 00043: binary_accuracy did not improve from 0.66154\n",
            "Epoch 44/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6641 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00044: binary_accuracy did not improve from 0.66154\n",
            "Epoch 45/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6637 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00045: binary_accuracy did not improve from 0.66154\n",
            "Epoch 46/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6633 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00046: binary_accuracy did not improve from 0.66154\n",
            "Epoch 47/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6629 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00047: binary_accuracy did not improve from 0.66154\n",
            "Epoch 48/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6625 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00048: binary_accuracy did not improve from 0.66154\n",
            "Epoch 49/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6621 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00049: binary_accuracy did not improve from 0.66154\n",
            "Epoch 50/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6617 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00050: binary_accuracy did not improve from 0.66154\n",
            "Epoch 51/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6613 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00051: binary_accuracy did not improve from 0.66154\n",
            "Epoch 52/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6608 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00052: binary_accuracy did not improve from 0.66154\n",
            "Epoch 53/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6604 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00053: binary_accuracy did not improve from 0.66154\n",
            "Epoch 54/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6600 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00054: binary_accuracy did not improve from 0.66154\n",
            "Epoch 55/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6596 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00055: binary_accuracy did not improve from 0.66154\n",
            "Epoch 56/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6592 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00056: binary_accuracy did not improve from 0.66154\n",
            "Epoch 57/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6587 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00057: binary_accuracy did not improve from 0.66154\n",
            "Epoch 58/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6583 - binary_accuracy: 0.6308\n",
            "\n",
            "Epoch 00058: binary_accuracy did not improve from 0.66154\n",
            "Epoch 59/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6578 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00059: binary_accuracy did not improve from 0.66154\n",
            "Epoch 60/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6574 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00060: binary_accuracy did not improve from 0.66154\n",
            "Epoch 61/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6569 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00061: binary_accuracy did not improve from 0.66154\n",
            "Epoch 62/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6565 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00062: binary_accuracy did not improve from 0.66154\n",
            "Epoch 63/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6560 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00063: binary_accuracy did not improve from 0.66154\n",
            "Epoch 64/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6555 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00064: binary_accuracy did not improve from 0.66154\n",
            "Epoch 65/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6551 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00065: binary_accuracy did not improve from 0.66154\n",
            "Epoch 66/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6546 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00066: binary_accuracy did not improve from 0.66154\n",
            "Epoch 67/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6541 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00067: binary_accuracy did not improve from 0.66154\n",
            "Epoch 68/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6536 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00068: binary_accuracy did not improve from 0.66154\n",
            "Epoch 69/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6532 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00069: binary_accuracy did not improve from 0.66154\n",
            "Epoch 70/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6527 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00070: binary_accuracy did not improve from 0.66154\n",
            "Epoch 71/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6522 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00071: binary_accuracy did not improve from 0.66154\n",
            "Epoch 72/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6517 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00072: binary_accuracy did not improve from 0.66154\n",
            "Epoch 73/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6512 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00073: binary_accuracy did not improve from 0.66154\n",
            "Epoch 74/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6507 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00074: binary_accuracy did not improve from 0.66154\n",
            "Epoch 75/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6502 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00075: binary_accuracy did not improve from 0.66154\n",
            "Epoch 76/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6498 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00076: binary_accuracy did not improve from 0.66154\n",
            "Epoch 77/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6493 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00077: binary_accuracy did not improve from 0.66154\n",
            "Epoch 78/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6487 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00078: binary_accuracy did not improve from 0.66154\n",
            "Epoch 79/300\n",
            "65/65 [==============================] - 0s 5ms/step - loss: 0.6482 - binary_accuracy: 0.6462\n",
            "\n",
            "Epoch 00079: binary_accuracy did not improve from 0.66154\n",
            "(5, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Close</th>\n",
              "      <th>marketRisk</th>\n",
              "      <th>returns</th>\n",
              "      <th>Target</th>\n",
              "      <th>Change</th>\n",
              "      <th>Signal</th>\n",
              "      <th>Close30</th>\n",
              "      <th>Close100</th>\n",
              "      <th>r1</th>\n",
              "      <th>r2</th>\n",
              "      <th>marketrisk_avg30</th>\n",
              "      <th>marketrisk_avg90</th>\n",
              "      <th>predictSignal</th>\n",
              "      <th>predictedweight</th>\n",
              "      <th>labelledweight</th>\n",
              "      <th>r</th>\n",
              "      <th>predictedreturns</th>\n",
              "      <th>labelledreturns</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2017-01-02</th>\n",
              "      <td>0.956297</td>\n",
              "      <td>0.074303</td>\n",
              "      <td>0.005355</td>\n",
              "      <td>0.004872</td>\n",
              "      <td>0.005094</td>\n",
              "      <td>False</td>\n",
              "      <td>0.948200</td>\n",
              "      <td>0.915872</td>\n",
              "      <td>0.005094</td>\n",
              "      <td>-0.002016</td>\n",
              "      <td>0.066802</td>\n",
              "      <td>0.065010</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-03</th>\n",
              "      <td>0.961169</td>\n",
              "      <td>0.101960</td>\n",
              "      <td>0.005094</td>\n",
              "      <td>-0.007516</td>\n",
              "      <td>-0.007820</td>\n",
              "      <td>False</td>\n",
              "      <td>0.948863</td>\n",
              "      <td>0.916616</td>\n",
              "      <td>0.004872</td>\n",
              "      <td>0.004597</td>\n",
              "      <td>0.068402</td>\n",
              "      <td>0.065185</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.005094</td>\n",
              "      <td>0.005094</td>\n",
              "      <td>-0.005094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-04</th>\n",
              "      <td>0.953652</td>\n",
              "      <td>0.058255</td>\n",
              "      <td>-0.007820</td>\n",
              "      <td>-0.010523</td>\n",
              "      <td>-0.011035</td>\n",
              "      <td>False</td>\n",
              "      <td>0.949074</td>\n",
              "      <td>0.917292</td>\n",
              "      <td>-0.007516</td>\n",
              "      <td>-0.003011</td>\n",
              "      <td>0.067669</td>\n",
              "      <td>0.064691</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.007820</td>\n",
              "      <td>-0.007820</td>\n",
              "      <td>0.007820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-05</th>\n",
              "      <td>0.943129</td>\n",
              "      <td>0.049752</td>\n",
              "      <td>-0.011035</td>\n",
              "      <td>0.006538</td>\n",
              "      <td>0.006933</td>\n",
              "      <td>True</td>\n",
              "      <td>0.948922</td>\n",
              "      <td>0.917914</td>\n",
              "      <td>-0.010523</td>\n",
              "      <td>-0.013351</td>\n",
              "      <td>0.066586</td>\n",
              "      <td>0.064512</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.011035</td>\n",
              "      <td>-0.011035</td>\n",
              "      <td>0.011035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-06</th>\n",
              "      <td>0.949668</td>\n",
              "      <td>0.064650</td>\n",
              "      <td>0.006933</td>\n",
              "      <td>-0.003773</td>\n",
              "      <td>-0.003973</td>\n",
              "      <td>False</td>\n",
              "      <td>0.949083</td>\n",
              "      <td>0.918581</td>\n",
              "      <td>0.006538</td>\n",
              "      <td>-0.011224</td>\n",
              "      <td>0.066382</td>\n",
              "      <td>0.064718</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.006933</td>\n",
              "      <td>0.006933</td>\n",
              "      <td>0.006933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-09</th>\n",
              "      <td>0.945895</td>\n",
              "      <td>0.059655</td>\n",
              "      <td>-0.003973</td>\n",
              "      <td>0.001703</td>\n",
              "      <td>0.001800</td>\n",
              "      <td>False</td>\n",
              "      <td>0.949205</td>\n",
              "      <td>0.919202</td>\n",
              "      <td>-0.003773</td>\n",
              "      <td>-0.007394</td>\n",
              "      <td>0.066756</td>\n",
              "      <td>0.064769</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.003973</td>\n",
              "      <td>-0.003973</td>\n",
              "      <td>0.003973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-10</th>\n",
              "      <td>0.947598</td>\n",
              "      <td>0.033261</td>\n",
              "      <td>0.001800</td>\n",
              "      <td>-0.002418</td>\n",
              "      <td>-0.002552</td>\n",
              "      <td>False</td>\n",
              "      <td>0.949484</td>\n",
              "      <td>0.919830</td>\n",
              "      <td>0.001703</td>\n",
              "      <td>-0.003605</td>\n",
              "      <td>0.065723</td>\n",
              "      <td>0.064691</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.001800</td>\n",
              "      <td>-0.001800</td>\n",
              "      <td>-0.001800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-11</th>\n",
              "      <td>0.945180</td>\n",
              "      <td>0.062518</td>\n",
              "      <td>-0.002552</td>\n",
              "      <td>-0.002673</td>\n",
              "      <td>-0.002828</td>\n",
              "      <td>False</td>\n",
              "      <td>0.949499</td>\n",
              "      <td>0.920402</td>\n",
              "      <td>-0.002418</td>\n",
              "      <td>-0.011118</td>\n",
              "      <td>0.065729</td>\n",
              "      <td>0.064932</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.002552</td>\n",
              "      <td>0.002552</td>\n",
              "      <td>0.002552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-12</th>\n",
              "      <td>0.942507</td>\n",
              "      <td>0.036572</td>\n",
              "      <td>-0.002828</td>\n",
              "      <td>-0.002746</td>\n",
              "      <td>-0.002913</td>\n",
              "      <td>False</td>\n",
              "      <td>0.949646</td>\n",
              "      <td>0.920961</td>\n",
              "      <td>-0.002673</td>\n",
              "      <td>-0.018662</td>\n",
              "      <td>0.064580</td>\n",
              "      <td>0.064800</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.002828</td>\n",
              "      <td>-0.002828</td>\n",
              "      <td>0.002828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-13</th>\n",
              "      <td>0.939761</td>\n",
              "      <td>0.053134</td>\n",
              "      <td>-0.002913</td>\n",
              "      <td>0.003635</td>\n",
              "      <td>0.003868</td>\n",
              "      <td>True</td>\n",
              "      <td>0.949731</td>\n",
              "      <td>0.921426</td>\n",
              "      <td>-0.002746</td>\n",
              "      <td>-0.013891</td>\n",
              "      <td>0.064965</td>\n",
              "      <td>0.064543</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.002913</td>\n",
              "      <td>-0.002913</td>\n",
              "      <td>0.002913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-16</th>\n",
              "      <td>0.943396</td>\n",
              "      <td>0.043191</td>\n",
              "      <td>0.003868</td>\n",
              "      <td>-0.009864</td>\n",
              "      <td>-0.010456</td>\n",
              "      <td>False</td>\n",
              "      <td>0.950205</td>\n",
              "      <td>0.921922</td>\n",
              "      <td>0.003635</td>\n",
              "      <td>0.000267</td>\n",
              "      <td>0.064260</td>\n",
              "      <td>0.064488</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.003868</td>\n",
              "      <td>0.003868</td>\n",
              "      <td>0.003868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-17</th>\n",
              "      <td>0.933532</td>\n",
              "      <td>0.039432</td>\n",
              "      <td>-0.010456</td>\n",
              "      <td>0.007201</td>\n",
              "      <td>0.007714</td>\n",
              "      <td>True</td>\n",
              "      <td>0.950219</td>\n",
              "      <td>0.922281</td>\n",
              "      <td>-0.009864</td>\n",
              "      <td>-0.016135</td>\n",
              "      <td>0.063088</td>\n",
              "      <td>0.064473</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.010456</td>\n",
              "      <td>0.010456</td>\n",
              "      <td>0.010456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-18</th>\n",
              "      <td>0.940734</td>\n",
              "      <td>0.050054</td>\n",
              "      <td>0.007714</td>\n",
              "      <td>-0.002911</td>\n",
              "      <td>-0.003095</td>\n",
              "      <td>False</td>\n",
              "      <td>0.950572</td>\n",
              "      <td>0.922725</td>\n",
              "      <td>0.007201</td>\n",
              "      <td>-0.005161</td>\n",
              "      <td>0.062997</td>\n",
              "      <td>0.064406</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.007714</td>\n",
              "      <td>-0.007714</td>\n",
              "      <td>0.007714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-19</th>\n",
              "      <td>0.937822</td>\n",
              "      <td>0.068003</td>\n",
              "      <td>-0.003095</td>\n",
              "      <td>-0.003156</td>\n",
              "      <td>-0.003365</td>\n",
              "      <td>False</td>\n",
              "      <td>0.950431</td>\n",
              "      <td>0.923171</td>\n",
              "      <td>-0.002911</td>\n",
              "      <td>-0.009775</td>\n",
              "      <td>0.063504</td>\n",
              "      <td>0.064415</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.003095</td>\n",
              "      <td>0.003095</td>\n",
              "      <td>0.003095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-20</th>\n",
              "      <td>0.934667</td>\n",
              "      <td>0.036442</td>\n",
              "      <td>-0.003365</td>\n",
              "      <td>-0.005385</td>\n",
              "      <td>-0.005762</td>\n",
              "      <td>False</td>\n",
              "      <td>0.950018</td>\n",
              "      <td>0.923552</td>\n",
              "      <td>-0.003156</td>\n",
              "      <td>-0.010513</td>\n",
              "      <td>0.062047</td>\n",
              "      <td>0.064140</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.003365</td>\n",
              "      <td>0.003365</td>\n",
              "      <td>0.003365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-23</th>\n",
              "      <td>0.929282</td>\n",
              "      <td>0.037652</td>\n",
              "      <td>-0.005762</td>\n",
              "      <td>0.002685</td>\n",
              "      <td>0.002889</td>\n",
              "      <td>True</td>\n",
              "      <td>0.949648</td>\n",
              "      <td>0.923871</td>\n",
              "      <td>-0.005385</td>\n",
              "      <td>-0.013225</td>\n",
              "      <td>0.060675</td>\n",
              "      <td>0.063988</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.005762</td>\n",
              "      <td>0.005762</td>\n",
              "      <td>0.005762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-24</th>\n",
              "      <td>0.931966</td>\n",
              "      <td>0.038307</td>\n",
              "      <td>0.002889</td>\n",
              "      <td>-0.001214</td>\n",
              "      <td>-0.001303</td>\n",
              "      <td>True</td>\n",
              "      <td>0.949341</td>\n",
              "      <td>0.924304</td>\n",
              "      <td>0.002685</td>\n",
              "      <td>-0.007795</td>\n",
              "      <td>0.060226</td>\n",
              "      <td>0.064062</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.002889</td>\n",
              "      <td>-0.002889</td>\n",
              "      <td>0.002889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-25</th>\n",
              "      <td>0.930752</td>\n",
              "      <td>0.054899</td>\n",
              "      <td>-0.001303</td>\n",
              "      <td>0.005578</td>\n",
              "      <td>0.005993</td>\n",
              "      <td>True</td>\n",
              "      <td>0.948725</td>\n",
              "      <td>0.924712</td>\n",
              "      <td>-0.001214</td>\n",
              "      <td>-0.012644</td>\n",
              "      <td>0.059850</td>\n",
              "      <td>0.064147</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.001303</td>\n",
              "      <td>0.001303</td>\n",
              "      <td>-0.001303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-26</th>\n",
              "      <td>0.936330</td>\n",
              "      <td>0.040985</td>\n",
              "      <td>0.005993</td>\n",
              "      <td>-0.001226</td>\n",
              "      <td>-0.001309</td>\n",
              "      <td>False</td>\n",
              "      <td>0.947922</td>\n",
              "      <td>0.925193</td>\n",
              "      <td>0.005578</td>\n",
              "      <td>0.002797</td>\n",
              "      <td>0.057834</td>\n",
              "      <td>0.064095</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.005993</td>\n",
              "      <td>-0.005993</td>\n",
              "      <td>0.005993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-27</th>\n",
              "      <td>0.935104</td>\n",
              "      <td>0.076645</td>\n",
              "      <td>-0.001309</td>\n",
              "      <td>0.000175</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>False</td>\n",
              "      <td>0.947185</td>\n",
              "      <td>0.925638</td>\n",
              "      <td>-0.001226</td>\n",
              "      <td>-0.005630</td>\n",
              "      <td>0.058025</td>\n",
              "      <td>0.064731</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.001309</td>\n",
              "      <td>0.001309</td>\n",
              "      <td>0.001309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-30</th>\n",
              "      <td>0.935279</td>\n",
              "      <td>0.022058</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>-0.008924</td>\n",
              "      <td>-0.009541</td>\n",
              "      <td>False</td>\n",
              "      <td>0.946316</td>\n",
              "      <td>0.926088</td>\n",
              "      <td>0.000175</td>\n",
              "      <td>-0.002544</td>\n",
              "      <td>0.056695</td>\n",
              "      <td>0.064603</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>-0.000187</td>\n",
              "      <td>-0.000187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-31</th>\n",
              "      <td>0.926355</td>\n",
              "      <td>0.031282</td>\n",
              "      <td>-0.009541</td>\n",
              "      <td>0.002409</td>\n",
              "      <td>0.002601</td>\n",
              "      <td>True</td>\n",
              "      <td>0.945097</td>\n",
              "      <td>0.926436</td>\n",
              "      <td>-0.008924</td>\n",
              "      <td>-0.008312</td>\n",
              "      <td>0.054650</td>\n",
              "      <td>0.064372</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.009541</td>\n",
              "      <td>0.009541</td>\n",
              "      <td>0.009541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-02-01</th>\n",
              "      <td>0.928764</td>\n",
              "      <td>0.039968</td>\n",
              "      <td>0.002601</td>\n",
              "      <td>0.000777</td>\n",
              "      <td>0.000837</td>\n",
              "      <td>True</td>\n",
              "      <td>0.944075</td>\n",
              "      <td>0.926834</td>\n",
              "      <td>0.002409</td>\n",
              "      <td>-0.000518</td>\n",
              "      <td>0.054057</td>\n",
              "      <td>0.064102</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.002601</td>\n",
              "      <td>-0.002601</td>\n",
              "      <td>0.002601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-02-02</th>\n",
              "      <td>0.929541</td>\n",
              "      <td>0.026518</td>\n",
              "      <td>0.000837</td>\n",
              "      <td>-0.001983</td>\n",
              "      <td>-0.002133</td>\n",
              "      <td>True</td>\n",
              "      <td>0.943116</td>\n",
              "      <td>0.927234</td>\n",
              "      <td>0.000777</td>\n",
              "      <td>-0.002426</td>\n",
              "      <td>0.053253</td>\n",
              "      <td>0.063772</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000837</td>\n",
              "      <td>-0.000837</td>\n",
              "      <td>0.000837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-02-03</th>\n",
              "      <td>0.927558</td>\n",
              "      <td>0.041529</td>\n",
              "      <td>-0.002133</td>\n",
              "      <td>0.002761</td>\n",
              "      <td>0.002977</td>\n",
              "      <td>True</td>\n",
              "      <td>0.942149</td>\n",
              "      <td>0.927543</td>\n",
              "      <td>-0.001983</td>\n",
              "      <td>-0.003194</td>\n",
              "      <td>0.053013</td>\n",
              "      <td>0.063526</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.002133</td>\n",
              "      <td>0.002133</td>\n",
              "      <td>-0.002133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-02-06</th>\n",
              "      <td>0.930319</td>\n",
              "      <td>0.028982</td>\n",
              "      <td>0.002977</td>\n",
              "      <td>0.006010</td>\n",
              "      <td>0.006461</td>\n",
              "      <td>True</td>\n",
              "      <td>0.941271</td>\n",
              "      <td>0.927896</td>\n",
              "      <td>0.002761</td>\n",
              "      <td>-0.006010</td>\n",
              "      <td>0.052926</td>\n",
              "      <td>0.063153</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.002977</td>\n",
              "      <td>-0.002977</td>\n",
              "      <td>0.002977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-02-07</th>\n",
              "      <td>0.936330</td>\n",
              "      <td>0.056471</td>\n",
              "      <td>0.006461</td>\n",
              "      <td>-0.001488</td>\n",
              "      <td>-0.001589</td>\n",
              "      <td>True</td>\n",
              "      <td>0.940599</td>\n",
              "      <td>0.928292</td>\n",
              "      <td>0.006010</td>\n",
              "      <td>0.001226</td>\n",
              "      <td>0.052020</td>\n",
              "      <td>0.062957</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.006461</td>\n",
              "      <td>0.006461</td>\n",
              "      <td>0.006461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-02-08</th>\n",
              "      <td>0.934842</td>\n",
              "      <td>0.038375</td>\n",
              "      <td>-0.001589</td>\n",
              "      <td>0.003773</td>\n",
              "      <td>0.004036</td>\n",
              "      <td>True</td>\n",
              "      <td>0.939731</td>\n",
              "      <td>0.928700</td>\n",
              "      <td>-0.001488</td>\n",
              "      <td>-0.000437</td>\n",
              "      <td>0.049903</td>\n",
              "      <td>0.062438</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.001589</td>\n",
              "      <td>-0.001589</td>\n",
              "      <td>-0.001589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-02-09</th>\n",
              "      <td>0.938615</td>\n",
              "      <td>0.051965</td>\n",
              "      <td>0.004036</td>\n",
              "      <td>0.001412</td>\n",
              "      <td>0.001504</td>\n",
              "      <td>True</td>\n",
              "      <td>0.939242</td>\n",
              "      <td>0.929163</td>\n",
              "      <td>0.003773</td>\n",
              "      <td>0.012260</td>\n",
              "      <td>0.049062</td>\n",
              "      <td>0.061897</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.004036</td>\n",
              "      <td>0.004036</td>\n",
              "      <td>0.004036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-02-10</th>\n",
              "      <td>0.940026</td>\n",
              "      <td>0.085581</td>\n",
              "      <td>0.001504</td>\n",
              "      <td>0.003637</td>\n",
              "      <td>0.003869</td>\n",
              "      <td>True</td>\n",
              "      <td>0.938869</td>\n",
              "      <td>0.929653</td>\n",
              "      <td>0.001412</td>\n",
              "      <td>0.011263</td>\n",
              "      <td>0.050080</td>\n",
              "      <td>0.062181</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.001504</td>\n",
              "      <td>0.001504</td>\n",
              "      <td>0.001504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-11-21</th>\n",
              "      <td>0.852006</td>\n",
              "      <td>0.032757</td>\n",
              "      <td>-0.000426</td>\n",
              "      <td>-0.005983</td>\n",
              "      <td>-0.007022</td>\n",
              "      <td>False</td>\n",
              "      <td>0.853261</td>\n",
              "      <td>0.851177</td>\n",
              "      <td>-0.000363</td>\n",
              "      <td>-0.005406</td>\n",
              "      <td>0.046575</td>\n",
              "      <td>0.046909</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.000426</td>\n",
              "      <td>0.000426</td>\n",
              "      <td>0.000426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-11-22</th>\n",
              "      <td>0.846024</td>\n",
              "      <td>0.022044</td>\n",
              "      <td>-0.007022</td>\n",
              "      <td>-0.002142</td>\n",
              "      <td>-0.002532</td>\n",
              "      <td>False</td>\n",
              "      <td>0.853349</td>\n",
              "      <td>0.850828</td>\n",
              "      <td>-0.005983</td>\n",
              "      <td>-0.011242</td>\n",
              "      <td>0.046114</td>\n",
              "      <td>0.046822</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.007022</td>\n",
              "      <td>0.007022</td>\n",
              "      <td>0.007022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-11-23</th>\n",
              "      <td>0.843882</td>\n",
              "      <td>0.018941</td>\n",
              "      <td>-0.002532</td>\n",
              "      <td>-0.005659</td>\n",
              "      <td>-0.006706</td>\n",
              "      <td>True</td>\n",
              "      <td>0.853299</td>\n",
              "      <td>0.850512</td>\n",
              "      <td>-0.002142</td>\n",
              "      <td>-0.003863</td>\n",
              "      <td>0.045806</td>\n",
              "      <td>0.046652</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.002532</td>\n",
              "      <td>0.002532</td>\n",
              "      <td>0.002532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-11-24</th>\n",
              "      <td>0.838223</td>\n",
              "      <td>0.030054</td>\n",
              "      <td>-0.006706</td>\n",
              "      <td>0.002396</td>\n",
              "      <td>0.002858</td>\n",
              "      <td>True</td>\n",
              "      <td>0.853044</td>\n",
              "      <td>0.850122</td>\n",
              "      <td>-0.005659</td>\n",
              "      <td>-0.009882</td>\n",
              "      <td>0.045548</td>\n",
              "      <td>0.046685</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.006706</td>\n",
              "      <td>0.006706</td>\n",
              "      <td>-0.006706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-11-27</th>\n",
              "      <td>0.840619</td>\n",
              "      <td>0.032492</td>\n",
              "      <td>0.002858</td>\n",
              "      <td>0.004047</td>\n",
              "      <td>0.004815</td>\n",
              "      <td>False</td>\n",
              "      <td>0.852804</td>\n",
              "      <td>0.849755</td>\n",
              "      <td>0.002396</td>\n",
              "      <td>-0.009071</td>\n",
              "      <td>0.045056</td>\n",
              "      <td>0.046735</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.002858</td>\n",
              "      <td>-0.002858</td>\n",
              "      <td>0.002858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-11-28</th>\n",
              "      <td>0.844666</td>\n",
              "      <td>0.044063</td>\n",
              "      <td>0.004815</td>\n",
              "      <td>-0.000570</td>\n",
              "      <td>-0.000675</td>\n",
              "      <td>False</td>\n",
              "      <td>0.852629</td>\n",
              "      <td>0.849480</td>\n",
              "      <td>0.004047</td>\n",
              "      <td>-0.003295</td>\n",
              "      <td>0.044239</td>\n",
              "      <td>0.046910</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.004815</td>\n",
              "      <td>-0.004815</td>\n",
              "      <td>-0.004815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-11-29</th>\n",
              "      <td>0.844096</td>\n",
              "      <td>0.060237</td>\n",
              "      <td>-0.000675</td>\n",
              "      <td>-0.003901</td>\n",
              "      <td>-0.004621</td>\n",
              "      <td>False</td>\n",
              "      <td>0.852486</td>\n",
              "      <td>0.849157</td>\n",
              "      <td>-0.000570</td>\n",
              "      <td>-0.008274</td>\n",
              "      <td>0.044113</td>\n",
              "      <td>0.047140</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.000675</td>\n",
              "      <td>0.000675</td>\n",
              "      <td>0.000675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-11-30</th>\n",
              "      <td>0.840195</td>\n",
              "      <td>0.053413</td>\n",
              "      <td>-0.004621</td>\n",
              "      <td>0.000919</td>\n",
              "      <td>0.001093</td>\n",
              "      <td>True</td>\n",
              "      <td>0.852363</td>\n",
              "      <td>0.848783</td>\n",
              "      <td>-0.003901</td>\n",
              "      <td>-0.011812</td>\n",
              "      <td>0.043694</td>\n",
              "      <td>0.047326</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.004621</td>\n",
              "      <td>0.004621</td>\n",
              "      <td>0.004621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-01</th>\n",
              "      <td>0.841114</td>\n",
              "      <td>0.049116</td>\n",
              "      <td>0.001093</td>\n",
              "      <td>0.001772</td>\n",
              "      <td>0.002107</td>\n",
              "      <td>True</td>\n",
              "      <td>0.852111</td>\n",
              "      <td>0.848473</td>\n",
              "      <td>0.000919</td>\n",
              "      <td>-0.004910</td>\n",
              "      <td>0.042181</td>\n",
              "      <td>0.047390</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.001093</td>\n",
              "      <td>-0.001093</td>\n",
              "      <td>0.001093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-04</th>\n",
              "      <td>0.842886</td>\n",
              "      <td>0.090952</td>\n",
              "      <td>0.002107</td>\n",
              "      <td>0.002851</td>\n",
              "      <td>0.003383</td>\n",
              "      <td>True</td>\n",
              "      <td>0.851831</td>\n",
              "      <td>0.848190</td>\n",
              "      <td>0.001772</td>\n",
              "      <td>-0.000996</td>\n",
              "      <td>0.042438</td>\n",
              "      <td>0.047974</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.002107</td>\n",
              "      <td>-0.002107</td>\n",
              "      <td>0.002107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-05</th>\n",
              "      <td>0.845737</td>\n",
              "      <td>0.072858</td>\n",
              "      <td>0.003383</td>\n",
              "      <td>0.002079</td>\n",
              "      <td>0.002459</td>\n",
              "      <td>True</td>\n",
              "      <td>0.851675</td>\n",
              "      <td>0.847991</td>\n",
              "      <td>0.002851</td>\n",
              "      <td>0.007515</td>\n",
              "      <td>0.043048</td>\n",
              "      <td>0.048544</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.003383</td>\n",
              "      <td>-0.003383</td>\n",
              "      <td>0.003383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-06</th>\n",
              "      <td>0.847817</td>\n",
              "      <td>0.033286</td>\n",
              "      <td>0.002459</td>\n",
              "      <td>0.001729</td>\n",
              "      <td>0.002039</td>\n",
              "      <td>True</td>\n",
              "      <td>0.851716</td>\n",
              "      <td>0.847784</td>\n",
              "      <td>0.002079</td>\n",
              "      <td>0.007198</td>\n",
              "      <td>0.042090</td>\n",
              "      <td>0.048541</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.002459</td>\n",
              "      <td>-0.002459</td>\n",
              "      <td>0.002459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-07</th>\n",
              "      <td>0.849545</td>\n",
              "      <td>0.065278</td>\n",
              "      <td>0.002039</td>\n",
              "      <td>0.000506</td>\n",
              "      <td>0.000595</td>\n",
              "      <td>True</td>\n",
              "      <td>0.851422</td>\n",
              "      <td>0.847680</td>\n",
              "      <td>0.001729</td>\n",
              "      <td>0.004880</td>\n",
              "      <td>0.042056</td>\n",
              "      <td>0.048816</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.002039</td>\n",
              "      <td>-0.002039</td>\n",
              "      <td>0.002039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-08</th>\n",
              "      <td>0.850051</td>\n",
              "      <td>0.085969</td>\n",
              "      <td>0.000595</td>\n",
              "      <td>-0.000289</td>\n",
              "      <td>-0.000340</td>\n",
              "      <td>False</td>\n",
              "      <td>0.851041</td>\n",
              "      <td>0.847605</td>\n",
              "      <td>0.000506</td>\n",
              "      <td>0.005955</td>\n",
              "      <td>0.042609</td>\n",
              "      <td>0.049215</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.000595</td>\n",
              "      <td>-0.000595</td>\n",
              "      <td>0.000595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-11</th>\n",
              "      <td>0.849762</td>\n",
              "      <td>0.036393</td>\n",
              "      <td>-0.000340</td>\n",
              "      <td>0.002027</td>\n",
              "      <td>0.002385</td>\n",
              "      <td>False</td>\n",
              "      <td>0.850752</td>\n",
              "      <td>0.847510</td>\n",
              "      <td>-0.000289</td>\n",
              "      <td>0.009567</td>\n",
              "      <td>0.042542</td>\n",
              "      <td>0.048944</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.000340</td>\n",
              "      <td>0.000340</td>\n",
              "      <td>0.000340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-12</th>\n",
              "      <td>0.851789</td>\n",
              "      <td>0.081947</td>\n",
              "      <td>0.002385</td>\n",
              "      <td>-0.006123</td>\n",
              "      <td>-0.007188</td>\n",
              "      <td>False</td>\n",
              "      <td>0.850517</td>\n",
              "      <td>0.847445</td>\n",
              "      <td>0.002027</td>\n",
              "      <td>0.010675</td>\n",
              "      <td>0.044133</td>\n",
              "      <td>0.049259</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.002385</td>\n",
              "      <td>-0.002385</td>\n",
              "      <td>-0.002385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-13</th>\n",
              "      <td>0.845666</td>\n",
              "      <td>0.032359</td>\n",
              "      <td>-0.007188</td>\n",
              "      <td>0.003447</td>\n",
              "      <td>0.004076</td>\n",
              "      <td>True</td>\n",
              "      <td>0.850013</td>\n",
              "      <td>0.847378</td>\n",
              "      <td>-0.006123</td>\n",
              "      <td>0.002780</td>\n",
              "      <td>0.042946</td>\n",
              "      <td>0.049144</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.007188</td>\n",
              "      <td>0.007188</td>\n",
              "      <td>0.007188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-14</th>\n",
              "      <td>0.849113</td>\n",
              "      <td>0.040492</td>\n",
              "      <td>0.004076</td>\n",
              "      <td>0.001806</td>\n",
              "      <td>0.002127</td>\n",
              "      <td>False</td>\n",
              "      <td>0.849719</td>\n",
              "      <td>0.847304</td>\n",
              "      <td>0.003447</td>\n",
              "      <td>0.003375</td>\n",
              "      <td>0.043390</td>\n",
              "      <td>0.049264</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.004076</td>\n",
              "      <td>-0.004076</td>\n",
              "      <td>0.004076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-15</th>\n",
              "      <td>0.850919</td>\n",
              "      <td>0.047422</td>\n",
              "      <td>0.002127</td>\n",
              "      <td>-0.002095</td>\n",
              "      <td>-0.002462</td>\n",
              "      <td>False</td>\n",
              "      <td>0.849365</td>\n",
              "      <td>0.847303</td>\n",
              "      <td>0.001806</td>\n",
              "      <td>0.003102</td>\n",
              "      <td>0.043575</td>\n",
              "      <td>0.049625</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.002127</td>\n",
              "      <td>-0.002127</td>\n",
              "      <td>-0.002127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-18</th>\n",
              "      <td>0.848824</td>\n",
              "      <td>0.046910</td>\n",
              "      <td>-0.002462</td>\n",
              "      <td>-0.004087</td>\n",
              "      <td>-0.004815</td>\n",
              "      <td>False</td>\n",
              "      <td>0.848945</td>\n",
              "      <td>0.847345</td>\n",
              "      <td>-0.002095</td>\n",
              "      <td>-0.000721</td>\n",
              "      <td>0.042431</td>\n",
              "      <td>0.049620</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.002462</td>\n",
              "      <td>0.002462</td>\n",
              "      <td>0.002462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-19</th>\n",
              "      <td>0.844737</td>\n",
              "      <td>0.044138</td>\n",
              "      <td>-0.004815</td>\n",
              "      <td>-0.002206</td>\n",
              "      <td>-0.002612</td>\n",
              "      <td>False</td>\n",
              "      <td>0.848330</td>\n",
              "      <td>0.847319</td>\n",
              "      <td>-0.004087</td>\n",
              "      <td>-0.005314</td>\n",
              "      <td>0.041673</td>\n",
              "      <td>0.049359</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.004815</td>\n",
              "      <td>0.004815</td>\n",
              "      <td>0.004815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-20</th>\n",
              "      <td>0.842531</td>\n",
              "      <td>0.065079</td>\n",
              "      <td>-0.002612</td>\n",
              "      <td>-0.000284</td>\n",
              "      <td>-0.000337</td>\n",
              "      <td>True</td>\n",
              "      <td>0.847662</td>\n",
              "      <td>0.847308</td>\n",
              "      <td>-0.002206</td>\n",
              "      <td>-0.007231</td>\n",
              "      <td>0.043813</td>\n",
              "      <td>0.049362</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.002612</td>\n",
              "      <td>0.002612</td>\n",
              "      <td>0.002612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-21</th>\n",
              "      <td>0.842247</td>\n",
              "      <td>0.053057</td>\n",
              "      <td>-0.000337</td>\n",
              "      <td>0.001137</td>\n",
              "      <td>0.001349</td>\n",
              "      <td>True</td>\n",
              "      <td>0.847100</td>\n",
              "      <td>0.847305</td>\n",
              "      <td>-0.000284</td>\n",
              "      <td>-0.009542</td>\n",
              "      <td>0.045579</td>\n",
              "      <td>0.049593</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.000337</td>\n",
              "      <td>0.000337</td>\n",
              "      <td>-0.000337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-22</th>\n",
              "      <td>0.843384</td>\n",
              "      <td>0.050513</td>\n",
              "      <td>0.001349</td>\n",
              "      <td>-0.000782</td>\n",
              "      <td>-0.000927</td>\n",
              "      <td>False</td>\n",
              "      <td>0.846632</td>\n",
              "      <td>0.847241</td>\n",
              "      <td>0.001137</td>\n",
              "      <td>-0.002282</td>\n",
              "      <td>0.046792</td>\n",
              "      <td>0.049929</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.001349</td>\n",
              "      <td>-0.001349</td>\n",
              "      <td>0.001349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-25</th>\n",
              "      <td>0.842602</td>\n",
              "      <td>0.055977</td>\n",
              "      <td>-0.000927</td>\n",
              "      <td>0.000711</td>\n",
              "      <td>0.000843</td>\n",
              "      <td>False</td>\n",
              "      <td>0.846143</td>\n",
              "      <td>0.847187</td>\n",
              "      <td>-0.000782</td>\n",
              "      <td>-0.006511</td>\n",
              "      <td>0.048229</td>\n",
              "      <td>0.050399</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.000927</td>\n",
              "      <td>0.000927</td>\n",
              "      <td>0.000927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-26</th>\n",
              "      <td>0.843313</td>\n",
              "      <td>0.053010</td>\n",
              "      <td>0.000843</td>\n",
              "      <td>-0.001987</td>\n",
              "      <td>-0.002356</td>\n",
              "      <td>False</td>\n",
              "      <td>0.845996</td>\n",
              "      <td>0.847109</td>\n",
              "      <td>0.000711</td>\n",
              "      <td>-0.007606</td>\n",
              "      <td>0.048635</td>\n",
              "      <td>0.050599</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.000843</td>\n",
              "      <td>-0.000843</td>\n",
              "      <td>-0.000843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-27</th>\n",
              "      <td>0.841326</td>\n",
              "      <td>0.039179</td>\n",
              "      <td>-0.002356</td>\n",
              "      <td>-0.003875</td>\n",
              "      <td>-0.004606</td>\n",
              "      <td>False</td>\n",
              "      <td>0.845770</td>\n",
              "      <td>0.847017</td>\n",
              "      <td>-0.001987</td>\n",
              "      <td>-0.007498</td>\n",
              "      <td>0.048731</td>\n",
              "      <td>0.050775</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.002356</td>\n",
              "      <td>0.002356</td>\n",
              "      <td>0.002356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-28</th>\n",
              "      <td>0.837451</td>\n",
              "      <td>0.041405</td>\n",
              "      <td>-0.004606</td>\n",
              "      <td>-0.003840</td>\n",
              "      <td>-0.004585</td>\n",
              "      <td>False</td>\n",
              "      <td>0.845362</td>\n",
              "      <td>0.846895</td>\n",
              "      <td>-0.003875</td>\n",
              "      <td>-0.007286</td>\n",
              "      <td>0.048284</td>\n",
              "      <td>0.050967</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.004606</td>\n",
              "      <td>0.004606</td>\n",
              "      <td>0.004606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-29</th>\n",
              "      <td>0.833611</td>\n",
              "      <td>0.033295</td>\n",
              "      <td>-0.004585</td>\n",
              "      <td>-0.000833</td>\n",
              "      <td>-0.000999</td>\n",
              "      <td>False</td>\n",
              "      <td>0.844883</td>\n",
              "      <td>0.846771</td>\n",
              "      <td>-0.003840</td>\n",
              "      <td>-0.008920</td>\n",
              "      <td>0.048487</td>\n",
              "      <td>0.050887</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.004585</td>\n",
              "      <td>0.004585</td>\n",
              "      <td>0.004585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01</th>\n",
              "      <td>0.832778</td>\n",
              "      <td>0.051245</td>\n",
              "      <td>-0.000999</td>\n",
              "      <td>-0.003384</td>\n",
              "      <td>-0.004064</td>\n",
              "      <td>False</td>\n",
              "      <td>0.844230</td>\n",
              "      <td>0.846608</td>\n",
              "      <td>-0.000833</td>\n",
              "      <td>-0.009469</td>\n",
              "      <td>0.048796</td>\n",
              "      <td>0.051115</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.000999</td>\n",
              "      <td>0.000999</td>\n",
              "      <td>0.000999</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>261 rows × 18 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               Close  marketRisk   returns    Target    Change  Signal  \\\n",
              "Date                                                                     \n",
              "2017-01-02  0.956297    0.074303  0.005355  0.004872  0.005094   False   \n",
              "2017-01-03  0.961169    0.101960  0.005094 -0.007516 -0.007820   False   \n",
              "2017-01-04  0.953652    0.058255 -0.007820 -0.010523 -0.011035   False   \n",
              "2017-01-05  0.943129    0.049752 -0.011035  0.006538  0.006933    True   \n",
              "2017-01-06  0.949668    0.064650  0.006933 -0.003773 -0.003973   False   \n",
              "2017-01-09  0.945895    0.059655 -0.003973  0.001703  0.001800   False   \n",
              "2017-01-10  0.947598    0.033261  0.001800 -0.002418 -0.002552   False   \n",
              "2017-01-11  0.945180    0.062518 -0.002552 -0.002673 -0.002828   False   \n",
              "2017-01-12  0.942507    0.036572 -0.002828 -0.002746 -0.002913   False   \n",
              "2017-01-13  0.939761    0.053134 -0.002913  0.003635  0.003868    True   \n",
              "2017-01-16  0.943396    0.043191  0.003868 -0.009864 -0.010456   False   \n",
              "2017-01-17  0.933532    0.039432 -0.010456  0.007201  0.007714    True   \n",
              "2017-01-18  0.940734    0.050054  0.007714 -0.002911 -0.003095   False   \n",
              "2017-01-19  0.937822    0.068003 -0.003095 -0.003156 -0.003365   False   \n",
              "2017-01-20  0.934667    0.036442 -0.003365 -0.005385 -0.005762   False   \n",
              "2017-01-23  0.929282    0.037652 -0.005762  0.002685  0.002889    True   \n",
              "2017-01-24  0.931966    0.038307  0.002889 -0.001214 -0.001303    True   \n",
              "2017-01-25  0.930752    0.054899 -0.001303  0.005578  0.005993    True   \n",
              "2017-01-26  0.936330    0.040985  0.005993 -0.001226 -0.001309   False   \n",
              "2017-01-27  0.935104    0.076645 -0.001309  0.000175  0.000187   False   \n",
              "2017-01-30  0.935279    0.022058  0.000187 -0.008924 -0.009541   False   \n",
              "2017-01-31  0.926355    0.031282 -0.009541  0.002409  0.002601    True   \n",
              "2017-02-01  0.928764    0.039968  0.002601  0.000777  0.000837    True   \n",
              "2017-02-02  0.929541    0.026518  0.000837 -0.001983 -0.002133    True   \n",
              "2017-02-03  0.927558    0.041529 -0.002133  0.002761  0.002977    True   \n",
              "2017-02-06  0.930319    0.028982  0.002977  0.006010  0.006461    True   \n",
              "2017-02-07  0.936330    0.056471  0.006461 -0.001488 -0.001589    True   \n",
              "2017-02-08  0.934842    0.038375 -0.001589  0.003773  0.004036    True   \n",
              "2017-02-09  0.938615    0.051965  0.004036  0.001412  0.001504    True   \n",
              "2017-02-10  0.940026    0.085581  0.001504  0.003637  0.003869    True   \n",
              "...              ...         ...       ...       ...       ...     ...   \n",
              "2017-11-21  0.852006    0.032757 -0.000426 -0.005983 -0.007022   False   \n",
              "2017-11-22  0.846024    0.022044 -0.007022 -0.002142 -0.002532   False   \n",
              "2017-11-23  0.843882    0.018941 -0.002532 -0.005659 -0.006706    True   \n",
              "2017-11-24  0.838223    0.030054 -0.006706  0.002396  0.002858    True   \n",
              "2017-11-27  0.840619    0.032492  0.002858  0.004047  0.004815   False   \n",
              "2017-11-28  0.844666    0.044063  0.004815 -0.000570 -0.000675   False   \n",
              "2017-11-29  0.844096    0.060237 -0.000675 -0.003901 -0.004621   False   \n",
              "2017-11-30  0.840195    0.053413 -0.004621  0.000919  0.001093    True   \n",
              "2017-12-01  0.841114    0.049116  0.001093  0.001772  0.002107    True   \n",
              "2017-12-04  0.842886    0.090952  0.002107  0.002851  0.003383    True   \n",
              "2017-12-05  0.845737    0.072858  0.003383  0.002079  0.002459    True   \n",
              "2017-12-06  0.847817    0.033286  0.002459  0.001729  0.002039    True   \n",
              "2017-12-07  0.849545    0.065278  0.002039  0.000506  0.000595    True   \n",
              "2017-12-08  0.850051    0.085969  0.000595 -0.000289 -0.000340   False   \n",
              "2017-12-11  0.849762    0.036393 -0.000340  0.002027  0.002385   False   \n",
              "2017-12-12  0.851789    0.081947  0.002385 -0.006123 -0.007188   False   \n",
              "2017-12-13  0.845666    0.032359 -0.007188  0.003447  0.004076    True   \n",
              "2017-12-14  0.849113    0.040492  0.004076  0.001806  0.002127   False   \n",
              "2017-12-15  0.850919    0.047422  0.002127 -0.002095 -0.002462   False   \n",
              "2017-12-18  0.848824    0.046910 -0.002462 -0.004087 -0.004815   False   \n",
              "2017-12-19  0.844737    0.044138 -0.004815 -0.002206 -0.002612   False   \n",
              "2017-12-20  0.842531    0.065079 -0.002612 -0.000284 -0.000337    True   \n",
              "2017-12-21  0.842247    0.053057 -0.000337  0.001137  0.001349    True   \n",
              "2017-12-22  0.843384    0.050513  0.001349 -0.000782 -0.000927   False   \n",
              "2017-12-25  0.842602    0.055977 -0.000927  0.000711  0.000843   False   \n",
              "2017-12-26  0.843313    0.053010  0.000843 -0.001987 -0.002356   False   \n",
              "2017-12-27  0.841326    0.039179 -0.002356 -0.003875 -0.004606   False   \n",
              "2017-12-28  0.837451    0.041405 -0.004606 -0.003840 -0.004585   False   \n",
              "2017-12-29  0.833611    0.033295 -0.004585 -0.000833 -0.000999   False   \n",
              "2018-01-01  0.832778    0.051245 -0.000999 -0.003384 -0.004064   False   \n",
              "\n",
              "             Close30  Close100        r1        r2  marketrisk_avg30  \\\n",
              "Date                                                                   \n",
              "2017-01-02  0.948200  0.915872  0.005094 -0.002016          0.066802   \n",
              "2017-01-03  0.948863  0.916616  0.004872  0.004597          0.068402   \n",
              "2017-01-04  0.949074  0.917292 -0.007516 -0.003011          0.067669   \n",
              "2017-01-05  0.948922  0.917914 -0.010523 -0.013351          0.066586   \n",
              "2017-01-06  0.949083  0.918581  0.006538 -0.011224          0.066382   \n",
              "2017-01-09  0.949205  0.919202 -0.003773 -0.007394          0.066756   \n",
              "2017-01-10  0.949484  0.919830  0.001703 -0.003605          0.065723   \n",
              "2017-01-11  0.949499  0.920402 -0.002418 -0.011118          0.065729   \n",
              "2017-01-12  0.949646  0.920961 -0.002673 -0.018662          0.064580   \n",
              "2017-01-13  0.949731  0.921426 -0.002746 -0.013891          0.064965   \n",
              "2017-01-16  0.950205  0.921922  0.003635  0.000267          0.064260   \n",
              "2017-01-17  0.950219  0.922281 -0.009864 -0.016135          0.063088   \n",
              "2017-01-18  0.950572  0.922725  0.007201 -0.005161          0.062997   \n",
              "2017-01-19  0.950431  0.923171 -0.002911 -0.009775          0.063504   \n",
              "2017-01-20  0.950018  0.923552 -0.003156 -0.010513          0.062047   \n",
              "2017-01-23  0.949648  0.923871 -0.005385 -0.013225          0.060675   \n",
              "2017-01-24  0.949341  0.924304  0.002685 -0.007795          0.060226   \n",
              "2017-01-25  0.948725  0.924712 -0.001214 -0.012644          0.059850   \n",
              "2017-01-26  0.947922  0.925193  0.005578  0.002797          0.057834   \n",
              "2017-01-27  0.947185  0.925638 -0.001226 -0.005630          0.058025   \n",
              "2017-01-30  0.946316  0.926088  0.000175 -0.002544          0.056695   \n",
              "2017-01-31  0.945097  0.926436 -0.008924 -0.008312          0.054650   \n",
              "2017-02-01  0.944075  0.926834  0.002409 -0.000518          0.054057   \n",
              "2017-02-02  0.943116  0.927234  0.000777 -0.002426          0.053253   \n",
              "2017-02-03  0.942149  0.927543 -0.001983 -0.003194          0.053013   \n",
              "2017-02-06  0.941271  0.927896  0.002761 -0.006010          0.052926   \n",
              "2017-02-07  0.940599  0.928292  0.006010  0.001226          0.052020   \n",
              "2017-02-08  0.939731  0.928700 -0.001488 -0.000437          0.049903   \n",
              "2017-02-09  0.939242  0.929163  0.003773  0.012260          0.049062   \n",
              "2017-02-10  0.938869  0.929653  0.001412  0.011263          0.050080   \n",
              "...              ...       ...       ...       ...               ...   \n",
              "2017-11-21  0.853261  0.851177 -0.000363 -0.005406          0.046575   \n",
              "2017-11-22  0.853349  0.850828 -0.005983 -0.011242          0.046114   \n",
              "2017-11-23  0.853299  0.850512 -0.002142 -0.003863          0.045806   \n",
              "2017-11-24  0.853044  0.850122 -0.005659 -0.009882          0.045548   \n",
              "2017-11-27  0.852804  0.849755  0.002396 -0.009071          0.045056   \n",
              "2017-11-28  0.852629  0.849480  0.004047 -0.003295          0.044239   \n",
              "2017-11-29  0.852486  0.849157 -0.000570 -0.008274          0.044113   \n",
              "2017-11-30  0.852363  0.848783 -0.003901 -0.011812          0.043694   \n",
              "2017-12-01  0.852111  0.848473  0.000919 -0.004910          0.042181   \n",
              "2017-12-04  0.851831  0.848190  0.001772 -0.000996          0.042438   \n",
              "2017-12-05  0.851675  0.847991  0.002851  0.007515          0.043048   \n",
              "2017-12-06  0.851716  0.847784  0.002079  0.007198          0.042090   \n",
              "2017-12-07  0.851422  0.847680  0.001729  0.004880          0.042056   \n",
              "2017-12-08  0.851041  0.847605  0.000506  0.005955          0.042609   \n",
              "2017-12-11  0.850752  0.847510 -0.000289  0.009567          0.042542   \n",
              "2017-12-12  0.850517  0.847445  0.002027  0.010675          0.044133   \n",
              "2017-12-13  0.850013  0.847378 -0.006123  0.002780          0.042946   \n",
              "2017-12-14  0.849719  0.847304  0.003447  0.003375          0.043390   \n",
              "2017-12-15  0.849365  0.847303  0.001806  0.003102          0.043575   \n",
              "2017-12-18  0.848945  0.847345 -0.002095 -0.000721          0.042431   \n",
              "2017-12-19  0.848330  0.847319 -0.004087 -0.005314          0.041673   \n",
              "2017-12-20  0.847662  0.847308 -0.002206 -0.007231          0.043813   \n",
              "2017-12-21  0.847100  0.847305 -0.000284 -0.009542          0.045579   \n",
              "2017-12-22  0.846632  0.847241  0.001137 -0.002282          0.046792   \n",
              "2017-12-25  0.846143  0.847187 -0.000782 -0.006511          0.048229   \n",
              "2017-12-26  0.845996  0.847109  0.000711 -0.007606          0.048635   \n",
              "2017-12-27  0.845770  0.847017 -0.001987 -0.007498          0.048731   \n",
              "2017-12-28  0.845362  0.846895 -0.003875 -0.007286          0.048284   \n",
              "2017-12-29  0.844883  0.846771 -0.003840 -0.008920          0.048487   \n",
              "2018-01-01  0.844230  0.846608 -0.000833 -0.009469          0.048796   \n",
              "\n",
              "            marketrisk_avg90 predictSignal  predictedweight  labelledweight  \\\n",
              "Date                                                                          \n",
              "2017-01-02          0.065010          True                1              -1   \n",
              "2017-01-03          0.065185          True                1              -1   \n",
              "2017-01-04          0.064691          True                1              -1   \n",
              "2017-01-05          0.064512          True                1               1   \n",
              "2017-01-06          0.064718          True                1              -1   \n",
              "2017-01-09          0.064769         False               -1              -1   \n",
              "2017-01-10          0.064691         False               -1              -1   \n",
              "2017-01-11          0.064932          True                1              -1   \n",
              "2017-01-12          0.064800          True                1              -1   \n",
              "2017-01-13          0.064543          True                1               1   \n",
              "2017-01-16          0.064488         False               -1              -1   \n",
              "2017-01-17          0.064473         False               -1               1   \n",
              "2017-01-18          0.064406         False               -1              -1   \n",
              "2017-01-19          0.064415         False               -1              -1   \n",
              "2017-01-20          0.064140         False               -1              -1   \n",
              "2017-01-23          0.063988         False               -1               1   \n",
              "2017-01-24          0.064062         False               -1               1   \n",
              "2017-01-25          0.064147         False               -1               1   \n",
              "2017-01-26          0.064095         False               -1              -1   \n",
              "2017-01-27          0.064731         False               -1              -1   \n",
              "2017-01-30          0.064603         False               -1              -1   \n",
              "2017-01-31          0.064372         False               -1               1   \n",
              "2017-02-01          0.064102         False               -1               1   \n",
              "2017-02-02          0.063772         False               -1               1   \n",
              "2017-02-03          0.063526         False               -1               1   \n",
              "2017-02-06          0.063153          True                1               1   \n",
              "2017-02-07          0.062957          True                1               1   \n",
              "2017-02-08          0.062438          True                1               1   \n",
              "2017-02-09          0.061897          True                1               1   \n",
              "2017-02-10          0.062181          True                1               1   \n",
              "...                      ...           ...              ...             ...   \n",
              "2017-11-21          0.046909             0               -1              -1   \n",
              "2017-11-22          0.046822             0               -1              -1   \n",
              "2017-11-23          0.046652             0               -1               1   \n",
              "2017-11-24          0.046685             0               -1               1   \n",
              "2017-11-27          0.046735             0               -1              -1   \n",
              "2017-11-28          0.046910             0               -1              -1   \n",
              "2017-11-29          0.047140             0               -1              -1   \n",
              "2017-11-30          0.047326             0               -1               1   \n",
              "2017-12-01          0.047390             0               -1               1   \n",
              "2017-12-04          0.047974             0               -1               1   \n",
              "2017-12-05          0.048544             0               -1               1   \n",
              "2017-12-06          0.048541             0               -1               1   \n",
              "2017-12-07          0.048816             0               -1               1   \n",
              "2017-12-08          0.049215             0               -1              -1   \n",
              "2017-12-11          0.048944             0               -1              -1   \n",
              "2017-12-12          0.049259             0               -1              -1   \n",
              "2017-12-13          0.049144             0               -1               1   \n",
              "2017-12-14          0.049264             0               -1              -1   \n",
              "2017-12-15          0.049625             0               -1              -1   \n",
              "2017-12-18          0.049620             0               -1              -1   \n",
              "2017-12-19          0.049359             0               -1              -1   \n",
              "2017-12-20          0.049362             0               -1               1   \n",
              "2017-12-21          0.049593             0               -1               1   \n",
              "2017-12-22          0.049929             0               -1              -1   \n",
              "2017-12-25          0.050399             0               -1              -1   \n",
              "2017-12-26          0.050599             0               -1              -1   \n",
              "2017-12-27          0.050775             0               -1              -1   \n",
              "2017-12-28          0.050967             0               -1              -1   \n",
              "2017-12-29          0.050887             0               -1              -1   \n",
              "2018-01-01          0.051115             0               -1              -1   \n",
              "\n",
              "                   r  predictedreturns  labelledreturns  \n",
              "Date                                                     \n",
              "2017-01-02       NaN               NaN              NaN  \n",
              "2017-01-03  0.005094          0.005094        -0.005094  \n",
              "2017-01-04 -0.007820         -0.007820         0.007820  \n",
              "2017-01-05 -0.011035         -0.011035         0.011035  \n",
              "2017-01-06  0.006933          0.006933         0.006933  \n",
              "2017-01-09 -0.003973         -0.003973         0.003973  \n",
              "2017-01-10  0.001800         -0.001800        -0.001800  \n",
              "2017-01-11 -0.002552          0.002552         0.002552  \n",
              "2017-01-12 -0.002828         -0.002828         0.002828  \n",
              "2017-01-13 -0.002913         -0.002913         0.002913  \n",
              "2017-01-16  0.003868          0.003868         0.003868  \n",
              "2017-01-17 -0.010456          0.010456         0.010456  \n",
              "2017-01-18  0.007714         -0.007714         0.007714  \n",
              "2017-01-19 -0.003095          0.003095         0.003095  \n",
              "2017-01-20 -0.003365          0.003365         0.003365  \n",
              "2017-01-23 -0.005762          0.005762         0.005762  \n",
              "2017-01-24  0.002889         -0.002889         0.002889  \n",
              "2017-01-25 -0.001303          0.001303        -0.001303  \n",
              "2017-01-26  0.005993         -0.005993         0.005993  \n",
              "2017-01-27 -0.001309          0.001309         0.001309  \n",
              "2017-01-30  0.000187         -0.000187        -0.000187  \n",
              "2017-01-31 -0.009541          0.009541         0.009541  \n",
              "2017-02-01  0.002601         -0.002601         0.002601  \n",
              "2017-02-02  0.000837         -0.000837         0.000837  \n",
              "2017-02-03 -0.002133          0.002133        -0.002133  \n",
              "2017-02-06  0.002977         -0.002977         0.002977  \n",
              "2017-02-07  0.006461          0.006461         0.006461  \n",
              "2017-02-08 -0.001589         -0.001589        -0.001589  \n",
              "2017-02-09  0.004036          0.004036         0.004036  \n",
              "2017-02-10  0.001504          0.001504         0.001504  \n",
              "...              ...               ...              ...  \n",
              "2017-11-21 -0.000426          0.000426         0.000426  \n",
              "2017-11-22 -0.007022          0.007022         0.007022  \n",
              "2017-11-23 -0.002532          0.002532         0.002532  \n",
              "2017-11-24 -0.006706          0.006706        -0.006706  \n",
              "2017-11-27  0.002858         -0.002858         0.002858  \n",
              "2017-11-28  0.004815         -0.004815        -0.004815  \n",
              "2017-11-29 -0.000675          0.000675         0.000675  \n",
              "2017-11-30 -0.004621          0.004621         0.004621  \n",
              "2017-12-01  0.001093         -0.001093         0.001093  \n",
              "2017-12-04  0.002107         -0.002107         0.002107  \n",
              "2017-12-05  0.003383         -0.003383         0.003383  \n",
              "2017-12-06  0.002459         -0.002459         0.002459  \n",
              "2017-12-07  0.002039         -0.002039         0.002039  \n",
              "2017-12-08  0.000595         -0.000595         0.000595  \n",
              "2017-12-11 -0.000340          0.000340         0.000340  \n",
              "2017-12-12  0.002385         -0.002385        -0.002385  \n",
              "2017-12-13 -0.007188          0.007188         0.007188  \n",
              "2017-12-14  0.004076         -0.004076         0.004076  \n",
              "2017-12-15  0.002127         -0.002127        -0.002127  \n",
              "2017-12-18 -0.002462          0.002462         0.002462  \n",
              "2017-12-19 -0.004815          0.004815         0.004815  \n",
              "2017-12-20 -0.002612          0.002612         0.002612  \n",
              "2017-12-21 -0.000337          0.000337        -0.000337  \n",
              "2017-12-22  0.001349         -0.001349         0.001349  \n",
              "2017-12-25 -0.000927          0.000927         0.000927  \n",
              "2017-12-26  0.000843         -0.000843        -0.000843  \n",
              "2017-12-27 -0.002356          0.002356         0.002356  \n",
              "2017-12-28 -0.004606          0.004606         0.004606  \n",
              "2017-12-29 -0.004585          0.004585         0.004585  \n",
              "2018-01-01 -0.000999          0.000999         0.000999  \n",
              "\n",
              "[261 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "metadata": {
        "id": "XP6EtlYYSjdd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define confusion matrix function\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "#     plt.figure(figsize= (10,10))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "#     plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"black\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    \n",
        "def plotConfusion(start, end, string):\n",
        "  real = tradeTable[\"labelledweight\"]\n",
        "  pred = tradeTable[\"predictedweight\"]\n",
        "  cnf_matrix = confusion_matrix(real[start:end], pred[start:end])\n",
        "  accuracy = np.around(accuracy_score(real[start:end], pred[start:end]),2)\n",
        "  f1= np.around(f1_score(real[start:end], pred[start:end], average='binary'), 2)\n",
        "  auc = np.around(areauc(real[start:end], pred[start:end]), 2)\n",
        "  prec = np.around(precision_score(real[start:end], pred[start:end], average='binary'), 2)\n",
        "  recall = np.around(recall_score(real[start:end], pred[start:end], average='binary'), 2)\n",
        "  mcc =  np.around(matthews_corrcoef(real[start:end], pred[start:end]), 2)\n",
        "  # np.set_printoptions(precision=2)\n",
        "\n",
        "\n",
        "  plot_confusion_matrix(cnf_matrix, classes=['Sell', 'buy'],\n",
        "                        title=string + ' Accuracy: ' +str(accuracy)\n",
        "                        + ', AUC:' + str(auc)\n",
        "#                         + ', F1 Score:' + str(f1)\n",
        "#                         + ', Precison:' + str(prec)\n",
        "#                         + ', Recall:' + str(recall)\n",
        "                        + ', MCC:' + str(mcc))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}